import{_ as o}from"./plugin-vue_export-helper-DlAUqK2U.js";import{r as i,o as c,c as r,a as n,d as a,w as p,b as s,e}from"./app-Coh1oo3x.js";const u="/assets/evict_strategy-1683885152669-14-9LymrW2h.png",d="/assets/72bada1fb3b644b68e0c32d0c2c0401e-BgkF98s3.png",k="/assets/5b33518e573e4d2793b492e1b1f428e0-D437P5NJ.png",m="/assets/mmexport1683360092606-BksPmBkX.png",v="/assets/mmexport1683360093936-BPDzbGUc.png",b="/assets/2023-05-06%2014-52-59%20%E7%9A%84%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE-1683884972652-10-_uxnbtuX.png",h="/assets/EBC_benchmarks_dlrm_emb-SuBKNH8_.png",_={},g=e('<h1 id="torchrec调研" tabindex="-1"><a class="header-anchor" href="#torchrec调研"><span>Torchrec调研</span></a></h1><h2 id="_0-0-未迁移依赖库汇总" tabindex="-1"><a class="header-anchor" href="#_0-0-未迁移依赖库汇总"><span>0.0 未迁移依赖库汇总</span></a></h2><ol><li>torch <ul><li>torch.distributed._shard</li><li>torch.fx._compatibility</li><li>torch.distributed._composable</li><li>torch.distributed.optim._apply_optimizer_in_backward</li><li>torch.distributed.fsdp</li><li>torch._C.distributed_c10d.ProcessGroupNCCL</li><li>torch.distributed._composable.contract</li></ul></li><li>fbgemm-gpu</li></ol><h2 id="_0-1-qa" tabindex="-1"><a class="header-anchor" href="#_0-1-qa"><span>0.1 QA</span></a></h2><ul><li>参考资料： <ul><li>https://pytorch.org/torchrec/ （torchrec文档）</li><li>https://github.com/facebookresearch/dlrm/tree/main/torchrec_dlrm （dlrm）</li><li>https://blog.csdn.net/u013701860/article/details/51140762 （uvm）</li><li>https://www.sohu.com/a/560275515_100093134（整体介绍）</li><li>https://zhuanlan.zhihu.com/p/619060815（整体介绍）</li></ul></li></ul>',5),y=n("ol",null,[n("li",null,"DMP（Distributed Model Parallel）， EBC（Embedding Bag Collections）, KJT(Keyed Jagged Tensor), Planner, Sharder等"),n("li",null,"主要是对模型并行，embedding切分方式以及自动制定分片计划，稀疏，量化等的实现")],-1),f=n("li",null,[n("p",null,"使用EBC数据结构存储embedding")],-1),E=n("li",null,[n("p",null,"使用fused ebc（见后续数据结构对比介绍）")],-1),T=n("p",null,"支持动态表",-1),q={href:"https://zhuanlan.zhihu.com/p/619060815",target:"_blank",rel:"noopener noreferrer"},w=n("li",null,"也类似于hugectr的gpu cache机制，cpu维护一个映射表，映射不同emb在gpu中的分布情况，将常用的emb放显存中，未命中的从ps内存捞，使用的驱逐策略比较有意思，结合了LRU和LFU",-1),B=n("li",null,"dynamic emb目前是以torchrec_dynamic_embedding扩展库的形式整合进torchrec项目中的，在torchrec/contrib/dynamic_embedding下，需要单独编译；整合进torchrec/torchrec/csrc中的代码是从cotrib路径下迁过来的，添加了更多benchmarks和unittests，但是python api貌似还没开发完成，建议使用contrib路径下的源码安装扩展",-1),L=n("li",null,[s("驱逐策略 "),n("ol",null,[n("li",null,"使用LRU和LFU混合的驱逐策略, 最后记录在队列中的内容被驱逐"),n("li",null,"频次使用指数位记录，5bits，概率算法，每次取频次位随机数，全为零是频次指数加一"),n("li",null,"时间使用27bits记录"),n("li",null,"频次位在时间位前，LFU优先级比LRU高（值越小说明使用的越少，则换出）"),n("li",null,"使用队列批量驱逐显存中的emb"),n("li",null,[n("img",{src:u,alt:"evict_strategy",tabindex:"0",loading:"lazy"}),n("figcaption",null,"evict_strategy")])])],-1),P=n("li",null,[n("p",null,"支持多种embedding切分方式："),n("p",null,"row-wise, column-wise, table-wise"),n("p",null,"及三种方式的混合"),n("ol",null,[n("li",null,[s("使用多种不同的分片方式的原因： "),n("ol",null,[n("li",null,"在实际场景中，不同特征的访问频率是不同的，呈现幂率分布"),n("li",null,"若只使用row-wise，则每个特征域对应的emb table将会分布到不同gpu上，由于特征的幂率分布，从不同gpu中获取emb vector的规模可能会不均衡，影响通信性能"),n("li",null,"而使用col-wise，则每找一个特征对应的emb vector都要从所有gpu中获取数据拼接成完整的数据，使通信量均衡"),n("li",null,"用户可根据业务场景自定义分片方式")])])])],-1),A=e("<ol><li>实现了DMP，对网络sparse部分作模型并行，对dense部分作DDP</li><li>pipline：支持数据读取、分发、训练流水 <ol><li>torchrec.distributed.train_pipeline.TrainPipelineBase 使用两个流 <ul><li>the current (default) stream: 执行前线反向优化计算</li><li>self._memcpy_stream:执行input从host到GPU</li></ul></li><li>torchrec.distributed.train_pipeline.<strong>TrainPipelineSparseDist</strong>, 隐藏all2all延迟，同时保留前向/反向的训练顺序 <ul><li>stage 3: forward, backward - uses default CUDA stream</li><li>stage 2: ShardedModule.input_dist() - uses data_dist CUDA stream</li><li>stage 1: device transfer - uses memcpy CUDA stream</li></ul></li></ol></li></ol>",1),x=e('<ol><li>传统推荐系统的ps架构 <ol><li><img src="'+d+'" alt="img" style="zoom:15%;"></li><li>以cpu为中心，ps做kv存储，worker在每个iter从ps捞所需emb vector，前向反向后将更新后的特征推送回ps</li><li>emb table在cpu上</li><li>训练过程需要等待通信，难以组成流水线，无法充分利用算力</li></ol></li><li>torchrec的ps架构 <ol><li><img src="'+k+'" alt="img" style="zoom:15%;"></li><li>以gpu为中心，利用gpu间高带宽通信交换所需emb vector（数据、模型混合并行），能减少与ps交换数据的通信开销</li><li>emb shards（emb table的子集）分布式存储在多个gpu上（或uvm内存中）（规模不算太大）</li><li>还可支持动态表，利用ps存储gpu放不下的emb（超大规模）</li></ol></li><li>ps和uvm的关系 <ol><li>uvm是在cpu内存中开辟一块空间当作是对gpu显存的扩展，cpu、gpu都能使用同样地指针访问其中的数据，便于开发，但实际上还是会做cpu、gpu间数据的隐式传输</li><li>使用uvm是在emb table规模不算特别大，但gpu显存无法完全放入所有emb table的情况下使用的，一定程度上扩展显存大小（逻辑大小）</li><li>而使用超大规模的emb table时，就需要ps存储更多的emb table，再配合动态emb和缓存来实现</li></ol></li></ol>',1),C={href:"https://blog.csdn.net/u013701860/article/details/51140762",target:"_blank",rel:"noopener noreferrer"},D=n("li",null,"使用sharder是为了对embedding做模型并行，是为了摆脱以cpu为中心的传统ps架构，提高gpu使用性能，不同的分片方式也是为了解决显存无法存下所有emb table的问题",-1),R=n("li",null,"不同的分片方式是考量了不同的系统运行环境以及用户需求，以求最大化性能，个人理解uvm是为分片提供了辅助和简化开发维护，并不是只要用了uvm就能解决显存问题，就不需要模型并行能直接从uvm中读emb做数据并行了，这样反而退化为早期的ps架构，无法充分利用gpu高带宽",-1),S=e(`<ol><li><p>EBC&amp;FusedEBC</p><ol><li><p>Embedding 和EmbeddingBag的区别</p><ol><li><p>使用embeddingbag主要是为了解决multi-hot的情况, 而且还能同时支持one-hot</p></li><li><p>embedding是直接查询出多个emb vec, 而embeddingbag是将查询出的多个emb vec做池化(此处为相加)后输出一个最终的emb vec, 这样就能解决multio-hot的情况(对于一个特征域,不但支持单选,还支持多选)</p><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code><span class="token keyword">import</span> torch

vocab_size <span class="token operator">=</span> <span class="token number">5</span> <span class="token comment"># feature_slot size</span>
embedding_dim <span class="token operator">=</span> <span class="token number">3</span>

<span class="token comment"># 大小相同的emb 和emb_bag</span>
embedding <span class="token operator">=</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>num_embeddings<span class="token operator">=</span>vocab_size<span class="token punctuation">,</span> embedding_dim<span class="token operator">=</span>embedding_dim<span class="token punctuation">)</span>
embedding_bag <span class="token operator">=</span> nn<span class="token punctuation">.</span>EmbeddingBag<span class="token punctuation">(</span>num_embeddings<span class="token operator">=</span>vocab_size<span class="token punctuation">,</span> embedding_dim<span class="token operator">=</span>embedding_dim<span class="token punctuation">,</span> mode<span class="token operator">=</span><span class="token string">&#39;sum&#39;</span><span class="token punctuation">)</span>

<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> embedding<span class="token punctuation">.</span>weight
Parameter containing<span class="token punctuation">:</span>
tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span> <span class="token number">0.2669</span><span class="token punctuation">,</span>  <span class="token number">0.0411</span><span class="token punctuation">,</span>  <span class="token number">1.8483</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">0.1264</span><span class="token punctuation">,</span>  <span class="token number">0.4678</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.7871</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">0.9744</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1.1333</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">2.0062</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span> <span class="token number">0.3138</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1.0656</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.6442</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span> <span class="token number">0.1350</span><span class="token punctuation">,</span>  <span class="token number">0.1416</span><span class="token punctuation">,</span>  <span class="token number">0.0687</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
        
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> embedding_bag<span class="token punctuation">.</span>weight
Parameter containing<span class="token punctuation">:</span>
tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">0.9410</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1.2599</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.5800</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1.3137</span><span class="token punctuation">,</span>  <span class="token number">0.2207</span><span class="token punctuation">,</span>  <span class="token number">0.1835</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span> <span class="token number">0.1689</span><span class="token punctuation">,</span>  <span class="token number">2.0827</span><span class="token punctuation">,</span>  <span class="token number">0.7237</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span> <span class="token number">0.2223</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.5492</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.6188</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span> <span class="token number">0.4136</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1.1578</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.7838</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
        
input_ids <span class="token operator">=</span> torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> embedding<span class="token punctuation">(</span>input_ids<span class="token punctuation">)</span>
tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">0.1264</span><span class="token punctuation">,</span>  <span class="token number">0.4678</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.7871</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
         <span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">0.9744</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1.1333</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">2.0062</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
         <span class="token punctuation">[</span> <span class="token number">0.3138</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1.0656</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.6442</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> grad_fn<span class="token operator">=</span><span class="token operator">&lt;</span>EmbeddingBackward0<span class="token operator">&gt;</span><span class="token punctuation">)</span>
         
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> embedding_bag<span class="token punctuation">(</span>input_ids<span class="token punctuation">)</span>
tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">0.9225</span><span class="token punctuation">,</span>  <span class="token number">1.7543</span><span class="token punctuation">,</span>  <span class="token number">0.2883</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> grad_fn<span class="token operator">=</span><span class="token operator">&lt;</span>EmbeddingBagBackward0<span class="token operator">&gt;</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></li></ol></li><li><p>EmbeddingBag和EmbeddingBagCollection的区别</p><ol><li><p>EBC用于管理多个EmbeddingBags</p><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code>B <span class="token operator">=</span> <span class="token number">2</span>
D <span class="token operator">=</span> <span class="token number">8</span>
dense_in_features <span class="token operator">=</span> <span class="token number">100</span>

eb1_config <span class="token operator">=</span> EmbeddingBagConfig<span class="token punctuation">(</span>
    name<span class="token operator">=</span><span class="token string">&quot;t1&quot;</span><span class="token punctuation">,</span> embedding_dim<span class="token operator">=</span>D<span class="token punctuation">,</span> num_embeddings<span class="token operator">=</span><span class="token number">100</span><span class="token punctuation">,</span> feature_names<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">&quot;f1&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;f3&quot;</span><span class="token punctuation">]</span>
<span class="token punctuation">)</span>
eb2_config <span class="token operator">=</span> EmbeddingBagConfig<span class="token punctuation">(</span>
    name<span class="token operator">=</span><span class="token string">&quot;t2&quot;</span><span class="token punctuation">,</span>
    embedding_dim<span class="token operator">=</span>D<span class="token punctuation">,</span>
    num_embeddings<span class="token operator">=</span><span class="token number">100</span><span class="token punctuation">,</span>
    feature_names<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">&quot;f2&quot;</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
<span class="token punctuation">)</span>

ebc <span class="token operator">=</span> EmbeddingBagCollection<span class="token punctuation">(</span>tables<span class="token operator">=</span><span class="token punctuation">[</span>eb1_config<span class="token punctuation">,</span> eb2_config<span class="token punctuation">]</span><span class="token punctuation">)</span>

<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> ebc
EmbeddingBagCollection<span class="token punctuation">(</span>
  <span class="token punctuation">(</span>embedding_bags<span class="token punctuation">)</span><span class="token punctuation">:</span> ModuleDict<span class="token punctuation">(</span>
    <span class="token punctuation">(</span>t1<span class="token punctuation">)</span><span class="token punctuation">:</span> EmbeddingBag<span class="token punctuation">(</span><span class="token number">100</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> mode<span class="token operator">=</span><span class="token string">&#39;sum&#39;</span><span class="token punctuation">)</span>
    <span class="token punctuation">(</span>t2<span class="token punctuation">)</span><span class="token punctuation">:</span> EmbeddingBag<span class="token punctuation">(</span><span class="token number">100</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> mode<span class="token operator">=</span><span class="token string">&#39;sum&#39;</span><span class="token punctuation">)</span>
  <span class="token punctuation">)</span>
<span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></li></ol></li><li><p>EBC和FusedEBC的区别</p><ol><li><img src="`+m+'" alt="mmexport1683360092606" style="zoom:25%;"></li><li><img src="'+v+`" alt="mmexport1683360093936" style="zoom:25%;"></li><li>fused ebc是对普通ebc作了算子方面的融合优化（依赖fbgemm库），如，可使用一个kernel实现多个emb的查询</li><li>在查表时，常规的方法是一个warp内的所有线程连续读取一组待查询key，再从emb table中随机读取对应emb vector，无法充分利用显存带宽；</li><li>fused ebc的查表方式是使用cuda的shfl_sync，一个warp内的所有thread同时拷贝同一条emb vector，这样两个过程都是连续读</li></ol></li></ol></li><li><p>KJT</p><ol><li><p>kjt数据结构是在cpu还是gpu使用?为何会用到不同长度的tensor?</p><ol><li><p>使用不同长度的tensor是为了表示如multi-hot, 缺省值，每个batch中对应特征域特征出现的情况, 在cpu, gpu都会使用</p></li><li><p>torchrec中是使用EBC结构(基于torch的embeddingbag)存储多个特征域的嵌入表的, 然后使用kjt数据结构作为待查询key的张量, 传入ebc得到对应的嵌入向量</p><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code><span class="token comment"># 示例：</span>
<span class="token comment">#|------------|------------|</span>
<span class="token comment">#| product ID | user ID    |</span>
<span class="token comment">#|------------|------------|</span>
<span class="token comment">#| [101, 202] | [404]      |</span>
<span class="token comment">#| []         | [505]      |</span>
<span class="token comment">#| [303]      | [606]      |</span>
<span class="token comment">#|------------|------------|</span>

mb <span class="token operator">=</span> torchrec<span class="token punctuation">.</span>KeyedJaggedTensor<span class="token punctuation">(</span>
    keys <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">&quot;product&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;user&quot;</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    values <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">101</span><span class="token punctuation">,</span> <span class="token number">202</span><span class="token punctuation">,</span> <span class="token number">303</span><span class="token punctuation">,</span> <span class="token number">404</span><span class="token punctuation">,</span> <span class="token number">505</span><span class="token punctuation">,</span> <span class="token number">606</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    lengths <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>int64<span class="token punctuation">)</span><span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
<span class="token punctuation">)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span>mb<span class="token punctuation">.</span>to<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">&quot;cpu&quot;</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> KeyedJaggedTensor<span class="token punctuation">(</span><span class="token punctuation">{</span>
    <span class="token string">&quot;product&quot;</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">101</span><span class="token punctuation">,</span> <span class="token number">202</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">303</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    <span class="token string">&quot;user&quot;</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">404</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">505</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">606</span><span class="token punctuation">]</span><span class="token punctuation">]</span>
<span class="token punctuation">}</span><span class="token punctuation">)</span>


<span class="token comment"># sparse 参数存储</span>
eb1_config <span class="token operator">=</span> EmbeddingBagConfig<span class="token punctuation">(</span>
name<span class="token operator">=</span><span class="token string">&quot;t1&quot;</span><span class="token punctuation">,</span> embedding_dim<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> num_embeddings<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">,</span> feature_names<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">&quot;f1&quot;</span><span class="token punctuation">]</span>
<span class="token punctuation">)</span>
eb2_config <span class="token operator">=</span> EmbeddingBagConfig<span class="token punctuation">(</span>
name<span class="token operator">=</span><span class="token string">&quot;t2&quot;</span><span class="token punctuation">,</span> embedding_dim<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> num_embeddings<span class="token operator">=</span><span class="token number">12</span><span class="token punctuation">,</span> feature_names<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">&quot;f2&quot;</span><span class="token punctuation">]</span>
<span class="token punctuation">)</span>

ebc <span class="token operator">=</span> EmbeddingBagCollection<span class="token punctuation">(</span>tables<span class="token operator">=</span><span class="token punctuation">[</span>eb1_config<span class="token punctuation">]</span><span class="token punctuation">)</span>
sparse_arch <span class="token operator">=</span> SparseArch<span class="token punctuation">(</span>ebc<span class="token punctuation">)</span>

<span class="token comment">#     0       1        2  &lt;-- batch</span>
<span class="token comment"># 0   [0,1] None    [2]</span>
<span class="token comment"># 1   [3]    [4]    [5,6,7]</span>
<span class="token comment"># feature</span>

features <span class="token operator">=</span> KeyedJaggedTensor<span class="token punctuation">.</span>from_offsets_sync<span class="token punctuation">(</span>
   keys<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">&quot;f1&quot;</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
   values<span class="token operator">=</span>torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token number">6</span><span class="token punctuation">,</span><span class="token number">7</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
   offsets<span class="token operator">=</span>torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
   <span class="token punctuation">)</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> sparse_arch<span class="token punctuation">(</span>features<span class="token punctuation">)</span>
tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">0.0635</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.6799</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1.8670</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token punctuation">[</span> <span class="token number">0.0000</span><span class="token punctuation">,</span>  <span class="token number">0.0000</span><span class="token punctuation">,</span>  <span class="token number">0.0000</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">0.8205</span><span class="token punctuation">,</span>  <span class="token number">0.8911</span><span class="token punctuation">,</span>  <span class="token number">0.5688</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token punctuation">[</span> <span class="token number">1.7768</span><span class="token punctuation">,</span>  <span class="token number">0.3763</span><span class="token punctuation">,</span>  <span class="token number">0.9286</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1.6304</span><span class="token punctuation">,</span>  <span class="token number">1.8683</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.0427</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1.6682</span><span class="token punctuation">,</span>  <span class="token number">2.4401</span><span class="token punctuation">,</span>  <span class="token number">0.6767</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> grad_fn<span class="token operator">=</span><span class="token operator">&lt;</span>ReshapeAliasBackward0<span class="token operator">&gt;</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></li></ol></li></ol></li></ol>`,1),z=e("<ol><li>embedding分片方式的区别: <ul><li>hugectr的distributedslotembeddinghash是将每个slot划分到所有GPU上,相当于torchrec的row-wise sharding</li><li>hugectr的localizedslotembeddinghash是将某个完整的slot分配到某个GPU上,相当于torchrec的table-wise sharding</li><li>除此之外, torchrec还<strong>支持更多的切分方式</strong>, 满足用户定制化网络拓扑的需求, 且能够<strong>自动选择</strong>在当前运行环境下最优的分片方式</li></ul></li><li>在分布式上都支持<strong>数据模型混合并行</strong>, dense网络部分的处理都是数据并行, embedding都是模型并行, 且前向反向的计算过程也都在GPU上进行, 最大程度利用了GPU的并行计算性能</li><li>在数据加载上也都支持数据读取,分发,训练流水</li><li>数据预处理: <ul><li>torchrec是先补全缺省值,然后重新映射key id到连续id空间,再将出现次数少于阈值3的key映射到key_id=1,再做shuffle</li><li>hugectr也是先补全缺省值,重新映射到连续id, 再将出现次数少于阈值6的key映射到某一特定id,再做shuffle,**最后还可以使用feature cross(特征组合)**进一步减少特征数量,提升组合特征的表达能力</li></ul></li></ol>",1),M=n("ol",null,[n("li",null,"用在模型并行模块中, 为FSDP方式提供支持, 实际上只用在test脚本中, dlrm网络未用到")],-1),U=e("<ol><li>简单总结,就是根据输入网络结构,参数规模, 运行的设备环境的拓扑,设备的带宽,显存, 数据类型等信息, 穷举所有切分方案,然后挑出所有可行的方案做一个模拟性能评估, 根据评估的结果再挑出性能最好的方案出来</li><li>执行流程&amp;细节： <ol><li>设置Topology, 生成ShardingPlan, Planner有两个阶段: <ol><li>Planning stage:在给定sharders和运行环境(Topology)的前提下决定如何切分模型,输出ShardingPlan, 会给出什么切分方式,使用什么compute kernel, perf, rank, 评估将会占用多少HBM存储空间 <ol><li>Topology类: 代表网络设备集群组织方式, 可设置设备类型, world size, 每个设备的存储空间(hbm,ddr)大小,带宽等.</li><li>Shard: emb的子表, 主要记录了size和offset</li><li>enumerator类用于列举所有可行的方案，之后estimator类再对不同的切分方式做perf /storage estimation, 之后将方案传入proposers按照不同策略对所有方案的评估性能作优劣排序, 最后选择最优的方案</li></ol></li><li>Sharding stage:根据ShardingPlan使用给定的sharder切分模型,需要在运行环境中执行 <ol><li>Partitioner, 用于切分shards, 有多种策略,如Greedy, BLDM, Linear等(目前只有Greedy的实现)</li></ol></li></ol></li><li>在rank0上制定plan然后broadcast到所有设备上</li><li>需要预留出kjt和dense部分的空间出来不考虑在shards占据的空间之内</li><li>预估embedding wall time perf 和峰值内存，是按照经验设定了一系列如带宽一类的常量来预估的（见constants.py），会计算每一个shards的性能</li><li>使用ParameterConstaints选择sharding类型提供pooling因子 <ol><li>能够帮助planner更准确的评估性能</li><li>用户设置一些sharding限制项来指导planner, 如,限定什么划分方式, 最少划分多少块, 使用什么计算kernel等</li></ol></li><li>超过GPU显存时自动触发UVM Caching</li></ol></li></ol>",1),O=n("ol",null,[n("li",null,"使用torch.utils.data.DataLoader从torchrec.datasets.criteo.InMemoryBinaryCriteoIterDataPipe中读取数据, 以torchrec.datasets.utils.Batch数据结构存入用于每个iter"),n("li",null,"没有像hugectr使用keyset list文件直接预先划分好每个pass需要使用什么emb的步骤")],-1),I=n("li",null,[n("p",null,[s("torchrec对dlrm网络结构做了对应实现以及优化版本，主要实现了SparseArch, DenseArch, InteractionArch, OverArch， 分别对应网络结构中的Embedding, Bottom MLP, Pairwise interaction, concat&Top MLP"),n("img",{src:b,alt:"2023-05-06 14-52-59 的屏幕截图",style:{zoom:"67%"}})])],-1),G={href:"https://github.com/facebookresearch/dlrm/tree/main/torchrec_dlrm",target:"_blank",rel:"noopener noreferrer"},H=e(`<li><p>首先读取网络参数，使用torch.distributed设置好网络拓扑, 后端可选nccl和gloo</p><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code>rank <span class="token operator">=</span> <span class="token builtin">int</span><span class="token punctuation">(</span>os<span class="token punctuation">.</span>environ<span class="token punctuation">[</span><span class="token string">&quot;LOCAL_RANK&quot;</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token keyword">if</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    device<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>device <span class="token operator">=</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f&quot;cuda:</span><span class="token interpolation"><span class="token punctuation">{</span>rank<span class="token punctuation">}</span></span><span class="token string">&quot;</span></span><span class="token punctuation">)</span>
    backend <span class="token operator">=</span> <span class="token string">&quot;nccl&quot;</span>
    torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>set_device<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
<span class="token keyword">else</span><span class="token punctuation">:</span>
    device<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>device <span class="token operator">=</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">&quot;cpu&quot;</span><span class="token punctuation">)</span>
    backend <span class="token operator">=</span> <span class="token string">&quot;gloo&quot;</span>

<span class="token keyword">if</span> rank <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>
        <span class="token string">&quot;PARAMS: (lr, batch_size, warmup_steps, decay_start, decay_steps): &quot;</span>
        <span class="token string-interpolation"><span class="token string">f&quot;</span><span class="token interpolation"><span class="token punctuation">{</span><span class="token punctuation">(</span>args<span class="token punctuation">.</span>learning_rate<span class="token punctuation">,</span> args<span class="token punctuation">.</span>batch_size<span class="token punctuation">,</span> args<span class="token punctuation">.</span>lr_warmup_steps<span class="token punctuation">,</span> args<span class="token punctuation">.</span>lr_decay_start<span class="token punctuation">,</span> args<span class="token punctuation">.</span>lr_decay_steps<span class="token punctuation">)</span><span class="token punctuation">}</span></span><span class="token string">&quot;</span></span>
    <span class="token punctuation">)</span>
dist<span class="token punctuation">.</span>init_process_group<span class="token punctuation">(</span>backend<span class="token operator">=</span>backend<span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></li><li><p>然后设置dataloader，定义模型结构，需要根据使用的数据集定义EBC，定义不同网络层的大小</p><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code>train_dataloader <span class="token operator">=</span> get_dataloader<span class="token punctuation">(</span>args<span class="token punctuation">,</span> backend<span class="token punctuation">,</span> <span class="token string">&quot;train&quot;</span><span class="token punctuation">)</span>
val_dataloader <span class="token operator">=</span> get_dataloader<span class="token punctuation">(</span>args<span class="token punctuation">,</span> backend<span class="token punctuation">,</span> <span class="token string">&quot;val&quot;</span><span class="token punctuation">)</span>
test_dataloader <span class="token operator">=</span> get_dataloader<span class="token punctuation">(</span>args<span class="token punctuation">,</span> backend<span class="token punctuation">,</span> <span class="token string">&quot;test&quot;</span><span class="token punctuation">)</span>

eb_configs <span class="token operator">=</span> <span class="token punctuation">[</span>
    EmbeddingBagConfig<span class="token punctuation">(</span>
        name<span class="token operator">=</span><span class="token string-interpolation"><span class="token string">f&quot;t_</span><span class="token interpolation"><span class="token punctuation">{</span>feature_name<span class="token punctuation">}</span></span><span class="token string">&quot;</span></span><span class="token punctuation">,</span>
        embedding_dim<span class="token operator">=</span>args<span class="token punctuation">.</span>embedding_dim<span class="token punctuation">,</span>
        num_embeddings<span class="token operator">=</span>none_throws<span class="token punctuation">(</span>args<span class="token punctuation">.</span>num_embeddings_per_feature<span class="token punctuation">)</span><span class="token punctuation">[</span>feature_idx<span class="token punctuation">]</span>
        <span class="token keyword">if</span> args<span class="token punctuation">.</span>num_embeddings <span class="token keyword">is</span> <span class="token boolean">None</span>
        <span class="token keyword">else</span> args<span class="token punctuation">.</span>num_embeddings<span class="token punctuation">,</span>
        feature_names<span class="token operator">=</span><span class="token punctuation">[</span>feature_name<span class="token punctuation">]</span><span class="token punctuation">,</span>
    <span class="token punctuation">)</span>
    <span class="token keyword">for</span> feature_idx<span class="token punctuation">,</span> feature_name <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>DEFAULT_CAT_NAMES<span class="token punctuation">)</span>
<span class="token punctuation">]</span>
sharded_module_kwargs <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>

dlrm_model <span class="token operator">=</span> DLRM<span class="token punctuation">(</span>
    embedding_bag_collection<span class="token operator">=</span>EmbeddingBagCollection<span class="token punctuation">(</span>
        tables<span class="token operator">=</span>eb_configs<span class="token punctuation">,</span> device<span class="token operator">=</span>torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">&quot;meta&quot;</span><span class="token punctuation">)</span>
    <span class="token punctuation">)</span><span class="token punctuation">,</span>
    dense_in_features<span class="token operator">=</span><span class="token builtin">len</span><span class="token punctuation">(</span>DEFAULT_INT_NAMES<span class="token punctuation">)</span><span class="token punctuation">,</span>
    dense_arch_layer_sizes<span class="token operator">=</span>args<span class="token punctuation">.</span>dense_arch_layer_sizes<span class="token punctuation">,</span>
    over_arch_layer_sizes<span class="token operator">=</span>args<span class="token punctuation">.</span>over_arch_layer_sizes<span class="token punctuation">,</span>
    dense_device<span class="token operator">=</span>device<span class="token punctuation">,</span>
<span class="token punctuation">)</span>
train_model <span class="token operator">=</span> DLRMTrain<span class="token punctuation">(</span>dlrm_model<span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></li><li><p>指定embedding_optimizer， 使用adagrad或sgd</p><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code>embedding_optimizer <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>Adagrad <span class="token keyword">if</span> args<span class="token punctuation">.</span>adagrad <span class="token keyword">else</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>SGD
apply_optimizer_in_backward<span class="token punctuation">(</span>
        embedding_optimizer<span class="token punctuation">,</span>
        train_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>sparse_arch<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        optimizer_kwargs<span class="token punctuation">,</span>
    <span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></li><li><p>定义planner，获取plan</p><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code>planner <span class="token operator">=</span> EmbeddingShardingPlanner<span class="token punctuation">(</span>
    topology<span class="token operator">=</span>Topology<span class="token punctuation">(</span>
        local_world_size<span class="token operator">=</span>get_local_size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        world_size<span class="token operator">=</span>dist<span class="token punctuation">.</span>get_world_size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        compute_device<span class="token operator">=</span>device<span class="token punctuation">.</span><span class="token builtin">type</span><span class="token punctuation">,</span>
    <span class="token punctuation">)</span><span class="token punctuation">,</span>
    batch_size<span class="token operator">=</span>args<span class="token punctuation">.</span>batch_size<span class="token punctuation">,</span>
    <span class="token comment"># If experience OOM, increase the percentage. see</span>
    <span class="token comment"># https://pytorch.org/torchrec/torchrec.distributed.planner.html#torchrec.distributed.planner.storage_reservations.HeuristicalStorageReservation</span>
    storage_reservation<span class="token operator">=</span>HeuristicalStorageReservation<span class="token punctuation">(</span>percentage<span class="token operator">=</span><span class="token number">0.05</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
<span class="token punctuation">)</span>
plan <span class="token operator">=</span> planner<span class="token punctuation">.</span>collective_plan<span class="token punctuation">(</span>
    train_model<span class="token punctuation">,</span> get_default_sharders<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> dist<span class="token punctuation">.</span>GroupMember<span class="token punctuation">.</span>WORLD
<span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></li><li><p>将模型结构、设备、plan传入DMP封装为model</p><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code>model <span class="token operator">=</span> DistributedModelParallel<span class="token punctuation">(</span>
        module<span class="token operator">=</span>train_model<span class="token punctuation">,</span>
        device<span class="token operator">=</span>device<span class="token punctuation">,</span>
        plan<span class="token operator">=</span>plan<span class="token punctuation">,</span>
    <span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></li><li><p>指定dense_optimizer后，再与embedding_optimizer合并为最终optimizer</p><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code>dense_optimizer <span class="token operator">=</span> KeyedOptimizerWrapper<span class="token punctuation">(</span>
    <span class="token builtin">dict</span><span class="token punctuation">(</span>in_backward_optimizer_filter<span class="token punctuation">(</span>model<span class="token punctuation">.</span>named_parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    optimizer_with_params<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
<span class="token punctuation">)</span>
optimizer <span class="token operator">=</span> CombinedOptimizer<span class="token punctuation">(</span><span class="token punctuation">[</span>model<span class="token punctuation">.</span>fused_optimizer<span class="token punctuation">,</span> dense_optimizer<span class="token punctuation">]</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></li><li><p>定义lr_scheduler</p><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code>lr_scheduler <span class="token operator">=</span> LRPolicyScheduler<span class="token punctuation">(</span>
        optimizer<span class="token punctuation">,</span> args<span class="token punctuation">.</span>lr_warmup_steps<span class="token punctuation">,</span> args<span class="token punctuation">.</span>lr_decay_start<span class="token punctuation">,</span> args<span class="token punctuation">.</span>lr_decay_steps
    <span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></li><li><p>训练</p><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code><span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>args<span class="token punctuation">.</span>epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>
    _train<span class="token punctuation">(</span>
        pipeline<span class="token punctuation">,</span>
        train_dataloader<span class="token punctuation">,</span>
        val_dataloader<span class="token punctuation">,</span>
        epoch<span class="token punctuation">,</span>
        lr_scheduler<span class="token punctuation">,</span>
        args<span class="token punctuation">.</span>print_lr<span class="token punctuation">,</span>
        args<span class="token punctuation">.</span>validation_freq_within_epoch<span class="token punctuation">,</span>
        args<span class="token punctuation">.</span>limit_train_batches<span class="token punctuation">,</span>
        args<span class="token punctuation">.</span>limit_val_batches<span class="token punctuation">,</span>
    <span class="token punctuation">)</span>
    val_auroc <span class="token operator">=</span> _evaluate<span class="token punctuation">(</span>args<span class="token punctuation">.</span>limit_val_batches<span class="token punctuation">,</span> pipeline<span class="token punctuation">,</span> val_dataloader<span class="token punctuation">,</span> <span class="token string">&quot;val&quot;</span><span class="token punctuation">)</span>
    results<span class="token punctuation">.</span>val_aurocs<span class="token punctuation">.</span>append<span class="token punctuation">(</span>val_auroc<span class="token punctuation">)</span>

test_auroc <span class="token operator">=</span> _evaluate<span class="token punctuation">(</span>args<span class="token punctuation">.</span>limit_test_batches<span class="token punctuation">,</span> pipeline<span class="token punctuation">,</span> test_dataloader<span class="token punctuation">,</span> <span class="token string">&quot;test&quot;</span><span class="token punctuation">)</span>
results<span class="token punctuation">.</span>test_auroc <span class="token operator">=</span> test_auroc
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ol><li><p>按batch读取数据进行训练</p><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code><span class="token keyword">def</span> <span class="token function">_train</span><span class="token punctuation">(</span>
    pipeline<span class="token punctuation">:</span> TrainPipelineSparseDist<span class="token punctuation">,</span>
    train_dataloader<span class="token punctuation">:</span> DataLoader<span class="token punctuation">,</span>
    val_dataloader<span class="token punctuation">:</span> DataLoader<span class="token punctuation">,</span>
    epoch<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">,</span>
    lr_scheduler<span class="token punctuation">,</span>
    print_lr<span class="token punctuation">:</span> <span class="token builtin">bool</span><span class="token punctuation">,</span>
    validation_freq<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span><span class="token builtin">int</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    limit_train_batches<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span><span class="token builtin">int</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    limit_val_batches<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span><span class="token builtin">int</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">&gt;</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">&quot;&quot;&quot;
    Trains model for 1 epoch. Helper function for train_val_test.

    Args:
        pipeline (TrainPipelineSparseDist): data pipeline.
        train_dataloader (DataLoader): Training set&#39;s dataloader.
        val_dataloader (DataLoader): Validation set&#39;s dataloader.
        epoch (int): The number of complete passes through the training set so far.
        lr_scheduler (LRPolicyScheduler): Learning rate scheduler.
        print_lr (bool): Whether to print the learning rate every training step.
        validation_freq (Optional[int]): The number of training steps between validation runs within an epoch.
        limit_train_batches (Optional[int]): Limits the training set to the first \`limit_train_batches\` batches.
        limit_val_batches (Optional[int]): Limits the validation set to the first \`limit_val_batches\` batches.

    Returns:
        None.
    &quot;&quot;&quot;</span>
    pipeline<span class="token punctuation">.</span>_model<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span>

    iterator <span class="token operator">=</span> itertools<span class="token punctuation">.</span>islice<span class="token punctuation">(</span><span class="token builtin">iter</span><span class="token punctuation">(</span>train_dataloader<span class="token punctuation">)</span><span class="token punctuation">,</span> limit_train_batches<span class="token punctuation">)</span>

    is_rank_zero <span class="token operator">=</span> dist<span class="token punctuation">.</span>get_rank<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">0</span>
    <span class="token keyword">if</span> is_rank_zero<span class="token punctuation">:</span>
        pbar <span class="token operator">=</span> tqdm<span class="token punctuation">(</span>
            <span class="token builtin">iter</span><span class="token punctuation">(</span><span class="token builtin">int</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            desc<span class="token operator">=</span><span class="token string-interpolation"><span class="token string">f&quot;Epoch </span><span class="token interpolation"><span class="token punctuation">{</span>epoch<span class="token punctuation">}</span></span><span class="token string">&quot;</span></span><span class="token punctuation">,</span>
            total<span class="token operator">=</span><span class="token builtin">len</span><span class="token punctuation">(</span>train_dataloader<span class="token punctuation">)</span><span class="token punctuation">,</span>
            disable<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>
        <span class="token punctuation">)</span>

    start_it <span class="token operator">=</span> <span class="token number">0</span>
    n <span class="token operator">=</span> <span class="token punctuation">(</span>
        validation_freq
        <span class="token keyword">if</span> validation_freq
        <span class="token keyword">else</span> limit_train_batches
        <span class="token keyword">if</span> limit_train_batches
        <span class="token keyword">else</span> <span class="token builtin">len</span><span class="token punctuation">(</span>train_dataloader<span class="token punctuation">)</span>
    <span class="token punctuation">)</span>
    <span class="token keyword">for</span> batched_iterator <span class="token keyword">in</span> batched<span class="token punctuation">(</span>iterator<span class="token punctuation">,</span> n<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">for</span> it <span class="token keyword">in</span> itertools<span class="token punctuation">.</span>count<span class="token punctuation">(</span>start_it<span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token keyword">try</span><span class="token punctuation">:</span>
                <span class="token keyword">if</span> is_rank_zero <span class="token keyword">and</span> print_lr<span class="token punctuation">:</span>
                    <span class="token keyword">for</span> i<span class="token punctuation">,</span> g <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>pipeline<span class="token punctuation">.</span>_optimizer<span class="token punctuation">.</span>param_groups<span class="token punctuation">)</span><span class="token punctuation">:</span>
                        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f&quot;lr: </span><span class="token interpolation"><span class="token punctuation">{</span>it<span class="token punctuation">}</span></span><span class="token string"> </span><span class="token interpolation"><span class="token punctuation">{</span>i<span class="token punctuation">}</span></span><span class="token string"> </span><span class="token interpolation"><span class="token punctuation">{</span>g<span class="token punctuation">[</span><span class="token string">&#39;lr&#39;</span><span class="token punctuation">]</span><span class="token punctuation">:</span><span class="token format-spec">.6f</span><span class="token punctuation">}</span></span><span class="token string">&quot;</span></span><span class="token punctuation">)</span>
                pipeline<span class="token punctuation">.</span>progress<span class="token punctuation">(</span>batched_iterator<span class="token punctuation">)</span>
                lr_scheduler<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
                <span class="token keyword">if</span> is_rank_zero<span class="token punctuation">:</span>
                    pbar<span class="token punctuation">.</span>update<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>
            <span class="token keyword">except</span> StopIteration<span class="token punctuation">:</span>
                <span class="token keyword">if</span> is_rank_zero<span class="token punctuation">:</span>
                    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;Total number of iterations:&quot;</span><span class="token punctuation">,</span> it<span class="token punctuation">)</span>
                start_it <span class="token operator">=</span> it
                <span class="token keyword">break</span>

        <span class="token keyword">if</span> validation_freq <span class="token keyword">and</span> start_it <span class="token operator">%</span> validation_freq <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>
            _evaluate<span class="token punctuation">(</span>limit_val_batches<span class="token punctuation">,</span> pipeline<span class="token punctuation">,</span> val_dataloader<span class="token punctuation">,</span> <span class="token string">&quot;val&quot;</span><span class="token punctuation">)</span>
            pipeline<span class="token punctuation">.</span>_model<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></li></ol></li>`,8),N=e(`<h2 id="_0-2-迁移计划" tabindex="-1"><a class="header-anchor" href="#_0-2-迁移计划"><span>0.2 迁移计划</span></a></h2><ol><li>将collection整合进框架中emb相关部分 <ol><li>torchrec的dynamic emb并非是真的动态表，只是实现了gpu cache用于存储更大的emb，并不能动态扩增gpu内表大小</li></ol></li><li>未支持依赖库解决方法 <ol><li>若pytorch组能提供临时版本支持就最好了</li><li>目前想到的方式是先将高版本pytorch中的对应依赖模块解耦出来放到现在的pytorch1.9版本中重新源码安装</li><li>或是现实现cpu版本</li></ol></li><li>迁移后DLRM网络性能测试、热点分析以及调优</li></ol><h2 id="_1-介绍" tabindex="-1"><a class="header-anchor" href="#_1-介绍"><span>1. 介绍</span></a></h2><ul><li><p>相关项目：</p><ul><li>https://github.com/pytorch/torchrec</li><li>https://github.com/facebookresearch/dlrm/tree/main/torchrec_dlrm</li></ul></li><li><p>TorchRec 是PyTorch下大规模推荐系统 (RecSys) 训练框架，能提供推荐系统所需的通用稀疏性和并行性原语，允许使用跨多个 GPU 分片的大型嵌入表来训练模型</p><ul><li>支持混合数据并行/模型并行，多设备/多节点训练</li><li>https://nvidia-merlin.github.io/HugeCTR/main/hugectr_embedding_training_cache.htmlhttps://nvidia-merlin.github.io/HugeCTR/main/hugectr_embedding_training_cache.htmlhttps://nvidia-merlin.github.io/HugeCTR/main/hugectr_embedding_training_cache.htmlhttps://nvidia-merlin.github.io/HugeCTR/main/hugectr_embedding_training_cache.htmlhttps://nvidia-merlin.github.io/HugeCTR/main/hugectr_embedding_training_cache.htmlhttps://nvidia-merlin.github.io/HugeCTR/main/hugectr_embedding_training_cache.htmlhttps://nvidia-merlin.github.io/HugeCTR/main/hugectr_embedding_training_cache.htmlhttps://nvidia-merlin.github.io/HugeCTR/main/hugectr_embedding_training_cache.html可以使用不同的分片策略对嵌入表进行分片，包括data-parallel, table-wise, row-wise, table-wise-row-wise, 和 column-wise sharding</li><li>TorchRec planner 可以自动为模型生成优化的分片计划</li><li>支持数据加载（复制到 GPU）、设备间通信和计算（前向、后向）重叠的流水线，以提高性能</li><li>由 FBGEMM 提供对RecSys的优化kernel，FBGEMM（Facebook 通用矩阵乘法）是用于服务器端推理的低精度、高性能矩阵-矩阵乘法和卷积库</li><li>支持降低精度的训练推理量化</li></ul></li><li><p>环境</p><div class="language-text line-numbers-mode" data-ext="text" data-title="text"><pre class="language-text"><code>Python &gt;= 3.7
CUDA &gt;= 11.0
nvcr.io/nvidia/pytorch:23.02-py3
nvcr.io/nvidia/pytorch:22.08-py3
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></li><li><p>依赖库</p><div class="language-txt line-numbers-mode" data-ext="txt" data-title="txt"><pre class="language-txt"><code>black
cmake
fbgemm-gpu-nightly
hypothesis
iopath
numpy
pandas
pyre-extensions
scikit-build
tabulate
torchmetrics
torchx
tqdm
usort
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></li></ul><h3 id="_1-1-安装-测试" tabindex="-1"><a class="header-anchor" href="#_1-1-安装-测试"><span>1.1 安装&amp;测试</span></a></h3><ul><li><div class="language-bash line-numbers-mode" data-ext="sh" data-title="sh"><pre class="language-bash"><code> <span class="token comment"># 建议使用pip安装</span>
 pip <span class="token function">install</span> torchrec_nightly --force-reinstall
 
 <span class="token comment"># 使用源码安装会出现fbgemm-gpu版本问题</span>
 pip <span class="token function">install</span> <span class="token parameter variable">-r</span> requirements.txt
 python setup.py <span class="token function">install</span> develop
 python setup.py --cpu-only <span class="token function">install</span> develop
 
 <span class="token comment"># 安装后测试</span>
 <span class="token comment"># torchx run -s local_cwd dist.ddp -j 1x2 --gpu 2 --script test_installation.py</span>
 
 <span class="token comment"># torchrun --nnodes 1 --nproc_per_node 2 --rdzv_backend c10d --rdzv_endpoint localhost --rdzv_id 54321 --role trainer test_installation.py</span>
 
 python <span class="token parameter variable">-m</span> torch.distributed.run <span class="token parameter variable">--nnodes</span> <span class="token number">1</span> <span class="token parameter variable">--nproc_per_node</span> <span class="token number">2</span> <span class="token parameter variable">--rdzv_backend</span> c10d <span class="token parameter variable">--rdzv_endpoint</span> localhost <span class="token parameter variable">--rdzv_id</span> <span class="token number">54321</span> <span class="token parameter variable">--role</span> trainer test_installation.py <span class="token parameter variable">--cpu_only</span>
 
 python <span class="token parameter variable">-m</span> torch.distributed.launch <span class="token punctuation">\\</span>
     <span class="token parameter variable">--nproc_per_node</span> <span class="token number">2</span> <span class="token punctuation">\\</span>
     <span class="token parameter variable">--nnodes</span> <span class="token number">1</span> <span class="token punctuation">\\</span>
     <span class="token parameter variable">--node_rank</span> <span class="token number">0</span><span class="token punctuation">\\</span>
     <span class="token parameter variable">--master_addr</span> localhost <span class="token punctuation">\\</span>
     <span class="token parameter variable">--master_port</span> <span class="token number">54321</span><span class="token punctuation">\\</span>
     --use-env test_installation.py
 
 <span class="token comment">#dlrm</span>
 python <span class="token parameter variable">-m</span> torch.distributed.run <span class="token parameter variable">--nnodes</span> <span class="token number">1</span> <span class="token parameter variable">--nproc_per_node</span> <span class="token number">2</span> <span class="token parameter variable">--rdzv_backend</span> c10d <span class="token parameter variable">--rdzv_endpoint</span> localhost <span class="token parameter variable">--rdzv_id</span> <span class="token number">54321</span> <span class="token parameter variable">--role</span> trainer dlrm_main.py
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></li><li><div class="language-bash line-numbers-mode" data-ext="sh" data-title="sh"><pre class="language-bash"><code> <span class="token comment"># pytorch安装位置:</span>
 /torch/venv3/pytorch/lib/python3.7/site-packages/torch
 <span class="token comment"># fbgemm安装位置:</span>
 /torch/venv3/pytorch/lib/python3.7/site-packages/fbgemm_gpu
 
 cmake  <span class="token parameter variable">-DCMAKE_PREFIX_PATH</span><span class="token operator">=</span>/torch/venv3/pytorch/lib/python3.7/site-packages/torch <span class="token punctuation">..</span> <span class="token operator">&amp;&amp;</span> <span class="token function">make</span> <span class="token parameter variable">-j</span>
 
 cmake  <span class="token parameter variable">-DCMAKE_PREFIX_PATH</span><span class="token operator">=</span>/opt/conda/lib/python3.8/site-packages/torch <span class="token punctuation">..</span> <span class="token operator">&amp;&amp;</span> <span class="token function">make</span> <span class="token parameter variable">-j</span>
 
 <span class="token comment"># fbgemm源码安装</span>
 <span class="token comment">#cmake -DUSE_SANITIZER=address -DFBGEMM_LIBRARY_TYPE=shared -DPYTHON_EXECUTABLE=/torch/venv3/pytorch/bin/python3 ..</span>
 
 <span class="token comment">#cmake -DUSE_SANITIZER=address -DFBGEMM_LIBRARY_TYPE=shared -DPYTHON_EXECUTABLE=/opt/conda/bin/python3 ..</span>
 
 <span class="token comment">#make -j VERBOSE=1    </span>
 
 <span class="token comment"># fbgemm-gpu源码安装:</span>
 <span class="token comment"># 安装conda</span>
 <span class="token assign-left variable">miniconda_prefix</span><span class="token operator">=</span><span class="token environment constant">$HOME</span>/miniconda
 <span class="token function">bash</span> miniconda.sh <span class="token parameter variable">-b</span> <span class="token parameter variable">-p</span> <span class="token string">&quot;<span class="token variable">$miniconda_prefix</span>&quot;</span> <span class="token parameter variable">-u</span>
 <span class="token builtin class-name">export</span> <span class="token assign-left variable"><span class="token environment constant">PATH</span></span><span class="token operator">=</span><span class="token variable">$miniconda_prefix</span>/bin:<span class="token environment constant">$PATH</span>
 conda update <span class="token parameter variable">-n</span> base <span class="token parameter variable">-c</span> defaults <span class="token parameter variable">-y</span> conda
 
 <span class="token assign-left variable">env_name</span><span class="token operator">=</span>fbgemm-install
 conda create <span class="token parameter variable">-y</span> <span class="token parameter variable">--name</span> <span class="token string">&quot;<span class="token variable">\${env_name}</span>&quot;</span> <span class="token assign-left variable">python</span><span class="token operator">=</span><span class="token string">&quot;3.7&quot;</span>
 <span class="token builtin class-name">source</span> /root/miniconda/etc/profile.d/conda.sh
 
 <span class="token comment"># source /opt/conda/etc/profile.d/conda.sh</span>
 <span class="token comment"># conda activate fbgemm_install</span>
 conda run <span class="token parameter variable">-n</span> <span class="token string">&quot;<span class="token variable">\${env_name}</span>&quot;</span> pip <span class="token function">install</span> <span class="token parameter variable">--upgrade</span> pip
 conda run <span class="token parameter variable">-n</span> <span class="token string">&quot;<span class="token variable">\${env_name}</span>&quot;</span> python <span class="token parameter variable">-m</span> pip <span class="token function">install</span> pyOpenSSL<span class="token operator">&gt;</span><span class="token number">22.1</span>.0
 conda <span class="token function">install</span> <span class="token parameter variable">-n</span> <span class="token string">&quot;<span class="token variable">\${env_name}</span>&quot;</span> <span class="token parameter variable">-y</span> gxx_linux-64<span class="token operator">=</span><span class="token number">10.4</span>.0 sysroot_linux-64<span class="token operator">=</span><span class="token number">2.17</span> <span class="token parameter variable">-c</span> conda-forge
 
 conda activate fbgemm_install
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></li></ul><h2 id="_2-data-preprocess" tabindex="-1"><a class="header-anchor" href="#_2-data-preprocess"><span>2. Data Preprocess</span></a></h2>`,7),Z=e(`<li><p>criteo-kaggle (7.8GB):</p><ul><li><p>下载与解压数据集</p><div class="language-bash line-numbers-mode" data-ext="sh" data-title="sh"><pre class="language-bash"><code><span class="token function">wget</span> http://go.criteo.net/criteo-research-kaggle-display-advertising-challenge-dataset.tar.gz <span class="token operator">&amp;&amp;</span> /
<span class="token function">tar</span> zxvf criteo-research-kaggle-display-advertising-challenge-dataset.tar.gz
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div></li><li><p>预处理数据集（需要70GB内存）</p></li></ul><div class="language-bash line-numbers-mode" data-ext="sh" data-title="sh"><pre class="language-bash"><code>python <span class="token parameter variable">-m</span> torchrec.datasets.scripts.npy_preproc_criteo <span class="token parameter variable">--input_dir</span> <span class="token variable">$INPUT_PATH</span> <span class="token parameter variable">--output_dir</span> <span class="token variable">$OUTPUT_PATH</span> <span class="token parameter variable">--dataset_name</span> criteo_kaggle
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div></li>`,1),F=n("p",null,"criteo-1t (655GB):",-1),$=e(`<li><p>脚本见项目dlrm https://github.com/facebookresearch/dlrm/blob/main/torchrec_dlrm/scripts/process_Criteo_1TB_Click_Logs_dataset.sh</p><div class="language-bash line-numbers-mode" data-ext="sh" data-title="sh"><pre class="language-bash"><code><span class="token function">git</span> clone <span class="token parameter variable">--recursive</span> https://github.com/facebookresearch/dlrm.git
 
<span class="token builtin class-name">cd</span> ./dlrm/torchrec_dlrm/scripts <span class="token operator">&amp;&amp;</span> <span class="token punctuation">\\</span>
<span class="token function">bash</span> ./process_Criteo_1TB_Click_Logs_dataset.sh /workspace/dataset/favorite/modelzoo-datasets/v1/criteo_terybyte/ /workspace/volume/<span class="token punctuation">[</span>your-workspace<span class="token punctuation">]</span>/criteo_terabyte/intermediate /workspace/volume/<span class="token punctuation">[</span>your-workspace<span class="token punctuation">]</span>/criteo_terabyte/output
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></li>`,1),V=n("p",null,"处理方法:",-1),K=n("p",null,"将tsv文件转为npy文件，划分为dense，sparse，label三个npy文件（需要320GB内存）",-1),W={href:"https://ailab.criteo.com/download-criteo-1tb-click-logs-dataset/",target:"_blank",rel:"noopener noreferrer"},j=e(`<li><p>每行数据格式： [label] [integer feature 1] … [integer feature 13] [categorical feature 1] … [categorical feature 26]</p></li><li><p>torchrec处理dense特征的方式，将整型dense特征转为大于1的float32型</p><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code><span class="token comment"># PyTorch tensors can&#39;t handle uint32, but we can save space by not using int64. Numpy will automatically handle dense values &gt;= 2 ** 31.</span>
dense_np <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>dense<span class="token punctuation">,</span> dtype<span class="token operator">=</span>np<span class="token punctuation">.</span>int32<span class="token punctuation">)</span>
sparse_np <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>sparse<span class="token punctuation">,</span> dtype<span class="token operator">=</span>np<span class="token punctuation">.</span>int32<span class="token punctuation">)</span>
labels_np <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>labels<span class="token punctuation">,</span> dtype<span class="token operator">=</span>np<span class="token punctuation">.</span>int32<span class="token punctuation">)</span>

<span class="token comment"># Log is expensive to compute at runtime.</span>
dense_np <span class="token operator">+=</span> <span class="token number">3</span>
dense_np <span class="token operator">=</span> np<span class="token punctuation">.</span>log<span class="token punctuation">(</span>dense_np<span class="token punctuation">,</span> dtype<span class="token operator">=</span>np<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>

<span class="token comment"># To be consistent with dense and sparse.</span>
labels_np <span class="token operator">=</span> labels_np<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></li>`,2),J=e(`<li><p>将sparse特征处理为contiguous特征（需要480GB内存）</p><ul><li><p>需要所有24天的数据同时输入脚本处理</p></li><li><p>在所有文件中分别统计每一个特征域出现过的特征的出现频率到sparse_to_frequency中</p></li><li><p>将出现频率低于frequency_threshold的特征值都映射为1，其余特征值映射为从2开始的连续值（可能出现频率低的特征对最后label的影响小，所以统一处理为1，能够减小特征域大小？）</p><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code><span class="token comment"># Iterate through each row in each file for the current column and remap each</span>
<span class="token comment"># sparse id to a contiguous id. The contiguous ints start at a value of 2 so that</span>
<span class="token comment"># infrequenct IDs (determined by the frequency_threshold) can be remapped to 1.</span>
running_sum <span class="token operator">=</span> <span class="token number">2</span>
sparse_to_contiguous_int<span class="token punctuation">:</span> Dict<span class="token punctuation">[</span><span class="token builtin">int</span><span class="token punctuation">,</span> <span class="token builtin">int</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>

<span class="token keyword">for</span> f <span class="token keyword">in</span> file_to_features<span class="token punctuation">:</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f&quot;Processing file: </span><span class="token interpolation"><span class="token punctuation">{</span>f<span class="token punctuation">}</span></span><span class="token string">&quot;</span></span><span class="token punctuation">)</span>

    <span class="token keyword">for</span> i<span class="token punctuation">,</span> sparse <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>file_to_features<span class="token punctuation">[</span>f<span class="token punctuation">]</span><span class="token punctuation">[</span>col<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">if</span> sparse <span class="token keyword">not</span> <span class="token keyword">in</span> sparse_to_contiguous_int<span class="token punctuation">:</span>
            <span class="token comment"># If the ID appears less than frequency_threshold amount of times</span>
            <span class="token comment"># remap the value to 1.</span>
            <span class="token keyword">if</span> <span class="token punctuation">(</span>
                frequency_threshold <span class="token operator">&gt;</span> <span class="token number">1</span>
                <span class="token keyword">and</span> sparse_to_frequency<span class="token punctuation">[</span>sparse<span class="token punctuation">]</span> <span class="token operator">&lt;</span> frequency_threshold
            <span class="token punctuation">)</span><span class="token punctuation">:</span>
                sparse_to_contiguous_int<span class="token punctuation">[</span>sparse<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">1</span>
            <span class="token keyword">else</span><span class="token punctuation">:</span>
                sparse_to_contiguous_int<span class="token punctuation">[</span>sparse<span class="token punctuation">]</span> <span class="token operator">=</span> running_sum
                running_sum <span class="token operator">+=</span> <span class="token number">1</span>

        <span class="token comment"># Re-map sparse value to contiguous in place.</span>
        file_to_features<span class="token punctuation">[</span>f<span class="token punctuation">]</span><span class="token punctuation">[</span>col<span class="token punctuation">]</span><span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> sparse_to_contiguous_int<span class="token punctuation">[</span>sparse<span class="token punctuation">]</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></li></ul></li><li><p>shuffle（需要700GB内存）</p><ul><li>day0-day22做训练集，shuffle</li><li>day23做验证集，不做shuffle</li></ul></li>`,2),X=e(`<li><p>criteo-multihot (3.5TB):</p><ul><li><p>利用之前处理好的criteo-1t数据集, 合成multi-hot数据集, 用于MLPerf DLRM v2 benchmark (需要200GB内存)</p><div class="language-bash line-numbers-mode" data-ext="sh" data-title="sh"><pre class="language-bash"><code>python materialize_synthetic_multihot_dataset.py <span class="token punctuation">\\</span>
    <span class="token parameter variable">--in_memory_binary_criteo_path</span> <span class="token variable">$PREPROCESSED_CRITEO_1TB_CLICK_LOGS_DATASET_PATH</span> <span class="token punctuation">\\</span>
    <span class="token parameter variable">--output_path</span> <span class="token variable">$MATERIALIZED_DATASET_PATH</span> <span class="token punctuation">\\</span>
    <span class="token parameter variable">--num_embeddings_per_feature</span> <span class="token number">40000000,39060</span>,17295,7424,20265,3,7122,1543,63,40000000,3067956,405282,10,2209,11938,155,4,976,14,40000000,40000000,40000000,590152,12973,108,36 <span class="token punctuation">\\</span>
    <span class="token parameter variable">--multi_hot_sizes</span> <span class="token number">3,2</span>,1,2,6,1,1,1,1,7,3,8,1,6,9,5,1,1,1,12,100,27,10,3,1,1 <span class="token punctuation">\\</span>
    <span class="token parameter variable">--multi_hot_distribution_type</span> uniform
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></li></ul></li>`,1),Y=n("h2",{id:"_3-torchrec-benchmarks",tabindex:"-1"},[n("a",{class:"header-anchor",href:"#_3-torchrec-benchmarks"},[n("span",null,"3. Torchrec Benchmarks")])],-1),Q=n("p",null,"benchmark对两种EmbeddingBagCollection模型进行比较",-1),nn=n("code",null,"EmbeddingBagCollection",-1),sn={href:"https://pytorch.org/torchrec/torchrec.modules.html#torchrec.modules.embedding_modules.EmbeddingBagCollection",target:"_blank",rel:"noopener noreferrer"},an={href:"https://pytorch.org/docs/stable/generated/torch.nn.EmbeddingBag.html",target:"_blank",rel:"noopener noreferrer"},en=n("code",null,"FusedEmbeddingBagCollection",-1),tn={href:"https://github.com/pytorch/torchrec/blob/main/torchrec/modules/fused_embedding_bag_collection.py#L299",target:"_blank",rel:"noopener noreferrer"},pn={href:"https://github.com/pytorch/FBGEMM",target:"_blank",rel:"noopener noreferrer"},ln=n("ul",null,[n("li",null,[s("UVM： "),n("ul",null,[n("li",null,"能够被CPU或GPU访问的host内存地址空间，使用cudaMalloManaged()分配内存"),n("li",null,"使用UVM能够分配超过显存大小的更多内存，存下更大的嵌入表"),n("li",null,"以page为粒度获取Embedding Table")])]),n("li",null,[s("UVM caching： "),n("ul",null,[n("li",null,"以Embedding row为粒度获取embedding"),n("li",null,"使用software managed cache管理，如果GPU miss，则从内存中调用这个row到GPU显存中"),n("li",null,"可设置caching ratio管理缓存大小占整个Embedding Table的比例")])])],-1),on=e(`<li><p>run</p><div class="language-bash line-numbers-mode" data-ext="sh" data-title="sh"><pre class="language-bash"><code><span class="token builtin class-name">cd</span> benchmarks
<span class="token comment"># modify ebc_benchmarks.py line14: from torchrec.github.benchmarks import ebc_benchmarks_utils -&gt; import ebc_benchmarks_utils</span>
<span class="token comment"># mode: ebc_comparison_dlrm (default) / fused_ebc_uvm / ebc_comparison_scaling</span>
python ebc_benchmarks.py <span class="token punctuation">[</span>--mode MODE<span class="token punctuation">]</span> <span class="token punctuation">[</span>--cpu_only<span class="token punctuation">]</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></li><li><p>结论：</p><ul><li>使用了UVM 或UVM caching的FusedEBC相比EBC模型具有更快的性能表现，且FusedEBC支持超过显存大小的Embedding<img src="`+h+'" alt="" loading="lazy"></li></ul></li>',2),cn=n("h2",{id:"_4-dlrm-benchmarks",tabindex:"-1"},[n("a",{class:"header-anchor",href:"#_4-dlrm-benchmarks"},[n("span",null,"4. DLRM Benchmarks")])],-1),rn=e(`<li><p>MLPerf DLRM v1 benchmark</p><ul><li><p>使用DLRM项目下的torchrec_dlrm</p><div class="language-bash line-numbers-mode" data-ext="sh" data-title="sh"><pre class="language-bash"><code><span class="token function">git</span> clone <span class="token parameter variable">--recursive</span> https://github.com/facebookresearch/dlrm.git
<span class="token builtin class-name">cd</span> dlrm/torchrec_dlrm
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div></li><li><p>预处理数据，处理方法见3小节</p></li><li><p>run</p><div class="language-bash line-numbers-mode" data-ext="sh" data-title="sh"><pre class="language-bash"><code><span class="token builtin class-name">export</span> <span class="token assign-left variable">PREPROCESSED_DATASET</span><span class="token operator">=</span><span class="token variable">$insert_your_path_here</span>
<span class="token builtin class-name">export</span> <span class="token assign-left variable">TOTAL_TRAINING_SAMPLES</span><span class="token operator">=</span><span class="token number">4195197692</span> <span class="token punctuation">;</span>
<span class="token builtin class-name">export</span> <span class="token assign-left variable">GLOBAL_BATCH_SIZE</span><span class="token operator">=</span><span class="token number">16384</span> <span class="token punctuation">;</span>
<span class="token builtin class-name">export</span> <span class="token assign-left variable">WORLD_SIZE</span><span class="token operator">=</span><span class="token number">8</span> <span class="token punctuation">;</span>
torchx run <span class="token parameter variable">-s</span> local_cwd dist.ddp <span class="token parameter variable">-j</span> 1x8 <span class="token parameter variable">--script</span> dlrm_main.py -- <span class="token punctuation">\\</span>
    <span class="token parameter variable">--embedding_dim</span> <span class="token number">128</span> <span class="token punctuation">\\</span>
    <span class="token parameter variable">--dense_arch_layer_sizes</span> <span class="token number">512,256</span>,128 <span class="token punctuation">\\</span>
    <span class="token parameter variable">--over_arch_layer_sizes</span> <span class="token number">1024,1024</span>,512,256,1 <span class="token punctuation">\\</span>
    <span class="token parameter variable">--in_memory_binary_criteo_path</span> <span class="token variable">$PREPROCESSED_DATASET</span> <span class="token punctuation">\\</span>
    <span class="token parameter variable">--num_embeddings_per_feature</span> <span class="token number">40000000,39060</span>,17295,7424,20265,3,7122,1543,63,40000000,3067956,405282,10,2209,11938,155,4,976,14,40000000,40000000,40000000,590152,12973,108,36 <span class="token punctuation">\\</span>
    <span class="token parameter variable">--validation_freq_within_epoch</span> <span class="token variable"><span class="token variable">$((</span>TOTAL_TRAINING_SAMPLES <span class="token operator">/</span> <span class="token punctuation">(</span>GLOBAL_BATCH_SIZE <span class="token operator">*</span> <span class="token number">20</span><span class="token variable">))</span></span><span class="token punctuation">)</span> <span class="token punctuation">\\</span>
    <span class="token parameter variable">--epochs</span> <span class="token number">1</span> <span class="token punctuation">\\</span>
    <span class="token parameter variable">--pin_memory</span> <span class="token punctuation">\\</span>
    <span class="token parameter variable">--mmap_mode</span> <span class="token punctuation">\\</span>
    <span class="token parameter variable">--batch_size</span> <span class="token variable"><span class="token variable">$((</span>GLOBAL_BATCH_SIZE <span class="token operator">/</span> WORLD_SIZE<span class="token variable">))</span></span> <span class="token punctuation">\\</span>
    <span class="token parameter variable">--learning_rate</span> <span class="token number">1.0</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></li></ul></li><li></li><li><p>MLPerf DLRM v2 benchmark</p><ul><li><p>使用DLRM项目下的torchrec_dlrm</p><div class="language-bash line-numbers-mode" data-ext="sh" data-title="sh"><pre class="language-bash"><code><span class="token function">git</span> clone <span class="token parameter variable">--recursive</span> https://github.com/facebookresearch/dlrm.git
<span class="token builtin class-name">cd</span> dlrm/torchrec_dlrm
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div></li><li><p>预处理数据，处理方法见3小节</p></li><li><p>run（使用合成multi-hot数据，3.8TB）</p><div class="language-bash line-numbers-mode" data-ext="sh" data-title="sh"><pre class="language-bash"><code><span class="token builtin class-name">export</span> <span class="token assign-left variable">MULTIHOT_PREPROCESSED_DATASET</span><span class="token operator">=</span><span class="token variable">$your_path_here</span>
<span class="token builtin class-name">export</span> <span class="token assign-left variable">TOTAL_TRAINING_SAMPLES</span><span class="token operator">=</span><span class="token number">4195197692</span> <span class="token punctuation">;</span>
<span class="token builtin class-name">export</span> <span class="token assign-left variable">GLOBAL_BATCH_SIZE</span><span class="token operator">=</span><span class="token number">65536</span> <span class="token punctuation">;</span>
<span class="token builtin class-name">export</span> <span class="token assign-left variable">WORLD_SIZE</span><span class="token operator">=</span><span class="token number">8</span> <span class="token punctuation">;</span>
torchx run <span class="token parameter variable">-s</span> local_cwd dist.ddp <span class="token parameter variable">-j</span> 1x8 <span class="token parameter variable">--script</span> dlrm_main.py -- <span class="token punctuation">\\</span>
    <span class="token parameter variable">--embedding_dim</span> <span class="token number">128</span> <span class="token punctuation">\\</span>
    <span class="token parameter variable">--dense_arch_layer_sizes</span> <span class="token number">512,256</span>,128 <span class="token punctuation">\\</span>
    <span class="token parameter variable">--over_arch_layer_sizes</span> <span class="token number">1024,1024</span>,512,256,1 <span class="token punctuation">\\</span>
    <span class="token parameter variable">--synthetic_multi_hot_criteo_path</span> <span class="token variable">$MULTIHOT_PREPROCESSED_DATASET</span> <span class="token punctuation">\\</span>
    <span class="token parameter variable">--num_embeddings_per_feature</span> <span class="token number">40000000,39060</span>,17295,7424,20265,3,7122,1543,63,40000000,3067956,405282,10,2209,11938,155,4,976,14,40000000,40000000,40000000,590152,12973,108,36 <span class="token punctuation">\\</span>
    <span class="token parameter variable">--validation_freq_within_epoch</span> <span class="token variable"><span class="token variable">$((</span>TOTAL_TRAINING_SAMPLES <span class="token operator">/</span> <span class="token punctuation">(</span>GLOBAL_BATCH_SIZE <span class="token operator">*</span> <span class="token number">20</span><span class="token variable">))</span></span><span class="token punctuation">)</span> <span class="token punctuation">\\</span>
    <span class="token parameter variable">--epochs</span> <span class="token number">1</span> <span class="token punctuation">\\</span>
    <span class="token parameter variable">--pin_memory</span> <span class="token punctuation">\\</span>
    <span class="token parameter variable">--mmap_mode</span> <span class="token punctuation">\\</span>
    <span class="token parameter variable">--batch_size</span> <span class="token variable"><span class="token variable">$((</span>GLOBAL_BATCH_SIZE <span class="token operator">/</span> WORLD_SIZE<span class="token variable">))</span></span> <span class="token punctuation">\\</span>
    <span class="token parameter variable">--interaction_type</span><span class="token operator">=</span>dcn <span class="token punctuation">\\</span>
    <span class="token parameter variable">--dcn_num_layers</span><span class="token operator">=</span><span class="token number">3</span> <span class="token punctuation">\\</span>
    <span class="token parameter variable">--dcn_low_rank_dim</span><span class="token operator">=</span><span class="token number">512</span> <span class="token punctuation">\\</span>
    <span class="token parameter variable">--adagrad</span> <span class="token punctuation">\\</span>
    <span class="token parameter variable">--learning_rate</span> <span class="token number">0.005</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></li><li><p>run（使用预处理后的criteo-1t数据集动态生成的multi-hot数据，存储空间不足3.8TB时可用）</p><div class="language-bash line-numbers-mode" data-ext="sh" data-title="sh"><pre class="language-bash"><code><span class="token builtin class-name">export</span> <span class="token assign-left variable">PREPROCESSED_DATASET</span><span class="token operator">=</span><span class="token variable">$insert_your_path_here</span>
<span class="token builtin class-name">export</span> <span class="token assign-left variable">TOTAL_TRAINING_SAMPLES</span><span class="token operator">=</span><span class="token number">4195197692</span> <span class="token punctuation">;</span>
<span class="token builtin class-name">export</span> <span class="token assign-left variable">BATCHSIZE</span><span class="token operator">=</span><span class="token number">65536</span> <span class="token punctuation">;</span>
<span class="token builtin class-name">export</span> <span class="token assign-left variable">WORLD_SIZE</span><span class="token operator">=</span><span class="token number">8</span> <span class="token punctuation">;</span>
torchx run <span class="token parameter variable">-s</span> local_cwd dist.ddp <span class="token parameter variable">-j</span> 1x8 <span class="token parameter variable">--script</span> dlrm_main.py -- <span class="token punctuation">\\</span>
    <span class="token parameter variable">--embedding_dim</span> <span class="token number">128</span> <span class="token punctuation">\\</span>
    <span class="token parameter variable">--dense_arch_layer_sizes</span> <span class="token number">512,256</span>,128 <span class="token punctuation">\\</span>
    <span class="token parameter variable">--over_arch_layer_sizes</span> <span class="token number">1024,1024</span>,512,256,1 <span class="token punctuation">\\</span>
    <span class="token parameter variable">--in_memory_binary_criteo_path</span> <span class="token variable">$PREPROCESSED_DATASET</span> <span class="token punctuation">\\</span>
    <span class="token parameter variable">--num_embeddings_per_feature</span> <span class="token number">40000000,39060</span>,17295,7424,20265,3,7122,1543,63,40000000,3067956,405282,10,2209,11938,155,4,976,14,40000000,40000000,40000000,590152,12973,108,36 <span class="token punctuation">\\</span>
    <span class="token parameter variable">--validation_freq_within_epoch</span> <span class="token variable"><span class="token variable">$((</span>TOTAL_TRAINING_SAMPLES <span class="token operator">/</span> <span class="token punctuation">(</span>BATCHSIZE <span class="token operator">*</span> <span class="token number">20</span><span class="token variable">))</span></span><span class="token punctuation">)</span> <span class="token punctuation">\\</span>
    <span class="token parameter variable">--epochs</span> <span class="token number">1</span> <span class="token punctuation">\\</span>
    <span class="token parameter variable">--pin_memory</span> <span class="token punctuation">\\</span>
    <span class="token parameter variable">--mmap_mode</span> <span class="token punctuation">\\</span>
    <span class="token parameter variable">--batch_size</span> <span class="token variable"><span class="token variable">$((</span>GLOBAL_BATCH_SIZE <span class="token operator">/</span> WORLD_SIZE<span class="token variable">))</span></span> <span class="token punctuation">\\</span>
    <span class="token parameter variable">--interaction_type</span><span class="token operator">=</span>dcn <span class="token punctuation">\\</span>
    <span class="token parameter variable">--dcn_num_layers</span><span class="token operator">=</span><span class="token number">3</span> <span class="token punctuation">\\</span>
    <span class="token parameter variable">--dcn_low_rank_dim</span><span class="token operator">=</span><span class="token number">512</span> <span class="token punctuation">\\</span>
    <span class="token parameter variable">--adagrad</span> <span class="token punctuation">\\</span>
    <span class="token parameter variable">--learning_rate</span> <span class="token number">0.005</span> <span class="token punctuation">\\</span>
    <span class="token parameter variable">--multi_hot_distribution_type</span> uniform <span class="token punctuation">\\</span>
    <span class="token parameter variable">--multi_hot_sizes</span><span class="token operator">=</span><span class="token number">3,2</span>,1,2,6,1,1,1,1,7,3,8,1,6,9,5,1,1,1,12,100,27,10,3,1,1

</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></li></ul></li>`,3),un=n("p",null,"MLPerf DLRM Benchmark v1 & v2比较：",-1),dn=n("thead",null,[n("tr",null,[n("th"),n("th",null,"DLRM v1"),n("th",null,"DLRM v2")])],-1),kn=n("tr",null,[n("td",null,[n("strong",null,"Optimizer")]),n("td",null,"SGD"),n("td",null,"Adagrad")],-1),mn=n("tr",null,[n("td",null,[n("strong",null,"Learning Rate")]),n("td",null,"1.0"),n("td",null,"0.005")],-1),vn=n("tr",null,[n("td",null,[n("strong",null,"Batch size")]),n("td",null,"AUC 0.8025 within 1 epoch using 16384"),n("td",null,"AUC 0.8025 within 1 epoch using 65536")],-1),bn=n("tr",null,[n("td",null,[n("strong",null,"Interaction Layer")]),n("td",null,"dot interaction"),n("td",null,"DCN V2 with low rank approximation")],-1),hn=n("td",null,[n("strong",null,"Dataset")],-1),_n={href:"https://github.com/facebookresearch/dlrm/blob/main/data_utils.py",target:"_blank",rel:"noopener noreferrer"},gn=n("td",null,"Criteo 1TB Click Logs Dataset but the sparse features are replaced with a multi-hot dataset.",-1),yn=e(`<li><p>使用Criteo Kaggle 数据集， 默认网络参数</p><ul><li><p>使用DLRM项目下的torchrec_dlrm</p><div class="language-bash line-numbers-mode" data-ext="sh" data-title="sh"><pre class="language-bash"><code><span class="token function">git</span> clone <span class="token parameter variable">--recursive</span> https://github.com/facebookresearch/dlrm.git
<span class="token builtin class-name">cd</span> dlrm/torchrec_dlrm
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div></li><li><p>预处理数据，处理方法见3小节</p></li><li><p>run</p><div class="language-bash line-numbers-mode" data-ext="sh" data-title="sh"><pre class="language-bash"><code><span class="token comment"># modify dlrm/torchrec_dlrm/data/dlrm_dataloader.py line87:</span>
<span class="token comment"># (root_name, stage) = (&quot;train&quot;, &quot;test&quot;) if stage == &quot;val&quot; else stage</span>
<span class="token comment"># -&gt; (root_name, stage) = (&quot;train&quot;, &quot;test&quot;) if stage == &quot;val&quot; else (stage, stage)</span>
<span class="token builtin class-name">export</span> <span class="token assign-left variable">PREPROCESSED_DATASET</span><span class="token operator">=</span><span class="token string">&quot;/workspace/volume/torchrec-criteo-datasets/criteo-kaggle&quot;</span>
<span class="token builtin class-name">export</span> <span class="token assign-left variable">GLOBAL_BATCH_SIZE</span><span class="token operator">=</span><span class="token number">16384</span> <span class="token punctuation">;</span>
<span class="token builtin class-name">export</span> <span class="token assign-left variable">WORLD_SIZE</span><span class="token operator">=</span><span class="token number">8</span> <span class="token punctuation">;</span>
torchx run <span class="token parameter variable">-s</span> local_cwd dist.ddp <span class="token parameter variable">-j</span> 1x8 <span class="token parameter variable">--script</span> dlrm_main.py -- <span class="token punctuation">\\</span>
    <span class="token parameter variable">--in_memory_binary_criteo_path</span> <span class="token variable">$PREPROCESSED_DATASET</span> <span class="token punctuation">\\</span>
    <span class="token parameter variable">--pin_memory</span> <span class="token punctuation">\\</span>
    <span class="token parameter variable">--mmap_mode</span> <span class="token punctuation">\\</span>
    <span class="token parameter variable">--batch_size</span> <span class="token variable"><span class="token variable">$((</span>GLOBAL_BATCH_SIZE <span class="token operator">/</span> WORLD_SIZE<span class="token variable">))</span></span> <span class="token punctuation">\\</span>
    <span class="token parameter variable">--learning_rate</span> <span class="token number">1.0</span> <span class="token punctuation">\\</span>
    <span class="token parameter variable">--dataset_name</span> criteo_kaggle
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></li></ul></li>`,1),fn=e(`<h2 id="_6-dlrm-benchmarks测试结果" tabindex="-1"><a class="header-anchor" href="#_6-dlrm-benchmarks测试结果"><span>6. DLRM Benchmarks测试结果</span></a></h2><ul><li><p>测试环境:</p><table><thead><tr><th>distributed-training/torchrec:pytorch22.08-py3</th></tr></thead><tbody><tr><td>8*A100-SXM4-80GB</td></tr></tbody></table><ul><li><p>MLPerf DLRM v1 benchmark, 1 epoch</p><ul><li>AUROC over val set: <strong>0.8004854917526245</strong></li><li>AUROC over test set: <strong>0.7949966788291931</strong></li></ul></li><li><p>MLPerf DLRM v2 benchmark, 1 epoch</p><ul><li>AUROC over val set: <strong>0.8040649890899658</strong></li><li>AUROC over test set: <strong>0.7980538010597229</strong></li></ul></li><li><p>使用Criteo Kaggle 数据集， 默认网络参数, 1 epoch</p><ul><li><p>AUROC over val set: <strong>0.5002527236938477</strong></p></li><li><p>torchrec-dlrm项目原来不支持criteo-kaggle数据集，今年2月增加支持</p></li><li><p>因为criteo-kaggle数据集没有验证集，只能从训练集中划分一部分做验证集</p></li><li><p>原代码加载数据集时报错：</p><div class="language-bash line-numbers-mode" data-ext="sh" data-title="sh"><pre class="language-bash"><code>File <span class="token string">&quot;./dlrm/torchrec_dlrm/data/dlrm_dataloader.py&quot;</span>, line <span class="token number">87</span>, <span class="token keyword">in</span> _get_in_memory_dataloader
dlrm_main/0 <span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span>:    <span class="token punctuation">(</span>root_name, stage<span class="token punctuation">)</span> <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token string">&quot;train&quot;</span>, <span class="token string">&quot;test&quot;</span><span class="token punctuation">)</span> <span class="token keyword">if</span> stage <span class="token operator">==</span> <span class="token string">&quot;val&quot;</span> <span class="token keyword">else</span> stage
dlrm_main/0 <span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">]</span>:ValueError: too many values to unpack <span class="token punctuation">(</span>expected <span class="token number">2</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>将(root_name, stage) = (&quot;train&quot;, &quot;test&quot;) if stage == &quot;val&quot; else stage，修改为(root_name, stage) = (&quot;train&quot;, &quot;test&quot;) if stage == &quot;val&quot; else (stage, stage)后可跑通，但验证时AUROC只有0.5002527236938477，且修改超参仍无法提升</p></li></ul></li></ul></li></ul><h2 id="_7-torchrec与hugectr比较" tabindex="-1"><a class="header-anchor" href="#_7-torchrec与hugectr比较"><span>7. Torchrec与HugeCTR比较</span></a></h2><ul><li><p>Torchrec:</p><ul><li>架构，特点，优缺点： <ul><li>优点： <ul><li>实现分布式模型并行，支持数据模型混合并行，多种emb切分方式</li><li>可根据设备型号与数量，内存大小等信息自动生成切分策略：根据输入网络结构,参数规模, 运行的设备环境的拓扑,设备的带宽,显存, 数据类型等信息, 穷举所有切分方案,然后挑出所有可行的方案做一个模拟性能评估, 根据评估的结果再挑出性能最好的方案出来</li><li>支持多级流水，重叠数据加载，设备间通信（数据分发）和前反向优化计算</li><li>fbgemm库优化kernel，量化等</li><li>PS架构，CPU做Server，GPU做Workers <ul><li>传统PS架构emb在CPU上，需要将更新特征推送回ps，需等待通信，难以组成流水，无法充分利用算力</li><li>当前架构利用gpu高带宽交换emb，减少通信开销，且充分利用并行计算性能</li></ul></li><li>支持GPU cache机制，存储热数据，使用LRU或LFU混合驱逐策略，使用32位数据结构记录数据使用情况：低27位记录时间戳，高5位记录出现频次（概率算法，每次取频次位随机数，全0加一），LFU比LRU优先级高</li><li>支持onehot和multihot以及缺省值数据，更符合真实场景</li></ul></li><li>缺点： <ul><li>支持功能多，导致框架内部中间数据传递低效，需多次转换</li><li>不支持动态扩表，只支持静态表，无法处理新特征</li><li>新网络适配到框架需要做一定开发，无法直接使用封装接口</li></ul></li></ul></li><li>数据流： <ul><li>大多实际业务数据是TB级的，可分为onehot和multihot数据，相当于某一选项单选还是多选</li><li>criteo-1t点击率数据集：1label-13dense（数值型）-26-sparse（分类型），共24天数据，大小达655GB</li><li>数据预处理： <ul><li>将dense int32转为float32（求log），划分为三个npy文件</li><li>将sparse特征连续化，统计所有出现的特征数量，低于某一阈值的特征都映射为key=1，其余依次累计key值</li><li>0-22天的数据做训练集，shuffle，23天做验证集，不shuffle</li></ul></li><li>数据并行-模型并行-数据并行： <ul><li>数据在内存中通过DDP均匀分发到不同设备上（每张卡数据不同）【scatter】</li><li>不同卡拿到训练数据需要到对应emb表中查询vector，每章表根据plan存储在不同设备上【all2all】</li><li>拿到emb数据后再传入后续网络结构中训练，每个设备上的dense 网络部分需要同步【allreduce】</li><li>最后经过反向梯度传到每个设备上的emb表中，再更新表的权值</li></ul></li></ul></li><li>测试结果：8A100 SXM4 80G <ul><li>MLPerf DLRM v1: 0.8004</li><li>MLPerf DLRM v2: 0.8041</li></ul></li><li>迁移方案&amp;遇到的问题与解决方法： <ul><li>使用设备端哈希表存储emb，来实现动态扩容 <ul><li>key和value分别存储到两张表，key映射到value,key表扩容开辟一块新空间，拷贝到新表，value以链表形式存储，直接追加到表尾</li></ul></li><li>版本与依赖库问题：原生框架只支持pt1.13以上，需要切分和fbgemm库提供支持，因此需要替换或避免这些功能</li><li>将使用到fbgemm库的位置替换实现或重写，使用cnco</li><li>将使用到高版本pt的模块解耦出来作为子模块使用</li><li>不同语言实现的模块使用torch_library绑定c++接口到py端</li><li>auroc使用sklearn替换实现，使用集合通信接口将数据传回cpu做计算</li></ul></li><li>性能提升： <ul><li>使用profiler获取host和device侧算子和kernel调用情况，找出热点算子，再提issue或替换算子来提升精度</li><li>因为不再使用fbgemm库，部分框架功能实现性能差，如内部数据间的相互转换使用低效的标量操作，替换为矩阵操作快很多</li></ul></li><li>精度提升： <ul><li>不同语言间实现的模块在配合使用时用到了不同的stream，传参时没能同步，会导致数据异常</li><li>数据初始化方式使用与表大小相关的均匀分布，精度会更高</li></ul></li></ul></li><li><p>HugeCTR:</p><ul><li>架构，特点，优缺点，异同： <ul><li>相同：都支持读取分发训练流水</li><li>不同： <ul><li>hugectr只支持按行按表切分emb，torchrec能自动选择且能混合多种切分方式</li><li>两者数据预处理方式相似，只是hugectr还可使用特征组合进一步减少特征数量，提升表达能力</li><li>hugectr使用keylist预先划分好训练数据，torchrec直接使用dataloader读取</li><li>hctr支持动态表，使用GPU上的hashtable实现（cucollection）</li></ul></li><li>hugectr更加完善，是merlin推荐系统模型推理训练解决方案下的训练框架，支持GPU中的hash表，异步与多线程流水，分层参数服务器做推理，TF插件sok</li><li>训练时，hugectr预取每个pass的key集合到keyset，从而解决无法存放所有数据的难题</li><li>参数服务器支持全量读如整个emb到host内存中（速度快），或使用多级缓存结构只存部分（克服对模型规模的限制）</li><li>推理时，三级存储结构：使用GPU嵌入缓存将热点emb放在gpu内存中，内存作为二级，使用redis保存部分数据；第三级使用SSD RocksDB保存所有参数（高效存储长尾分布数据），使用的是LRU</li></ul></li></ul></li><li><p>Relevant：</p><ul><li>GPU hashtable： <ul><li>hash冲突处理方法： <ul><li>double hash，分别计算再聚合</li><li>frequency hash：对低频做双hash，高频做identity hash</li></ul></li></ul></li><li>性能指标：profiling timechart， 吞吐量</li><li>精度指标：AUC</li></ul></li></ul>`,4);function En(Tn,qn){const t=i("font"),l=i("ExternalLinkIcon");return c(),r("div",null,[g,n("ol",null,[n("li",null,[a(t,{color:"green"},{default:p(()=>[s("框架主要实现了什么")]),_:1}),y]),n("li",null,[a(t,{color:"green"},{default:p(()=>[s("如何作稀疏训练")]),_:1}),n("ol",null,[f,E,n("li",null,[T,n("ol",null,[n("li",null,[s("动态表是cpu还是gpu实现的? "),n("ol",null,[n("li",null,[s("CPU\\GPU混合实现，torchrec早期版本不支持动态扩容，对超百亿规模模型无法支持，后续是微信团队开发动态表功能后22.9月整合进新版本（见"),n("a",q,[s("介绍"),a(l)]),s("）")]),w,B])]),L])]),P])]),n("li",null,[a(t,{color:"green"},{default:p(()=>[s("如何做分布式")]),_:1}),A]),n("li",null,[a(t,{color:"green"},{default:p(()=>[s("uvm和ps是如何协同工作的? 实现了uvm还有必要使用ps吗?")]),_:1}),x]),n("li",null,[a(t,{color:"green"},{default:p(()=>[s("sharder和uvm有共同实现的必要吗?既然uvm能够解除memory barrier 为何还要不同的分片方式呢?")]),_:1}),n("ol",null,[n("li",null,[s("uvm只是对显存一定程度的扩展，便于开发与维护，不能完全解决显存不够用的情况，而实际上还是会发生cpu、gpu间的隐式数据拷贝，甚至使用stream和cudaMemcpyAsync性能会比用uvm更高（参考"),n("a",C,[s("链接"),a(l)]),s("）")]),D,R])]),n("li",null,[a(t,{color:"green"},{default:p(()=>[s("使用什么基本数据结构")]),_:1}),S]),n("li",null,[a(t,{color:"green"},{default:p(()=>[s("与hugectr的异同")]),_:1}),z]),n("li",null,[a(t,{color:"green"},{default:p(()=>[s("torch.distributed.fsdp用在什么地方")]),_:1}),M]),n("li",null,[a(t,{color:"green"},{default:p(()=>[s("planner如何在训练前知道怎么分emb?")]),_:1}),U]),n("li",null,[a(t,{color:"green"},{default:p(()=>[s("训练前数据需要预先划分然后预取到gpu中吗")]),_:1}),O]),n("li",null,[a(t,{color:"green"},{default:p(()=>[s("dlrm网络训练流程")]),_:1}),n("ol",null,[I,n("li",null,[n("p",null,[n("a",G,[s("项目"),a(l)]),s("中有torchrec对应的网络训练脚本")])]),H])])]),N,n("ol",null,[Z,n("li",null,[F,n("ul",null,[$,n("li",null,[V,n("ol",null,[n("li",null,[K,n("ul",null,[n("li",null,[n("p",null,[n("a",W,[s("原始数据类型"),a(l)]),s("：每条数据1个label（是否被点击），13个dense特征（int型，多为计数值），26个sparse特征（经hash为32bits数据）")])]),j])]),J])])])]),X]),Y,n("ul",null,[n("li",null,[Q,n("ul",null,[n("li",null,[nn,s(" (EBC) ("),n("a",sn,[s("code"),a(l)]),s("): 由 "),n("a",an,[s("torch.nn.EmbeddingBag"),a(l)]),s("支持")]),n("li",null,[en,s(" (Fused EBC) ("),n("a",tn,[s("code"),a(l)]),s("): 由"),n("a",pn,[s("FBGEMM"),a(l)]),s(" kernels 支持，配备了融合优化器和 UVM 或UVM Caching，可为 GPU 提供更大的内存 "),ln])])]),on]),cn,n("ul",null,[rn,n("li",null,[un,n("ul",null,[n("li",null,[n("table",null,[dn,n("tbody",null,[kn,mn,vn,bn,n("tr",null,[hn,n("td",null,[s("Criteo 1TB Click Logs Dataset, but uses a different preprocessing script ("),n("a",_n,[s("repo_root/data_utils.py"),a(l)]),s(")")]),gn])])])])])]),yn]),fn])}const Ln=o(_,[["render",En],["__file","torchrec_note.html.vue"]]),Pn=JSON.parse(`{"path":"/notes/torchrec_note.html","title":"Torchrec调研","lang":"zh-CN","frontmatter":{"date":"2023-06-21T00:00:00.000Z","tag":["Torchrec"],"category":["推荐系统"],"description":"Torchrec调研 0.0 未迁移依赖库汇总 torch torch.distributed._shard torch.fx._compatibility torch.distributed._composable torch.distributed.optim._apply_optimizer_in_backward torch.distribut...","head":[["meta",{"property":"og:url","content":"https://bradzhone.github.io/notes/torchrec_note.html"}],["meta",{"property":"og:site_name","content":"BradZhone's Blog"}],["meta",{"property":"og:title","content":"Torchrec调研"}],["meta",{"property":"og:description","content":"Torchrec调研 0.0 未迁移依赖库汇总 torch torch.distributed._shard torch.fx._compatibility torch.distributed._composable torch.distributed.optim._apply_optimizer_in_backward torch.distribut..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"article:author","content":"BradZhone"}],["meta",{"property":"article:tag","content":"Torchrec"}],["meta",{"property":"article:published_time","content":"2023-06-21T00:00:00.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"Torchrec调研\\",\\"image\\":[\\"\\"],\\"datePublished\\":\\"2023-06-21T00:00:00.000Z\\",\\"dateModified\\":null,\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"BradZhone\\",\\"url\\":\\"https://github.com/BradZhone\\"}]}"]]},"headers":[{"level":2,"title":"0.0 未迁移依赖库汇总","slug":"_0-0-未迁移依赖库汇总","link":"#_0-0-未迁移依赖库汇总","children":[]},{"level":2,"title":"0.1 QA","slug":"_0-1-qa","link":"#_0-1-qa","children":[]},{"level":2,"title":"0.2 迁移计划","slug":"_0-2-迁移计划","link":"#_0-2-迁移计划","children":[]},{"level":2,"title":"1. 介绍","slug":"_1-介绍","link":"#_1-介绍","children":[{"level":3,"title":"1.1 安装&测试","slug":"_1-1-安装-测试","link":"#_1-1-安装-测试","children":[]}]},{"level":2,"title":"2. Data Preprocess","slug":"_2-data-preprocess","link":"#_2-data-preprocess","children":[]},{"level":2,"title":"3. Torchrec Benchmarks","slug":"_3-torchrec-benchmarks","link":"#_3-torchrec-benchmarks","children":[]},{"level":2,"title":"4. DLRM Benchmarks","slug":"_4-dlrm-benchmarks","link":"#_4-dlrm-benchmarks","children":[]},{"level":2,"title":"6. DLRM Benchmarks测试结果","slug":"_6-dlrm-benchmarks测试结果","link":"#_6-dlrm-benchmarks测试结果","children":[]},{"level":2,"title":"7. Torchrec与HugeCTR比较","slug":"_7-torchrec与hugectr比较","link":"#_7-torchrec与hugectr比较","children":[]}],"git":{"createdTime":null,"updatedTime":null,"contributors":[]},"readingTime":{"minutes":25.64,"words":7693},"filePathRelative":"notes/torchrec_note.md","localizedDate":"2023年6月21日","excerpt":"\\n<h2>0.0 未迁移依赖库汇总</h2>\\n<ol>\\n<li>torch\\n<ul>\\n<li>torch.distributed._shard</li>\\n<li>torch.fx._compatibility</li>\\n<li>torch.distributed._composable</li>\\n<li>torch.distributed.optim._apply_optimizer_in_backward</li>\\n<li>torch.distributed.fsdp</li>\\n<li>torch._C.distributed_c10d.ProcessGroupNCCL</li>\\n<li>torch.distributed._composable.contract</li>\\n</ul>\\n</li>\\n<li>fbgemm-gpu</li>\\n</ol>","autoDesc":true}`);export{Ln as comp,Pn as data};
