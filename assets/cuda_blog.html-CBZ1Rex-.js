import{_ as e}from"./plugin-vue_export-helper-DlAUqK2U.js";import{o,c,e as l}from"./app-Coh1oo3x.js";const i="/assets/image.IFC8T1-C5hhRmI7.png",t="/assets/image-20221009170046118-lhImaK9c.png",d="/assets/image-20221009170314775-B5coRsTt.png",n="/assets/image-20221009170518054-DKGvzJSu.png",r="/assets/image-20221009171236619-KPcJOQDA.png",a="/assets/image-20221009172516961-CwDhPdAn.png",s="/assets/image-20221009173359550-V6E9q-Z8.png",g={},p=l('<h1 id="cuda学习笔记" tabindex="-1"><a class="header-anchor" href="#cuda学习笔记"><span>CUDA学习笔记</span></a></h1><h2 id="_0-目标" tabindex="-1"><a class="header-anchor" href="#_0-目标"><span>0 目标</span></a></h2><ul><li>快速掌握<code>CUDA</code>编程的大致方式</li><li>了解<code>CUDA</code>软硬件组织架构与关系</li><li>了解如何查找文档</li></ul><h2 id="_1-介绍" tabindex="-1"><a class="header-anchor" href="#_1-介绍"><span>1 介绍</span></a></h2><h3 id="_1-1-架构" tabindex="-1"><a class="header-anchor" href="#_1-1-架构"><span>1.1 架构</span></a></h3><ul><li><p><strong>CUDA (Compute Unified Device Architecture)</strong>：统一计算设备架构，在 GPU 上发布的一个新的硬件和软件架构,它不需要映射到一个图形 API 便可在 GPU 上管理和进行并行数据计算</p></li><li><p><strong>CPU、GPU架构对比</strong>：GPU 被设计用于高密度和并行计算,更确切地说是用于图形渲染，因此更多的晶体管被投入到数据处理而不是数据缓存和流量控制<img src="'+i+'" alt="image.IFC8T1" loading="lazy"></p></li><li><p><strong>CUDA软件堆栈</strong>：CUDA 软件堆栈由几层组成:一个硬件驱动程序,一个应用程序编程接口(API)和它的Runtime, 还有二个高级的通用数学库,CUFFT 和CUBLAS<img src="'+t+'" alt="image-20221009170046118" loading="lazy"></p></li><li><p><strong>CUDA内存操作</strong>：CUDA 提供一般 DRAM 内存寻址方式: Gather 和 Scatter内存操作，它可以在 DRAM的任何区域进行读写数据的操作<img src="'+d+'" alt="image-20221009170314775" loading="lazy"></p></li><li><p>允许并行数据缓冲或者在 On-chip 内存共享使数据更接近ALU，可以进行快速的常规读写存取,在线程之间共享数据。应用程序可以最小化数据到 DRAM 的 overfetch 和 round-trips, 从而减少对 DRAM 内存带宽的依赖<img src="'+n+'" alt="image-20221009170518054" loading="lazy"></p></li></ul><h3 id="_1-2-编程模型" tabindex="-1"><a class="header-anchor" href="#_1-2-编程模型"><span>1.2 编程模型</span></a></h3><ul><li><p><strong>Kernel</strong>：一个被执行许多次不同数据的应用程序部分,可以被分离成为一个有很多不同线程在设备上执行的函数，最终被编译成设备的指令集</p></li><li><p><strong>DMA</strong>：主机和设备使用它们自己的 DRAM ,主机内存和设备内存。并可以通过利用设备高性能直接内存存取 (<code>DMA</code>)的引擎( <code>API </code>)从一个 <code>DRAM</code> 复制数据到其他 <code>DRAM</code></p></li><li><p><strong>线程批处理</strong>：线程批处理就是执行一个被组织成许多线程块的 <code>Kernel</code>，主机发送一个连续的 <code>kernel</code> 调用到设备。每个 <code>kernel </code>作为一个由线程块组成的批处理线程来执行<img src="'+r+'" alt="image-20221009171236619" loading="lazy"></p></li><li><p><strong>Block</strong>：包含多个线程，线程间共享数据，可在<code>Kernel</code>中指定同步点，一个块里的线程被挂起直到它们所有都到达同步点，每个线程拥有<code>Block</code>内的<code>Thread ID</code></p></li><li><p><strong>Grid</strong>：执行同一个 <code>Kernel</code> 的块可以合成为一个<code>Grid</code>，同一个<code>Grid</code>中的不同<code>Block</code>中的线程不能通讯和同步，每个<code>Block</code>有对应的<code>Block ID</code></p></li></ul><h3 id="_1-3-内存模型" tabindex="-1"><a class="header-anchor" href="#_1-3-内存模型"><span>1.3 内存模型</span></a></h3><ul><li><p>线程允许访问的内存空间：</p><ul><li><strong>读写</strong>每条线程的<code>寄存器</code></li><li><strong>读写</strong>每条线程的<code>本地内存</code></li><li><strong>读写</strong>每个<code>Block</code>的<code>共享内存</code></li><li><strong>读写</strong>每个<code>Grid</code>的<code>全局内存</code></li><li><strong>只读</strong>每个<code>Grid</code>的<code>常量内存</code></li><li><strong>只读</strong>每个<code>Grid</code>的<code>纹理内存</code></li></ul></li><li><figure><img src="'+a+'" alt="image-20221009172516961" tabindex="0" loading="lazy"><figcaption>image-20221009172516961</figcaption></figure></li><li><p>全局,常量,和纹理内存空间可以通过主机或者同一应用程序持续的通过 kernel 调用来完成读取或写入</p></li><li><p>全局,常量,和纹理内存空间对不同内存的用法加以优化。纹理内存同样提供不同的寻址模式,也为一些特殊的数据格式进行数据过滤</p></li></ul><h3 id="_1-4-硬件实现" tabindex="-1"><a class="header-anchor" href="#_1-4-硬件实现"><span>1.4 硬件实现</span></a></h3><ul><li><strong>SIMD</strong>：单指令多数据，在给定时钟周期内，多处理器的每个处理器执行同一指令，操作不同数据，并行数据缓存或共享内存,被所有处理器共享实现内存空间共享，一个<code>Block</code>只被一个多处理器处理<img src="'+s+'" alt="image-20221009173359550" loading="lazy"></li><li><strong>Active</strong>：被一个多处理器执行的<code>Block</code>,被称作<code> active</code></li><li><strong>Warp</strong>：每个<code>active</code>被划分为多个<code>SIMD</code>方式的线程组，每一组称为一个<code>warp</code>，每个<code>warp</code>大小相同，线程调度程序周期性地从一个<code>warp</code>切换到另一个<code>warp</code></li></ul><h2 id="_2-api" tabindex="-1"><a class="header-anchor" href="#_2-api"><span>2 API</span></a></h2><ul><li>C语言扩展： <ul><li>函数类型限定：指定函数执行的位置和调用位置 <ul><li><code>__device__</code> : 设备上执行，仅可从设备调用</li><li><code>__global__</code>: 设备上执行，仅可从主机调用</li><li><code>__host__</code>: 主机上执行，仅可从主机调用</li></ul></li><li>变量类型限定：指定设备上变量的内存位置 <ul><li><code>__device__</code>：驻留在设备</li><li><code>__constant__</code>: 驻留在常量内存空间</li><li><code>__shared__</code>: 驻留在线程块的共享内存空间中</li><li>只能通过设备代码中的<code>__device__</code>, <code>__shared__</code>或<code>__constant__</code>变量来获取地址。<code>__device__</code>或<code>__constant__</code>变量的地址只能通过主机代码获得, <code>cudaGetSymbolAddress()</code></li></ul></li><li>在设备上执行的方式配置、指定<code>Grid</code>和<code>Block</code>的维数，<code>Block</code>和<code>Thread</code>的ID <ul><li>所有<code>__global__</code>函数的调用必须指定执行配置，执行配置定义了通常在设备执行的函数的<code>Grid</code>和<code>Block</code>的维数</li><li><strong>Dg</strong>: <code>dim3</code>类型，指定<code>Grid</code>维数，<code>Dg.x * Dg.y</code> 等于被发送<code>Block</code>的数量</li><li><strong>Db</strong>: <code>dim3</code>类型，指定<code>Block</code>维数，<code>Db.x * Db.y * Db.z</code> 等于每个<code>Block</code>的线程数</li><li><strong>Ns</strong>: <code>size_t</code>类型，指定共享内存大小，可被任何外部数组变量使用，默认为0，可选参数</li><li><strong>S</strong>: <code>cudaStream_t</code>类型，指定相关<code>stream</code>，默认为0，可选参数</li><li>示例： <ul><li>声明：<code>__global__ void Func(float* parameter);</code></li><li>调用：<code>Func&lt;&lt;&lt; Dg, Db, Ns &gt;&gt;&gt;(parameter);</code></li></ul></li></ul></li></ul></li><li>内置变量： <ul><li><strong>gridDim：</strong> <code>dim3</code>类型，指定<code>Grid</code>维数</li><li><strong>blockIdx：</strong> <code>uint3</code>类型，指定<code>Block ID</code></li><li><strong>blockDim：</strong> <code>dim3</code>类型，指定<code>Block</code>维数</li><li><strong>threadIdx：</strong> <code>uint3</code>类型，指定<code>Thread ID</code></li><li>内置变量不允许取得任何地址且不允许赋值到任何内置变量</li></ul></li><li>每个包含CUDA 语言扩展的源文件必须通过CUDA 编译器<code>nvcc</code>编译</li><li></li></ul>',14),_=[p];function h(u,m){return o(),c("div",null,_)}const U=e(g,[["render",h],["__file","cuda_blog.html.vue"]]),C=JSON.parse(`{"path":"/blogs/cuda_blog.html","title":"CUDA学习笔记","lang":"zh-CN","frontmatter":{"date":"2023-11-24T00:00:00.000Z","tag":["CUDA"],"category":["硬件"],"description":"CUDA学习笔记 0 目标 快速掌握CUDA编程的大致方式 了解CUDA软硬件组织架构与关系 了解如何查找文档 1 介绍 1.1 架构 CUDA (Compute Unified Device Architecture)：统一计算设备架构，在 GPU 上发布的一个新的硬件和软件架构,它不需要映射到一个图形 API 便可在 GPU 上管理和进行并行数据计...","head":[["meta",{"property":"og:url","content":"https://bradzhone.github.io/blogs/cuda_blog.html"}],["meta",{"property":"og:site_name","content":"BradZhone's Blog"}],["meta",{"property":"og:title","content":"CUDA学习笔记"}],["meta",{"property":"og:description","content":"CUDA学习笔记 0 目标 快速掌握CUDA编程的大致方式 了解CUDA软硬件组织架构与关系 了解如何查找文档 1 介绍 1.1 架构 CUDA (Compute Unified Device Architecture)：统一计算设备架构，在 GPU 上发布的一个新的硬件和软件架构,它不需要映射到一个图形 API 便可在 GPU 上管理和进行并行数据计..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"article:author","content":"BradZhone"}],["meta",{"property":"article:tag","content":"CUDA"}],["meta",{"property":"article:published_time","content":"2023-11-24T00:00:00.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"CUDA学习笔记\\",\\"image\\":[\\"\\"],\\"datePublished\\":\\"2023-11-24T00:00:00.000Z\\",\\"dateModified\\":null,\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"BradZhone\\",\\"url\\":\\"https://github.com/BradZhone\\"}]}"]]},"headers":[{"level":2,"title":"0 目标","slug":"_0-目标","link":"#_0-目标","children":[]},{"level":2,"title":"1 介绍","slug":"_1-介绍","link":"#_1-介绍","children":[{"level":3,"title":"1.1 架构","slug":"_1-1-架构","link":"#_1-1-架构","children":[]},{"level":3,"title":"1.2 编程模型","slug":"_1-2-编程模型","link":"#_1-2-编程模型","children":[]},{"level":3,"title":"1.3 内存模型","slug":"_1-3-内存模型","link":"#_1-3-内存模型","children":[]},{"level":3,"title":"1.4 硬件实现","slug":"_1-4-硬件实现","link":"#_1-4-硬件实现","children":[]}]},{"level":2,"title":"2 API","slug":"_2-api","link":"#_2-api","children":[]}],"git":{"createdTime":null,"updatedTime":null,"contributors":[]},"readingTime":{"minutes":4.65,"words":1396},"filePathRelative":"blogs/cuda_blog.md","localizedDate":"2023年11月24日","excerpt":"\\n<h2>0 目标</h2>\\n<ul>\\n<li>快速掌握<code>CUDA</code>编程的大致方式</li>\\n<li>了解<code>CUDA</code>软硬件组织架构与关系</li>\\n<li>了解如何查找文档</li>\\n</ul>\\n<h2>1 介绍</h2>\\n<h3>1.1 架构</h3>\\n<ul>\\n<li>\\n<p><strong>CUDA (Compute Unified Device Architecture)</strong>：统一计算设备架构，在 GPU 上发布的一个新的硬件和软件架构,它不需要映射到一个图形 API 便可在 GPU 上管理和进行并行数据计算</p>\\n</li>\\n<li>\\n<p><strong>CPU、GPU架构对比</strong>：GPU 被设计用于高密度和并行计算,更确切地说是用于图形渲染，因此更多的晶体管被投入到数据处理而不是数据缓存和流量控制</p>\\n</li>\\n<li>\\n<p><strong>CUDA软件堆栈</strong>：CUDA 软件堆栈由几层组成:一个硬件驱动程序,一个应用程序编程接口(API)和它的Runtime, 还有二个高级的通用数学库,CUFFT 和CUBLAS</p>\\n</li>\\n<li>\\n<p><strong>CUDA内存操作</strong>：CUDA 提供一般 DRAM 内存寻址方式: Gather 和 Scatter内存操作，它可以在 DRAM的任何区域进行读写数据的操作</p>\\n</li>\\n<li>\\n<p>允许并行数据缓冲或者在 On-chip 内存共享使数据更接近ALU，可以进行快速的常规读写存取,在线程之间共享数据。应用程序可以最小化数据到 DRAM 的 overfetch 和 round-trips, 从而减少对 DRAM 内存带宽的依赖</p>\\n</li>\\n</ul>","autoDesc":true}`);export{U as comp,C as data};
