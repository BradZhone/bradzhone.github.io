import{_ as r}from"./plugin-vue_export-helper-DlAUqK2U.js";import{r as s,o as c,c as p,a as e,b as t,d as n,w as o,e as i}from"./app-Coh1oo3x.js";const d="/assets/learning_rate_and_batch_size-1703232507054-5-FvzUGnj5.png",u={},h=i('<h1 id="dl相关" tabindex="-1"><a class="header-anchor" href="#dl相关"><span>DL相关</span></a></h1><h2 id="_0-wtn" tabindex="-1"><a class="header-anchor" href="#_0-wtn"><span>0.WTN</span></a></h2><ul><li>[ ] Dataloader的worker对训练推理的影响 https://zhuanlan.zhihu.com/p/673642279</li><li>[ ] train &amp; val &amp; inference</li><li>[ ] gradient accumulation</li><li>[ ] mixed precision</li><li>[ ] datasets (type, data structure)</li><li>[ ] fully finetune, finetune, lora, pretrain</li><li>[ ] prompt, 幻读， difficulties</li><li>[ ] llama1,2, chatgpt</li><li>[ ] bert，transformers</li><li>[ ] deepspeed, fastchat, megatron, colossalAi ...</li><li>[ ] torch.run, torch.launch, mp.spawn</li></ul><h2 id="_1-博客-总结" tabindex="-1"><a class="header-anchor" href="#_1-博客-总结"><span>1.博客&amp;总结</span></a></h2>',4),_=e("thead",null,[e("tr",null,[e("th",null,"博客"),e("th",null,"总结")])],-1),m={href:"https://www.zhihu.com/question/64134994/answer/216895968",target:"_blank",rel:"noopener noreferrer"},g=e("code",null,"GD(Gradient Descent)",-1),k=e("br",null,null,-1),b=e("code",null,"SGD(Stochastic Gradient Descent)",-1),f=e("br",null,null,-1),y=e("code",null,"mini-batch SGD",-1),v=e("br",null,null,-1),D=e("br",null,null,-1),z=e("code",null,"mini-batch SGD",-1),w=e("code",null,"SGD",-1),x=e("br",null,null,-1),N=e("br",null,null,-1),T=e("br",null,null,-1),B=e("br",null,null,-1),L=e("br",null,null,-1),P=e("img",{src:d,alt:"learning_rate_and_batch_size",loading:"lazy"},null,-1),G={href:"https://zhuanlan.zhihu.com/p/345681737",target:"_blank",rel:"noopener noreferrer"},S=e("br",null,null,-1),Z={href:"https://zhuanlan.zhihu.com/p/357075502",target:"_blank",rel:"noopener noreferrer"},q=e("td",null,[e("code",null,"requires_grad = False"),t(": 不计算梯度，可freeze部分model训练"),e("br"),e("code",null,"torch.no_grad()"),t(": 所有计算结果都带有"),e("code",null,"requires_grad = False"),t("属性，从使用了"),e("code",null,"with torch.no_grad():"),t("之前的网络层的反向传播都会截断"),e("br"),e("code",null,"model.eval()"),t(":配合"),e("code",null,"torch.no_grad()"),t("将模型设为evaluation模式，将dropout关掉，利用到了所有网络连接，即不进行随机舍弃神经元，保证BN层能够用全部训练数据的均值和方差，即测试过程中要保证BN层的均值和方差不变"),e("br"),e("code",null,"model.train()"),t(":启用batchnorm和dropout，保证BN层能够用到每一批数据的均值和方差，dropout随机取一部分网络连接来训练更新参数")],-1),C={href:"https://blog.csdn.net/hustwayne/article/details/120324639",target:"_blank",rel:"noopener noreferrer"},V=e("td",null,[t("注意不同的方式模型的权重key会不一样导致一些问题"),e("br"),t("TODO：可尝试blog中的所有方法做对比")],-1),W={href:"https://zhuanlan.zhihu.com/p/673642279",target:"_blank",rel:"noopener noreferrer"},A=e("td",null,"基本概念综述",-1),E=i(`<h2 id="_2-经验总结" tabindex="-1"><a class="header-anchor" href="#_2-经验总结"><span>2.经验总结</span></a></h2><ol><li><p>RuntimeError: Found no NVIDIA driver on your system</p><ul><li>查看创建container的参数有没有加<code>--gpus all</code></li></ul></li><li><p>Missing keys &amp; unexpected keys in state_dict</p><ul><li><p>使用DDP时某张卡保存模型<code>torch.save(model.state_dict(), &#39;checkpoint.pth&#39;)</code>，之后使用单卡或在调用<code>torch.nn.parallel.DistributedDataParallel(model, device_ids=[local_rank])</code>封装model之前使用<code>model.load_state_dict(torch.load(&#39;checkpoint.pth&#39;))</code>则会出现state_dict中的key多了一个module字段导致该问题</p></li><li><p>可将<code>load_state_dict</code>放到<code>DistributedDataParallel</code>调用以后再执行（适用于多卡情况）</p></li><li><p>或是直接加载stat_dict之后删除这个字段：</p><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code>stat_dict <span class="token operator">=</span> torch<span class="token punctuation">.</span>load<span class="token punctuation">(</span>args<span class="token punctuation">.</span>pretrained<span class="token punctuation">,</span> map_location<span class="token operator">=</span>torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">&#39;cpu&#39;</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span>load_state_dict<span class="token punctuation">(</span><span class="token punctuation">{</span>k<span class="token punctuation">.</span>replace<span class="token punctuation">(</span><span class="token string">&#39;module.&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;&#39;</span><span class="token punctuation">)</span><span class="token punctuation">:</span> v <span class="token keyword">for</span> k<span class="token punctuation">,</span> v <span class="token keyword">in</span> stat_dict<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">}</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div></li><li><p>若是不同计算设备加载其它计算设备保存的模型，则需加上参数<code>torch.load(args.pretrained, map_location=torch.device(device))</code>，此处的device可以先设为cpu，之后再<code>model.to(&quot;cuda&quot;)</code></p></li></ul></li></ol>`,2);function F(I,O){const a=s("ExternalLinkIcon"),l=s("font");return c(),p("div",null,[h,e("table",null,[_,e("tbody",null,[e("tr",null,[e("td",null,[e("a",m,[t("如何理解深度学习分布式训练中的large batch size与learning rate的关系？"),n(a)])]),e("td",null,[g,t(": 基于整个训练集计算梯度更新参数； "),n(l,{color:"green"},{default:o(()=>[t("优点：")]),_:1}),t("梯度估计准确，更新过程准确 "),n(l,{color:"red"},{default:o(()=>[t("缺点：")]),_:1}),t("计算耗时，容易落入局部最优"),k,b,t(": 基于单个sample计算梯度更新参数；"),n(l,{color:"green"},{default:o(()=>[t("优点：")]),_:1}),t("适合online-learning场景 "),n(l,{color:"red"},{default:o(()=>[t("缺点：")]),_:1}),t("单个sample梯度不够准确，需要很小的lr，难以占满CPU/GPU使用率，资源浪费"),f,y,t(": 基于batchsize个samples计算梯度更新参数"),n(l,{color:"green"},{default:o(()=>[t("优点：")]),_:1}),t("折中,引入梯度噪声，不易落入局部最优"),v,D,z,t("的batchsize增大m倍，相当于将梯度的方差减少m倍，因此梯度比"),w,t("更加准确"),x,t("为了保证相同的数据量利用率, large batchsize下的学习率应当是baseline的k倍，提高收敛速度和精度"),N,t("在初始训练阶段，一般不会直接将lr增大为k倍，而是从baseline的lr慢慢warmup到k倍"),T,t("lr不能无限大，lr太大直接沿loss切线跑得太远，导致收敛出现问题"),B,L,t("batchsize越大，收敛方向的confidence越大，前进方向更加坚定，因此调大学习率；小的batchsize显得杂乱无规律，需要小的学习率保证不出错。参考：https://oldpan.me/archives/how-to-use-tricks-to-train-network"),P,t("参考：https://miguel-data-sc.github.io/2017-11-05-first/")])]),e("tr",null,[e("td",null,[e("a",G,[t("Pytorch requires_grad torch.no_grad() .eval()"),n(a)]),S,e("a",Z,[t("Pytorch：model.train()和model.eval()用法和区别，以及model.eval()和torch.no_grad()的区别"),n(a)])]),q]),e("tr",null,[e("td",null,[e("a",C,[t("torch DDP训练-模型保存-加载问题"),n(a)])]),V]),e("tr",null,[e("td",null,[e("a",W,[t("一文说尽「大模型推理」！12家高校机构联合发布150页报告，综述750篇论文"),n(a)])]),A])])]),E])}const M=r(u,[["render",F],["__file","deep_learning.html.vue"]]),R=JSON.parse(`{"path":"/notes/deep_learning.html","title":"DL相关","lang":"zh-CN","frontmatter":{"date":"2023-12-29T00:00:00.000Z","tag":["DeepLearning"],"category":["DeepLearning"],"description":"DL相关 0.WTN [ ] Dataloader的worker对训练推理的影响 https://zhuanlan.zhihu.com/p/673642279 [ ] train & val & inference [ ] gradient accumulation [ ] mixed precision [ ] datasets (type, dat...","head":[["meta",{"property":"og:url","content":"https://bradzhone.github.io/notes/deep_learning.html"}],["meta",{"property":"og:site_name","content":"BradZhone's Blog"}],["meta",{"property":"og:title","content":"DL相关"}],["meta",{"property":"og:description","content":"DL相关 0.WTN [ ] Dataloader的worker对训练推理的影响 https://zhuanlan.zhihu.com/p/673642279 [ ] train & val & inference [ ] gradient accumulation [ ] mixed precision [ ] datasets (type, dat..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"article:author","content":"BradZhone"}],["meta",{"property":"article:tag","content":"DeepLearning"}],["meta",{"property":"article:published_time","content":"2023-12-29T00:00:00.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"DL相关\\",\\"image\\":[\\"\\"],\\"datePublished\\":\\"2023-12-29T00:00:00.000Z\\",\\"dateModified\\":null,\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"BradZhone\\",\\"url\\":\\"https://github.com/BradZhone\\"}]}"]]},"headers":[{"level":2,"title":"0.WTN","slug":"_0-wtn","link":"#_0-wtn","children":[]},{"level":2,"title":"1.博客&总结","slug":"_1-博客-总结","link":"#_1-博客-总结","children":[]},{"level":2,"title":"2.经验总结","slug":"_2-经验总结","link":"#_2-经验总结","children":[]}],"git":{"createdTime":null,"updatedTime":null,"contributors":[]},"readingTime":{"minutes":3.1,"words":931},"filePathRelative":"notes/deep_learning.md","localizedDate":"2023年12月29日","excerpt":"\\n<h2>0.WTN</h2>\\n<ul>\\n<li>[ ] Dataloader的worker对训练推理的影响 https://zhuanlan.zhihu.com/p/673642279</li>\\n<li>[ ] train &amp; val &amp; inference</li>\\n<li>[ ] gradient accumulation</li>\\n<li>[ ] mixed precision</li>\\n<li>[ ] datasets (type, data structure)</li>\\n<li>[ ] fully finetune, finetune, lora, pretrain</li>\\n<li>[ ] prompt, 幻读， difficulties</li>\\n<li>[ ] llama1,2, chatgpt</li>\\n<li>[ ] bert，transformers</li>\\n<li>[ ] deepspeed, fastchat, megatron, colossalAi ...</li>\\n<li>[ ] torch.run, torch.launch, mp.spawn</li>\\n</ul>","autoDesc":true}`);export{M as comp,R as data};
