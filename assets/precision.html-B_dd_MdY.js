import{_ as i}from"./plugin-vue_export-helper-DlAUqK2U.js";import{o as t,c as e,e as n}from"./app-Coh1oo3x.js";const a="/assets/AI_training_TF32_tensor_cores_F2-625x371-T_qMy9U3.png",o="/assets/EXX-blog-fp64-fp32-fp-16-3-DP1YoOkK.jpg",r="/assets/EXX-blog-fp64-fp32-fp-16-2-El9OatKC.jpg",s="/assets/AI_training_TF32_tensor_cores_F5-1024x292-OsJiKx2G.png",p="/assets/EXX-blog-fp64-fp32-fp-16-4-DAs6wmGk.jpg",c="/assets/EXX-blog-fp64-fp32-fp-16-5_(1)-D5lyRa7f.jpg",l={},g=n('<h1 id="floating-point-precision-formats" tabindex="-1"><a class="header-anchor" href="#floating-point-precision-formats"><span>Floating Point precision formats</span></a></h1><h2 id="" tabindex="-1"><a class="header-anchor" href="#"><span><img src="'+a+'" alt="Breakdowns of sign, range and mantissa bits for common DL precision formats." loading="lazy"></span></a></h2><p>参考：https://developer.nvidia.com/blog/accelerating-ai-training-with-tf32-tensor-cores/</p><p>在深度学习中，范围比精度更重要。</p><h2 id="fp64" tabindex="-1"><a class="header-anchor" href="#fp64"><span>FP64</span></a></h2><figure><img src="'+o+'" alt="img" tabindex="0" loading="lazy"><figcaption>img</figcaption></figure><p>双精度</p><h2 id="fp32" tabindex="-1"><a class="header-anchor" href="#fp32"><span>FP32</span></a></h2><figure><img src="'+r+'" alt="img" tabindex="0" loading="lazy"><figcaption>img</figcaption></figure><p>单精度</p><h2 id="tf32" tabindex="-1"><a class="header-anchor" href="#tf32"><span>TF32</span></a></h2><p>用于A100架构TensorCore，指数范围i与FP32相同，尾数范围与FP16相同。兼容FP32和FP16，只需截断就可相互转换。先截断为TF32计算再转为FP32对历史工作无影响，且无需更改代码即可使用。</p><p>更少bit的尾数意味着所需要的乘法器位宽更低，即可以实现更小的芯片面积或更高的计算密度。</p><figure><img src="'+s+'" alt="img" tabindex="0" loading="lazy"><figcaption>img</figcaption></figure><h2 id="fp16" tabindex="-1"><a class="header-anchor" href="#fp16"><span>FP16</span></a></h2><figure><img src="'+p+'" alt="img" tabindex="0" loading="lazy"><figcaption>img</figcaption></figure><p>半精度</p><h2 id="bf16" tabindex="-1"><a class="header-anchor" href="#bf16"><span>BF16</span></a></h2><figure><img src="'+c+'" alt="img" tabindex="0" loading="lazy"><figcaption>img</figcaption></figure><p>Brain Floating Point Format， 谷歌提出，在FP16基础上提升指数范围，减少尾数范围</p>',20),f=[g];function h(d,m){return t(),e("div",null,f)}const F=i(l,[["render",h],["__file","precision.html.vue"]]),b=JSON.parse(`{"path":"/notes/precision.html","title":"Floating Point precision formats","lang":"zh-CN","frontmatter":{"date":"2024-02-21T00:00:00.000Z","tag":["precision"],"category":["DeepLearning"],"description":"Floating Point precision formats Breakdowns of sign, range and mantissa bits for common DL precision formats. 参考：https://developer.nvidia.com/blog/accelerating-ai-training-with-...","head":[["meta",{"property":"og:url","content":"https://bradzhone.github.io/notes/precision.html"}],["meta",{"property":"og:site_name","content":"BradZhone's Blog"}],["meta",{"property":"og:title","content":"Floating Point precision formats"}],["meta",{"property":"og:description","content":"Floating Point precision formats Breakdowns of sign, range and mantissa bits for common DL precision formats. 参考：https://developer.nvidia.com/blog/accelerating-ai-training-with-..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"article:author","content":"BradZhone"}],["meta",{"property":"article:tag","content":"precision"}],["meta",{"property":"article:published_time","content":"2024-02-21T00:00:00.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"Floating Point precision formats\\",\\"image\\":[\\"\\"],\\"datePublished\\":\\"2024-02-21T00:00:00.000Z\\",\\"dateModified\\":null,\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"BradZhone\\",\\"url\\":\\"https://github.com/BradZhone\\"}]}"]]},"headers":[{"level":2,"title":"","slug":"","link":"#","children":[]},{"level":2,"title":"FP64","slug":"fp64","link":"#fp64","children":[]},{"level":2,"title":"FP32","slug":"fp32","link":"#fp32","children":[]},{"level":2,"title":"TF32","slug":"tf32","link":"#tf32","children":[]},{"level":2,"title":"FP16","slug":"fp16","link":"#fp16","children":[]},{"level":2,"title":"BF16","slug":"bf16","link":"#bf16","children":[]}],"git":{"createdTime":null,"updatedTime":null,"contributors":[]},"readingTime":{"minutes":0.78,"words":234},"filePathRelative":"notes/precision.md","localizedDate":"2024年2月21日","excerpt":"\\n<h2></h2>\\n<p>参考：https://developer.nvidia.com/blog/accelerating-ai-training-with-tf32-tensor-cores/</p>\\n<p>在深度学习中，范围比精度更重要。</p>\\n<h2>FP64</h2>\\n<figure><figcaption>img</figcaption></figure>\\n<p>双精度</p>\\n<h2>FP32</h2>\\n<figure><figcaption>img</figcaption></figure>\\n<p>单精度</p>\\n<h2>TF32</h2>\\n<p>用于A100架构TensorCore，指数范围i与FP32相同，尾数范围与FP16相同。兼容FP32和FP16，只需截断就可相互转换。先截断为TF32计算再转为FP32对历史工作无影响，且无需更改代码即可使用。</p>","autoDesc":true}`);export{F as comp,b as data};
