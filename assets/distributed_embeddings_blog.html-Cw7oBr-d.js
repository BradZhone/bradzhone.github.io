import{_ as e}from"./plugin-vue_export-helper-DlAUqK2U.js";import{r as p,o,c as l,a as n,b as a,d as i,e as s}from"./app-Coh1oo3x.js";const r="/assets/merlin-distributed-embeddings-featured-BT7QQGSX.png",c={},d=n("h1",{id:"distributed-embeddings",tabindex:"-1"},[n("a",{class:"header-anchor",href:"#distributed-embeddings"},[n("span",null,"Distributed_embeddings")])],-1),u=n("h2",{id:"_1-introduce",tabindex:"-1"},[n("a",{class:"header-anchor",href:"#_1-introduce"},[n("span",null,"1. Introduce")])],-1),m=s('<li><p>参考链接：</p><ul><li>项目地址：https://github.com/NVIDIA-Merlin/distributed-embeddings</li><li>相关blog：https://developer.nvidia.com/blog/fast-terabyte-scale-recommender-training-made-easy-with-nvidia-merlin-distributed-embeddings/</li><li>相关项目：https://github.com/NVIDIA/DeepLearningExamples/tree/master/TensorFlow2/Recommendation/DLRM#hybrid-parallel-training-with-merlin-distributed-embeddings</li><li>项目文档：https://nvidia-merlin.github.io/distributed-embeddings/index.html</li></ul></li><li><p>基于TF2构建大embedding，提供可伸缩模型并行封装器，能够自动将嵌入表分布到多GPU上（目前支持table-wise和column-wise）</p></li><li><p>支持混合模型并行（dist_model_parallel）：</p><figure><img src="'+r+`" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure></li><li><p>仅需修改少量代码即可使用</p><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code><span class="token keyword">import</span> dist_model_parallel <span class="token keyword">as</span> dmp
 
<span class="token keyword">class</span> <span class="token class-name">MyEmbeddingModel</span><span class="token punctuation">(</span>tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>Model<span class="token punctuation">)</span><span class="token punctuation">:</span>
  <span class="token keyword">def</span>  <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> table_sizes<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
    self<span class="token punctuation">.</span>embedding_layers <span class="token operator">=</span> <span class="token punctuation">[</span>tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>input_dim<span class="token punctuation">,</span> output_dim<span class="token punctuation">)</span> <span class="token keyword">for</span> input_dim<span class="token punctuation">,</span> output_dim <span class="token keyword">in</span> table_sizes<span class="token punctuation">]</span>
    <span class="token comment"># 1. Add this line to wrap list of embedding layers used in the model</span>
    self<span class="token punctuation">.</span>embedding_layers <span class="token operator">=</span> dmp<span class="token punctuation">.</span>DistributedEmbedding<span class="token punctuation">(</span>self<span class="token punctuation">.</span>embedding_layers<span class="token punctuation">)</span>
  <span class="token keyword">def</span> <span class="token function">call</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> inputs<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># embedding_outputs = [e(i) for e, i in zip(self.embedding_layers, inputs)]</span>
    embedding_outputs <span class="token operator">=</span> self<span class="token punctuation">.</span>embedding_layers<span class="token punctuation">(</span>inputs<span class="token punctuation">)</span>
     
<span class="token decorator annotation punctuation">@tf<span class="token punctuation">.</span>function</span>
<span class="token keyword">def</span> <span class="token function">training_step</span><span class="token punctuation">(</span>inputs<span class="token punctuation">,</span> labels<span class="token punctuation">,</span> first_batch<span class="token punctuation">)</span><span class="token punctuation">:</span>
  <span class="token keyword">with</span> tf<span class="token punctuation">.</span>GradientTape<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">as</span> tape<span class="token punctuation">:</span>
    probs <span class="token operator">=</span> model<span class="token punctuation">(</span>inputs<span class="token punctuation">)</span>
    loss_value <span class="token operator">=</span> loss<span class="token punctuation">(</span>labels<span class="token punctuation">,</span> probs<span class="token punctuation">)</span>
 
  <span class="token comment"># 2. Change Horovod Gradient Tape to dmp tape</span>
  <span class="token comment"># tape = hvd.DistributedGradientTape(tape)</span>
  tape <span class="token operator">=</span> dmp<span class="token punctuation">.</span>DistributedGradientTape<span class="token punctuation">(</span>tape<span class="token punctuation">)</span>
  grads <span class="token operator">=</span> tape<span class="token punctuation">.</span>gradient<span class="token punctuation">(</span>loss_value<span class="token punctuation">,</span> model<span class="token punctuation">.</span>trainable_variables<span class="token punctuation">)</span>
  opt<span class="token punctuation">.</span>apply_gradients<span class="token punctuation">(</span><span class="token builtin">zip</span><span class="token punctuation">(</span>grads<span class="token punctuation">,</span> model<span class="token punctuation">.</span>trainable_variables<span class="token punctuation">)</span><span class="token punctuation">)</span>
 
  <span class="token keyword">if</span> first_batch<span class="token punctuation">:</span>
    <span class="token comment"># 3. Change Horovod broadcast_variables to dmp&#39;s</span>
    <span class="token comment"># hvd.broadcast_variables(model.variables, root_rank=0)</span>
    dmp<span class="token punctuation">.</span>broadcast_variables<span class="token punctuation">(</span>model<span class="token punctuation">.</span>variables<span class="token punctuation">,</span> root_rank<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
  <span class="token keyword">return</span> loss_value
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></li><li><p>示例</p><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code><span class="token keyword">import</span> tensorflow <span class="token keyword">as</span> tf
<span class="token keyword">import</span> distributed_embeddings<span class="token punctuation">.</span>python<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>dist_model_parallel <span class="token keyword">as</span> dmp
<span class="token keyword">from</span> distributed_embeddings<span class="token punctuation">.</span>python<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>dist_model_parallel <span class="token keyword">import</span> Embedding<span class="token punctuation">,</span> DistributedEmbedding
layer0 <span class="token operator">=</span> Embedding<span class="token punctuation">(</span><span class="token number">1000</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token string">&quot;emb_0&quot;</span><span class="token punctuation">)</span>
layer1 <span class="token operator">=</span> Embedding<span class="token punctuation">(</span><span class="token number">1000</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">,</span> combiner<span class="token operator">=</span><span class="token string">&quot;mean&quot;</span><span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token string">&quot;emb_1&quot;</span><span class="token punctuation">)</span>
layer2 <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span><span class="token number">1000</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token string">&quot;emb_2&quot;</span><span class="token punctuation">)</span>
layer3 <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span><span class="token number">1000</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token string">&quot;emb_3&quot;</span><span class="token punctuation">)</span>
layers0 <span class="token operator">=</span> <span class="token punctuation">[</span>layer0<span class="token punctuation">,</span> layer1<span class="token punctuation">]</span>
layers1 <span class="token operator">=</span> <span class="token punctuation">[</span>layer2<span class="token punctuation">,</span> layer3<span class="token punctuation">]</span>
emb_layers0 <span class="token operator">=</span> DistributedEmbedding<span class="token punctuation">(</span>layers0<span class="token punctuation">,</span> column_slice_threshold<span class="token operator">=</span><span class="token number">32000</span><span class="token punctuation">)</span>
emb_layers1 <span class="token operator">=</span> DistributedEmbedding<span class="token punctuation">(</span>layers1<span class="token punctuation">,</span> column_slice_threshold<span class="token operator">=</span><span class="token number">32000</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></li>`,5),k=s("<p>API:</p><ul><li><p>distributed_embeddings.python.layers.embedding.</p><p>Embedding</p><ul><li>接口基本上对齐 tf.keras.layers.Embedding，且增加了支持同时lookup多个embedding 再将它们combine（sum/mean）为一个vector输出</li><li>支持多种数据类型：onehot/fixed hotness (multihot, 输入tensor维度相同)/variable hotness（multihot， 输入tensor维度不同，使用tf.RaggedTensor）/sparse tensor(multihot, 使用tf.sparse.SparseTensor，按照csr格式存储稀疏tensor)</li></ul></li><li><p>distributed_embeddings.python.layers.dist_model_parallel.</p><p>DistributedEmbedding</p><ul><li>支持按列切分embedding table，适用于emb table规模庞大且无法存入单张卡内存的情况</li><li>可设置column_slice_threshold参数确定切分阈值（elements num）</li><li>可封装共享embedding table，如两个特征域，一个表示watched_video, 一个表示browsed_video，他们的特征都是videos,因此可公用同一个emb table</li></ul></li></ul><p>性能表现：</p>",3),b={href:"https://labs.criteo.com/2013/12/download-terabyte-click-logs/",target:"_blank",rel:"noopener noreferrer"},g=s('<table><thead><tr><th>Hardware</th><th>Description</th><th>Training Throughput (samples/second)</th><th>Speedup over CPU</th></tr></thead><tbody><tr><td><strong>2 x AMD EPYC 7742</strong></td><td>Both MLP layers and embeddings on CPU</td><td>17.7k</td><td>1x</td></tr><tr><td><strong>1 x A100-80GB</strong>; 2 x AMD EPYC 7742</td><td>Large embeddings on CPU, everything else on GPU</td><td>768k</td><td>43x</td></tr><tr><td><strong>DGX A100 (8xA100-80GB)</strong></td><td>Hybrid parallel with NVIDIA Merlin <strong>Distributed-Embeddings,</strong> whole model on GPU</td><td>12.1M</td><td><strong>683x</strong></td></tr></tbody></table><p>Synthetic models benchmark （见第三节test部分benchmarks测试）</p><ul><li>模型规模定义</li></ul><table><thead><tr><th>Model</th><th>Total number of embedding tables</th><th>Total embedding size (GiB)</th></tr></thead><tbody><tr><td>Tiny</td><td>55</td><td>4.2</td></tr><tr><td>Small</td><td>107</td><td>26.3</td></tr><tr><td>Medium</td><td>311</td><td>206.2</td></tr><tr><td>Large</td><td>612</td><td>773.8</td></tr><tr><td>Jumbo</td><td>1,022</td><td>3,109.5</td></tr></tbody></table><ul><li>官方性能(DGX-A100-80GB, batchsize=65536, optimizer=adagrad )</li></ul><table><thead><tr><th style="text-align:center;"><strong>Model</strong></th><th style="text-align:center;"><strong>Training step time (ms)</strong></th><th style="text-align:center;"><strong>Training step time (ms)</strong></th><th style="text-align:center;"><strong>Training step time (ms)</strong></th><th style="text-align:center;"><strong>Training step time (ms)</strong></th><th><strong>Training step time (ms)</strong></th></tr></thead><tbody><tr><td style="text-align:center;"><strong>Model</strong></td><td style="text-align:center;"><strong>1 GPU</strong></td><td style="text-align:center;"><strong>8 GPU</strong></td><td style="text-align:center;"><strong>16 GPU</strong></td><td style="text-align:center;"><strong>32 GPU</strong></td><td><strong>128 GPU</strong></td></tr><tr><td style="text-align:center;"><strong>Tiny</strong></td><td style="text-align:center;">17.6</td><td style="text-align:center;">3.6</td><td style="text-align:center;">3.2</td><td style="text-align:center;"></td><td></td></tr><tr><td style="text-align:center;"><strong>Small</strong></td><td style="text-align:center;">57.8</td><td style="text-align:center;">14.0</td><td style="text-align:center;">11.6</td><td style="text-align:center;">7.4</td><td></td></tr><tr><td style="text-align:center;"><strong>Medium</strong></td><td style="text-align:center;"></td><td style="text-align:center;">64.4</td><td style="text-align:center;">44.9</td><td style="text-align:center;">31.1</td><td>17.2</td></tr><tr><td style="text-align:center;"><strong>Large</strong></td><td style="text-align:center;"></td><td style="text-align:center;"></td><td style="text-align:center;"></td><td style="text-align:center;">65.0</td><td>33.4</td></tr><tr><td style="text-align:center;"><strong>Jumbo</strong></td><td style="text-align:center;"></td><td style="text-align:center;"></td><td style="text-align:center;"></td><td style="text-align:center;"></td><td>102.3</td></tr></tbody></table><ul><li>实测性能(Tesla T4, batchsize=65536, optimizer=adagrad)</li></ul><table><thead><tr><th style="text-align:center;"><strong>Model</strong></th><th style="text-align:center;"><strong>Training step time (ms)</strong></th><th><strong>Training step time (ms)</strong></th></tr></thead><tbody><tr><td style="text-align:center;"><strong>Model</strong></td><td style="text-align:center;">1 GPU</td><td>4 GPU</td></tr><tr><td style="text-align:center;"><strong>Tiny</strong></td><td style="text-align:center;">42.703</td><td>82.856</td></tr></tbody></table><ul><li>对比TF原生数据并行</li></ul><table><thead><tr><th><strong>Solution</strong></th><th style="text-align:center;"><strong>Training step time (ms)</strong></th><th><strong>Training step time (ms)</strong></th><th><strong>Training step time (ms)</strong></th><th><strong>Training step time (ms)</strong></th></tr></thead><tbody><tr><td><strong>Solution</strong></td><td style="text-align:center;"><strong>1 GPU</strong></td><td><strong>2 GPU</strong></td><td><strong>4 GPU</strong></td><td><strong>8 GPU</strong></td></tr><tr><td>NVIDIA Merlin Distributed Embeddings <strong>Model Parallel</strong></td><td style="text-align:center;">17.7</td><td><strong>11.6</strong></td><td><strong>6.4</strong></td><td><strong>4.2</strong></td></tr><tr><td>Native TensorFlow <strong>Data Parallel</strong></td><td style="text-align:center;">19.9</td><td>20.2</td><td>21.2</td><td>22.3</td></tr></tbody></table>',10),v=s(`<h2 id="_2-build" tabindex="-1"><a class="header-anchor" href="#_2-build"><span>2. build</span></a></h2><ul><li><p>docker</p><div class="language-bash line-numbers-mode" data-ext="sh" data-title="sh"><pre class="language-bash"><code><span class="token function">docker</span> run <span class="token parameter variable">-it</span> <span class="token punctuation">\\</span>
    <span class="token parameter variable">--name</span><span class="token operator">=</span><span class="token string">&quot;distributed_embedding&quot;</span> <span class="token punctuation">\\</span>
    <span class="token parameter variable">--net</span> <span class="token function">host</span> <span class="token punctuation">\\</span>
    <span class="token parameter variable">--gpus</span> all <span class="token punctuation">\\</span>
    <span class="token parameter variable">-v</span> /data:/data <span class="token punctuation">\\</span>
    <span class="token parameter variable">-v</span> /tmp:/tmp <span class="token punctuation">\\</span>
    <span class="token parameter variable">-v</span> /home:/home <span class="token punctuation">\\</span>
    --shm-size <span class="token string">&#39;64gb&#39;</span> <span class="token punctuation">\\</span>
    <span class="token parameter variable">--privileged</span> nvcr.io/nvidia/tensorflow:22.12-tf2-py3 /bin/bash
    
<span class="token function">docker</span> run <span class="token parameter variable">-it</span> <span class="token punctuation">\\</span>
    <span class="token parameter variable">--name</span><span class="token operator">=</span><span class="token string">&quot;distributed_embedding&quot;</span> <span class="token punctuation">\\</span>
    <span class="token parameter variable">--net</span> <span class="token function">host</span> <span class="token punctuation">\\</span>
    <span class="token parameter variable">--gpus</span> all <span class="token punctuation">\\</span>
    <span class="token parameter variable">-v</span> /data:/data <span class="token punctuation">\\</span>
    <span class="token parameter variable">-v</span> /tmp:/tmp <span class="token punctuation">\\</span>
    <span class="token parameter variable">-v</span> /home:/home <span class="token punctuation">\\</span>
    --shm-size <span class="token string">&#39;64gb&#39;</span> <span class="token punctuation">\\</span>
    <span class="token parameter variable">--privileged</span> nvcr.io/nvidia/tensorflow:23.03-tf2-py3 /bin/bash
    

</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></li><li><p>build</p><div class="language-bash line-numbers-mode" data-ext="sh" data-title="sh"><pre class="language-bash"><code><span class="token builtin class-name">cd</span> /workspace
<span class="token function">git</span> clone https://github.com/NVIDIA-Merlin/distributed-embeddings.git
<span class="token builtin class-name">cd</span> /workspace/distributed-embeddings
<span class="token function">git</span> submodule update <span class="token parameter variable">--init</span> <span class="token parameter variable">--recursive</span>
<span class="token function">make</span> pip_pkg <span class="token operator">&amp;&amp;</span> pip <span class="token function">install</span> artifacts/*.whl
<span class="token builtin class-name">export</span> <span class="token assign-left variable">HOROVOD_GPU_OPERATIONS</span><span class="token operator">=</span>NCCL <span class="token operator">&amp;&amp;</span> pip <span class="token function">install</span> <span class="token assign-left variable">horovod</span><span class="token operator">==</span><span class="token number">0.27</span>
python <span class="token parameter variable">-c</span> <span class="token string">&quot;import distributed_embeddings&quot;</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></li></ul><h2 id="_2-test-benchmark" tabindex="-1"><a class="header-anchor" href="#_2-test-benchmark"><span>2. test&amp;benchmark</span></a></h2><ul><li><p>API测试</p><div class="language-bash line-numbers-mode" data-ext="sh" data-title="sh"><pre class="language-bash"><code><span class="token comment">#test</span>
<span class="token builtin class-name">cd</span> /workspace/distributed-embeddings/tests
python dist_model_parallel_test.py
python embedding_test.py
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></li><li><p>benchmarks测试</p><div class="language-bash line-numbers-mode" data-ext="sh" data-title="sh"><pre class="language-bash"><code><span class="token comment">#benchmarks</span>
<span class="token builtin class-name">cd</span> /workspace/distributed-embeddings/examples/benchmarks/synthetic_models
<span class="token comment"># single gpu</span>
python main.py <span class="token parameter variable">--model</span> tiny <span class="token parameter variable">--optimizer</span> adagrad <span class="token parameter variable">--batch_size</span> <span class="token number">65536</span>
<span class="token comment"># multiple gpus</span>
horovodrun <span class="token parameter variable">-np</span> <span class="token number">4</span> python main.py <span class="token parameter variable">--model</span> tiny <span class="token parameter variable">--optimizer</span> adagrad <span class="token parameter variable">--batch_size</span> <span class="token number">65536</span> <span class="token parameter variable">--column_slice_threshold</span> <span class="token variable"><span class="token variable">$((</span><span class="token number">1280</span><span class="token operator">*</span><span class="token number">1048576</span><span class="token variable">))</span></span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><div class="language-bash line-numbers-mode" data-ext="sh" data-title="sh"><pre class="language-bash"><code>$ python main.py <span class="token parameter variable">--model</span> tiny <span class="token parameter variable">--optimizer</span> adagrad <span class="token parameter variable">--batch_size</span> <span class="token number">65536</span>
<span class="token number">2023</span>-05-30 09:39:20.547772: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1637<span class="token punctuation">]</span> Created device /job:localhost/replica:0/task:0/device:GPU:0 with <span class="token number">13745</span> MB memory:  -<span class="token operator">&gt;</span> device: <span class="token number">0</span>, name: Tesla T4, pci bus id: 0000:3b:00.0, compute capability: <span class="token number">7.5</span>
I0530 09:39:20.613760 <span class="token number">139850810697536</span> synthetic_models.py:143<span class="token punctuation">]</span> <span class="token number">55</span> embedding tables created.
I0530 09:39:20.630478 <span class="token number">139850810697536</span> synthetic_models.py:83<span class="token punctuation">]</span> Generated <span class="token number">58</span> categorical inputs <span class="token keyword">for</span> <span class="token number">55</span> embedding tables
Initial loss: <span class="token number">1.074</span>
Benchmark step <span class="token punctuation">[</span><span class="token number">0</span>/100<span class="token punctuation">]</span>
Benchmark step <span class="token punctuation">[</span><span class="token number">50</span>/100<span class="token punctuation">]</span>
loss: <span class="token number">0.738</span>
Iteration time: <span class="token number">42.703</span> ms
<span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">=</span>
$ horovodrun <span class="token parameter variable">-np</span> <span class="token number">4</span> python main.py <span class="token parameter variable">--model</span> tiny <span class="token parameter variable">--optimizer</span> adagrad <span class="token parameter variable">--batch_size</span> <span class="token number">65536</span> <span class="token parameter variable">--column_slice_threshold</span> <span class="token variable"><span class="token variable">$((</span><span class="token number">1280</span><span class="token operator">*</span><span class="token number">1048576</span><span class="token variable">))</span></span>
<span class="token punctuation">[</span><span class="token number">1,2</span><span class="token punctuation">]</span><span class="token operator">&lt;</span>stderr<span class="token operator">&gt;</span>:2023-05-30 09:43:56.845312: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1637<span class="token punctuation">]</span> Created device /job:localhost/replica:0/task:0/device:GPU:0 with <span class="token number">13745</span> MB memory:  -<span class="token operator">&gt;</span> device: <span class="token number">2</span>, name: Tesla T4, pci bus id: 0000:af:00.0, compute capability: <span class="token number">7.5</span>
<span class="token punctuation">[</span><span class="token number">1,0</span><span class="token punctuation">]</span><span class="token operator">&lt;</span>stderr<span class="token operator">&gt;</span>:2023-05-30 09:43:56.854465: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1637<span class="token punctuation">]</span> Created device /job:localhost/replica:0/task:0/device:GPU:0 with <span class="token number">13745</span> MB memory:  -<span class="token operator">&gt;</span> device: <span class="token number">0</span>, name: Tesla T4, pci bus id: 0000:3b:00.0, compute capability: <span class="token number">7.5</span>
<span class="token punctuation">[</span><span class="token number">1,3</span><span class="token punctuation">]</span><span class="token operator">&lt;</span>stderr<span class="token operator">&gt;</span>:2023-05-30 09:43:56.857031: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1637<span class="token punctuation">]</span> Created device /job:localhost/replica:0/task:0/device:GPU:0 with <span class="token number">13745</span> MB memory:  -<span class="token operator">&gt;</span> device: <span class="token number">3</span>, name: Tesla T4, pci bus id: 0000:d8:00.0, compute capability: <span class="token number">7.5</span>
<span class="token punctuation">[</span><span class="token number">1,1</span><span class="token punctuation">]</span><span class="token operator">&lt;</span>stderr<span class="token operator">&gt;</span>:2023-05-30 09:43:56.859317: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1637<span class="token punctuation">]</span> Created device /job:localhost/replica:0/task:0/device:GPU:0 with <span class="token number">13745</span> MB memory:  -<span class="token operator">&gt;</span> device: <span class="token number">1</span>, name: Tesla T4, pci bus id: 0000:5e:00.0, compute capability: <span class="token number">7.5</span>
<span class="token punctuation">[</span><span class="token number">1,2</span><span class="token punctuation">]</span><span class="token operator">&lt;</span>stderr<span class="token operator">&gt;</span>:I0530 09:43:56.907896 <span class="token number">140019202987840</span> synthetic_models.py:143<span class="token punctuation">]</span> <span class="token number">55</span> embedding tables created.
<span class="token punctuation">[</span><span class="token number">1,2</span><span class="token punctuation">]</span><span class="token operator">&lt;</span>stderr<span class="token operator">&gt;</span>:I0530 09:43:56.922748 <span class="token number">140019202987840</span> synthetic_models.py:83<span class="token punctuation">]</span> Generated <span class="token number">58</span> categorical inputs <span class="token keyword">for</span> <span class="token number">55</span> embedding tables
<span class="token punctuation">[</span><span class="token number">1,0</span><span class="token punctuation">]</span><span class="token operator">&lt;</span>stderr<span class="token operator">&gt;</span>:I0530 09:43:56.935591 <span class="token number">140299773019968</span> synthetic_models.py:143<span class="token punctuation">]</span> <span class="token number">55</span> embedding tables created.
<span class="token punctuation">[</span><span class="token number">1,3</span><span class="token punctuation">]</span><span class="token operator">&lt;</span>stderr<span class="token operator">&gt;</span>:I0530 09:43:56.944558 <span class="token number">140711478306624</span> synthetic_models.py:143<span class="token punctuation">]</span> <span class="token number">55</span> embedding tables created.
<span class="token punctuation">[</span><span class="token number">1,0</span><span class="token punctuation">]</span><span class="token operator">&lt;</span>stderr<span class="token operator">&gt;</span>:I0530 09:43:56.950328 <span class="token number">140299773019968</span> synthetic_models.py:83<span class="token punctuation">]</span> Generated <span class="token number">58</span> categorical inputs <span class="token keyword">for</span> <span class="token number">55</span> embedding tables
<span class="token punctuation">[</span><span class="token number">1,3</span><span class="token punctuation">]</span><span class="token operator">&lt;</span>stderr<span class="token operator">&gt;</span>:I0530 09:43:56.960225 <span class="token number">140711478306624</span> synthetic_models.py:83<span class="token punctuation">]</span> Generated <span class="token number">58</span> categorical inputs <span class="token keyword">for</span> <span class="token number">55</span> embedding tables
<span class="token punctuation">[</span><span class="token number">1,1</span><span class="token punctuation">]</span><span class="token operator">&lt;</span>stderr<span class="token operator">&gt;</span>:I0530 09:43:56.962677 <span class="token number">140080325678912</span> synthetic_models.py:143<span class="token punctuation">]</span> <span class="token number">55</span> embedding tables created.
<span class="token punctuation">[</span><span class="token number">1,1</span><span class="token punctuation">]</span><span class="token operator">&lt;</span>stderr<span class="token operator">&gt;</span>:I0530 09:43:56.983801 <span class="token number">140080325678912</span> synthetic_models.py:83<span class="token punctuation">]</span> Generated <span class="token number">58</span> categorical inputs <span class="token keyword">for</span> <span class="token number">55</span> embedding tables
<span class="token punctuation">[</span><span class="token number">1,1</span><span class="token punctuation">]</span><span class="token operator">&lt;</span>stdout<span class="token operator">&gt;</span>:Initial loss: <span class="token number">1.256</span>
<span class="token punctuation">[</span><span class="token number">1,0</span><span class="token punctuation">]</span><span class="token operator">&lt;</span>stdout<span class="token operator">&gt;</span>:Initial loss: <span class="token number">1.256</span>
<span class="token punctuation">[</span><span class="token number">1,2</span><span class="token punctuation">]</span><span class="token operator">&lt;</span>stdout<span class="token operator">&gt;</span>:Initial loss: <span class="token number">1.256</span>
<span class="token punctuation">[</span><span class="token number">1,3</span><span class="token punctuation">]</span><span class="token operator">&lt;</span>stdout<span class="token operator">&gt;</span>:Initial loss: <span class="token number">1.256</span>
<span class="token punctuation">[</span><span class="token number">1,0</span><span class="token punctuation">]</span><span class="token operator">&lt;</span>stdout<span class="token operator">&gt;</span>:Benchmark step <span class="token punctuation">[</span><span class="token number">0</span>/100<span class="token punctuation">]</span>
<span class="token punctuation">[</span><span class="token number">1,0</span><span class="token punctuation">]</span><span class="token operator">&lt;</span>stdout<span class="token operator">&gt;</span>:Benchmark step <span class="token punctuation">[</span><span class="token number">50</span>/100<span class="token punctuation">]</span>
<span class="token punctuation">[</span><span class="token number">1,0</span><span class="token punctuation">]</span><span class="token operator">&lt;</span>stdout<span class="token operator">&gt;</span>:loss: <span class="token number">0.759</span>
<span class="token punctuation">[</span><span class="token number">1,0</span><span class="token punctuation">]</span><span class="token operator">&lt;</span>stdout<span class="token operator">&gt;</span>:Iteration time: <span class="token number">82.856</span> ms
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></li><li><p>dlrm测试</p><div class="language-bash line-numbers-mode" data-ext="sh" data-title="sh"><pre class="language-bash"><code><span class="token comment"># dlrm</span>
<span class="token builtin class-name">cd</span> /workspace/distributed-embeddings/examples/dlrm
horovodrun <span class="token parameter variable">-np</span> <span class="token number">4</span> python main.py
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></li></ul>`,4);function h(y,_){const t=p("ExternalLinkIcon");return o(),l("div",null,[d,u,n("ul",null,[m,n("li",null,[k,n("ul",null,[n("li",null,[a("DLRM model with 113 billion parameters (421 GiB model size) trained on the "),n("a",b,[a("Criteo Terabyte Click Logs"),i(t)]),a(" dataset（官方数据，未实测）")])]),g])]),v])}const w=e(c,[["render",h],["__file","distributed_embeddings_blog.html.vue"]]),T=JSON.parse(`{"path":"/blogs/distributed_embeddings_blog.html","title":"Distributed_embeddings","lang":"zh-CN","frontmatter":{"date":"2023-08-03T00:00:00.000Z","tag":["CUDA","Tensorflow","Embedding"],"category":["高性能"],"description":"Distributed_embeddings 1. Introduce 参考链接： 项目地址：https://github.com/NVIDIA-Merlin/distributed-embeddings 相关blog：https://developer.nvidia.com/blog/fast-terabyte-scale-recommender-t...","head":[["meta",{"property":"og:url","content":"https://bradzhone.github.io/blogs/distributed_embeddings_blog.html"}],["meta",{"property":"og:site_name","content":"BradZhone's Blog"}],["meta",{"property":"og:title","content":"Distributed_embeddings"}],["meta",{"property":"og:description","content":"Distributed_embeddings 1. Introduce 参考链接： 项目地址：https://github.com/NVIDIA-Merlin/distributed-embeddings 相关blog：https://developer.nvidia.com/blog/fast-terabyte-scale-recommender-t..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"article:author","content":"BradZhone"}],["meta",{"property":"article:tag","content":"CUDA"}],["meta",{"property":"article:tag","content":"Tensorflow"}],["meta",{"property":"article:tag","content":"Embedding"}],["meta",{"property":"article:published_time","content":"2023-08-03T00:00:00.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"Distributed_embeddings\\",\\"image\\":[\\"\\"],\\"datePublished\\":\\"2023-08-03T00:00:00.000Z\\",\\"dateModified\\":null,\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"BradZhone\\",\\"url\\":\\"https://github.com/BradZhone\\"}]}"]]},"headers":[{"level":2,"title":"1. Introduce","slug":"_1-introduce","link":"#_1-introduce","children":[]},{"level":2,"title":"2. build","slug":"_2-build","link":"#_2-build","children":[]},{"level":2,"title":"2. test&benchmark","slug":"_2-test-benchmark","link":"#_2-test-benchmark","children":[]}],"git":{"createdTime":null,"updatedTime":null,"contributors":[]},"readingTime":{"minutes":4.45,"words":1334},"filePathRelative":"blogs/distributed_embeddings_blog.md","localizedDate":"2023年8月3日","excerpt":"\\n<h2>1. Introduce</h2>\\n<ul>\\n<li>\\n<p>参考链接：</p>\\n<ul>\\n<li>项目地址：https://github.com/NVIDIA-Merlin/distributed-embeddings</li>\\n<li>相关blog：https://developer.nvidia.com/blog/fast-terabyte-scale-recommender-training-made-easy-with-nvidia-merlin-distributed-embeddings/</li>\\n<li>相关项目：https://github.com/NVIDIA/DeepLearningExamples/tree/master/TensorFlow2/Recommendation/DLRM#hybrid-parallel-training-with-merlin-distributed-embeddings</li>\\n<li>项目文档：https://nvidia-merlin.github.io/distributed-embeddings/index.html</li>\\n</ul>\\n</li>\\n<li>\\n<p>基于TF2构建大embedding，提供可伸缩模型并行封装器，能够自动将嵌入表分布到多GPU上（目前支持table-wise和column-wise）</p>\\n</li>\\n<li>\\n<p>支持混合模型并行（dist_model_parallel）：</p>\\n<figure><figcaption></figcaption></figure>\\n</li>\\n<li>\\n<p>仅需修改少量代码即可使用</p>\\n<div class=\\"language-python\\" data-ext=\\"py\\" data-title=\\"py\\"><pre class=\\"language-python\\"><code><span class=\\"token keyword\\">import</span> dist_model_parallel <span class=\\"token keyword\\">as</span> dmp\\n \\n<span class=\\"token keyword\\">class</span> <span class=\\"token class-name\\">MyEmbeddingModel</span><span class=\\"token punctuation\\">(</span>tf<span class=\\"token punctuation\\">.</span>keras<span class=\\"token punctuation\\">.</span>Model<span class=\\"token punctuation\\">)</span><span class=\\"token punctuation\\">:</span>\\n  <span class=\\"token keyword\\">def</span>  <span class=\\"token function\\">__init__</span><span class=\\"token punctuation\\">(</span>self<span class=\\"token punctuation\\">,</span> table_sizes<span class=\\"token punctuation\\">)</span><span class=\\"token punctuation\\">:</span>\\n    <span class=\\"token punctuation\\">.</span><span class=\\"token punctuation\\">.</span><span class=\\"token punctuation\\">.</span>\\n    self<span class=\\"token punctuation\\">.</span>embedding_layers <span class=\\"token operator\\">=</span> <span class=\\"token punctuation\\">[</span>tf<span class=\\"token punctuation\\">.</span>keras<span class=\\"token punctuation\\">.</span>layers<span class=\\"token punctuation\\">.</span>Embedding<span class=\\"token punctuation\\">(</span>input_dim<span class=\\"token punctuation\\">,</span> output_dim<span class=\\"token punctuation\\">)</span> <span class=\\"token keyword\\">for</span> input_dim<span class=\\"token punctuation\\">,</span> output_dim <span class=\\"token keyword\\">in</span> table_sizes<span class=\\"token punctuation\\">]</span>\\n    <span class=\\"token comment\\"># 1. Add this line to wrap list of embedding layers used in the model</span>\\n    self<span class=\\"token punctuation\\">.</span>embedding_layers <span class=\\"token operator\\">=</span> dmp<span class=\\"token punctuation\\">.</span>DistributedEmbedding<span class=\\"token punctuation\\">(</span>self<span class=\\"token punctuation\\">.</span>embedding_layers<span class=\\"token punctuation\\">)</span>\\n  <span class=\\"token keyword\\">def</span> <span class=\\"token function\\">call</span><span class=\\"token punctuation\\">(</span>self<span class=\\"token punctuation\\">,</span> inputs<span class=\\"token punctuation\\">)</span><span class=\\"token punctuation\\">:</span>\\n    <span class=\\"token comment\\"># embedding_outputs = [e(i) for e, i in zip(self.embedding_layers, inputs)]</span>\\n    embedding_outputs <span class=\\"token operator\\">=</span> self<span class=\\"token punctuation\\">.</span>embedding_layers<span class=\\"token punctuation\\">(</span>inputs<span class=\\"token punctuation\\">)</span>\\n     \\n<span class=\\"token decorator annotation punctuation\\">@tf<span class=\\"token punctuation\\">.</span>function</span>\\n<span class=\\"token keyword\\">def</span> <span class=\\"token function\\">training_step</span><span class=\\"token punctuation\\">(</span>inputs<span class=\\"token punctuation\\">,</span> labels<span class=\\"token punctuation\\">,</span> first_batch<span class=\\"token punctuation\\">)</span><span class=\\"token punctuation\\">:</span>\\n  <span class=\\"token keyword\\">with</span> tf<span class=\\"token punctuation\\">.</span>GradientTape<span class=\\"token punctuation\\">(</span><span class=\\"token punctuation\\">)</span> <span class=\\"token keyword\\">as</span> tape<span class=\\"token punctuation\\">:</span>\\n    probs <span class=\\"token operator\\">=</span> model<span class=\\"token punctuation\\">(</span>inputs<span class=\\"token punctuation\\">)</span>\\n    loss_value <span class=\\"token operator\\">=</span> loss<span class=\\"token punctuation\\">(</span>labels<span class=\\"token punctuation\\">,</span> probs<span class=\\"token punctuation\\">)</span>\\n \\n  <span class=\\"token comment\\"># 2. Change Horovod Gradient Tape to dmp tape</span>\\n  <span class=\\"token comment\\"># tape = hvd.DistributedGradientTape(tape)</span>\\n  tape <span class=\\"token operator\\">=</span> dmp<span class=\\"token punctuation\\">.</span>DistributedGradientTape<span class=\\"token punctuation\\">(</span>tape<span class=\\"token punctuation\\">)</span>\\n  grads <span class=\\"token operator\\">=</span> tape<span class=\\"token punctuation\\">.</span>gradient<span class=\\"token punctuation\\">(</span>loss_value<span class=\\"token punctuation\\">,</span> model<span class=\\"token punctuation\\">.</span>trainable_variables<span class=\\"token punctuation\\">)</span>\\n  opt<span class=\\"token punctuation\\">.</span>apply_gradients<span class=\\"token punctuation\\">(</span><span class=\\"token builtin\\">zip</span><span class=\\"token punctuation\\">(</span>grads<span class=\\"token punctuation\\">,</span> model<span class=\\"token punctuation\\">.</span>trainable_variables<span class=\\"token punctuation\\">)</span><span class=\\"token punctuation\\">)</span>\\n \\n  <span class=\\"token keyword\\">if</span> first_batch<span class=\\"token punctuation\\">:</span>\\n    <span class=\\"token comment\\"># 3. Change Horovod broadcast_variables to dmp's</span>\\n    <span class=\\"token comment\\"># hvd.broadcast_variables(model.variables, root_rank=0)</span>\\n    dmp<span class=\\"token punctuation\\">.</span>broadcast_variables<span class=\\"token punctuation\\">(</span>model<span class=\\"token punctuation\\">.</span>variables<span class=\\"token punctuation\\">,</span> root_rank<span class=\\"token operator\\">=</span><span class=\\"token number\\">0</span><span class=\\"token punctuation\\">)</span>\\n  <span class=\\"token keyword\\">return</span> loss_value\\n</code></pre></div></li>\\n<li>\\n<p>示例</p>\\n<div class=\\"language-python\\" data-ext=\\"py\\" data-title=\\"py\\"><pre class=\\"language-python\\"><code><span class=\\"token keyword\\">import</span> tensorflow <span class=\\"token keyword\\">as</span> tf\\n<span class=\\"token keyword\\">import</span> distributed_embeddings<span class=\\"token punctuation\\">.</span>python<span class=\\"token punctuation\\">.</span>layers<span class=\\"token punctuation\\">.</span>dist_model_parallel <span class=\\"token keyword\\">as</span> dmp\\n<span class=\\"token keyword\\">from</span> distributed_embeddings<span class=\\"token punctuation\\">.</span>python<span class=\\"token punctuation\\">.</span>layers<span class=\\"token punctuation\\">.</span>dist_model_parallel <span class=\\"token keyword\\">import</span> Embedding<span class=\\"token punctuation\\">,</span> DistributedEmbedding\\nlayer0 <span class=\\"token operator\\">=</span> Embedding<span class=\\"token punctuation\\">(</span><span class=\\"token number\\">1000</span><span class=\\"token punctuation\\">,</span> <span class=\\"token number\\">64</span><span class=\\"token punctuation\\">,</span> name<span class=\\"token operator\\">=</span><span class=\\"token string\\">\\"emb_0\\"</span><span class=\\"token punctuation\\">)</span>\\nlayer1 <span class=\\"token operator\\">=</span> Embedding<span class=\\"token punctuation\\">(</span><span class=\\"token number\\">1000</span><span class=\\"token punctuation\\">,</span> <span class=\\"token number\\">64</span><span class=\\"token punctuation\\">,</span> combiner<span class=\\"token operator\\">=</span><span class=\\"token string\\">\\"mean\\"</span><span class=\\"token punctuation\\">,</span> name<span class=\\"token operator\\">=</span><span class=\\"token string\\">\\"emb_1\\"</span><span class=\\"token punctuation\\">)</span>\\nlayer2 <span class=\\"token operator\\">=</span> tf<span class=\\"token punctuation\\">.</span>keras<span class=\\"token punctuation\\">.</span>layers<span class=\\"token punctuation\\">.</span>Embedding<span class=\\"token punctuation\\">(</span><span class=\\"token number\\">1000</span><span class=\\"token punctuation\\">,</span> <span class=\\"token number\\">64</span><span class=\\"token punctuation\\">,</span> name<span class=\\"token operator\\">=</span><span class=\\"token string\\">\\"emb_2\\"</span><span class=\\"token punctuation\\">)</span>\\nlayer3 <span class=\\"token operator\\">=</span> tf<span class=\\"token punctuation\\">.</span>keras<span class=\\"token punctuation\\">.</span>layers<span class=\\"token punctuation\\">.</span>Embedding<span class=\\"token punctuation\\">(</span><span class=\\"token number\\">1000</span><span class=\\"token punctuation\\">,</span> <span class=\\"token number\\">64</span><span class=\\"token punctuation\\">,</span> name<span class=\\"token operator\\">=</span><span class=\\"token string\\">\\"emb_3\\"</span><span class=\\"token punctuation\\">)</span>\\nlayers0 <span class=\\"token operator\\">=</span> <span class=\\"token punctuation\\">[</span>layer0<span class=\\"token punctuation\\">,</span> layer1<span class=\\"token punctuation\\">]</span>\\nlayers1 <span class=\\"token operator\\">=</span> <span class=\\"token punctuation\\">[</span>layer2<span class=\\"token punctuation\\">,</span> layer3<span class=\\"token punctuation\\">]</span>\\nemb_layers0 <span class=\\"token operator\\">=</span> DistributedEmbedding<span class=\\"token punctuation\\">(</span>layers0<span class=\\"token punctuation\\">,</span> column_slice_threshold<span class=\\"token operator\\">=</span><span class=\\"token number\\">32000</span><span class=\\"token punctuation\\">)</span>\\nemb_layers1 <span class=\\"token operator\\">=</span> DistributedEmbedding<span class=\\"token punctuation\\">(</span>layers1<span class=\\"token punctuation\\">,</span> column_slice_threshold<span class=\\"token operator\\">=</span><span class=\\"token number\\">32000</span><span class=\\"token punctuation\\">)</span>\\n</code></pre></div></li>\\n<li>\\n<p>API:</p>\\n<ul>\\n<li>\\n<p>distributed_embeddings.python.layers.embedding.</p>\\n<p>Embedding</p>\\n<ul>\\n<li>接口基本上对齐 tf.keras.layers.Embedding，且增加了支持同时lookup多个embedding 再将它们combine（sum/mean）为一个vector输出</li>\\n<li>支持多种数据类型：onehot/fixed hotness (multihot, 输入tensor维度相同)/variable hotness（multihot，  输入tensor维度不同，使用tf.RaggedTensor）/sparse tensor(multihot,  使用tf.sparse.SparseTensor，按照csr格式存储稀疏tensor)</li>\\n</ul>\\n</li>\\n<li>\\n<p>distributed_embeddings.python.layers.dist_model_parallel.</p>\\n<p>DistributedEmbedding</p>\\n<ul>\\n<li>支持按列切分embedding table，适用于emb table规模庞大且无法存入单张卡内存的情况</li>\\n<li>可设置column_slice_threshold参数确定切分阈值（elements num）</li>\\n<li>可封装共享embedding table，如两个特征域，一个表示watched_video, 一个表示browsed_video，他们的特征都是videos,因此可公用同一个emb table</li>\\n</ul>\\n</li>\\n</ul>\\n<p>性能表现：</p>\\n<ul>\\n<li>DLRM model with 113 billion parameters (421 GiB model size) trained on the <a href=\\"https://labs.criteo.com/2013/12/download-terabyte-click-logs/\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">Criteo Terabyte Click Logs</a> dataset（官方数据，未实测）</li>\\n</ul>\\n<table>\\n<thead>\\n<tr>\\n<th>Hardware</th>\\n<th>Description</th>\\n<th>Training Throughput (samples/second)</th>\\n<th>Speedup over CPU</th>\\n</tr>\\n</thead>\\n<tbody>\\n<tr>\\n<td><strong>2 x AMD EPYC 7742</strong></td>\\n<td>Both MLP layers and embeddings on CPU</td>\\n<td>17.7k</td>\\n<td>1x</td>\\n</tr>\\n<tr>\\n<td><strong>1 x A100-80GB</strong>; 2 x AMD EPYC 7742</td>\\n<td>Large embeddings on CPU, everything else on GPU</td>\\n<td>768k</td>\\n<td>43x</td>\\n</tr>\\n<tr>\\n<td><strong>DGX A100 (8xA100-80GB)</strong></td>\\n<td>Hybrid parallel with NVIDIA Merlin <strong>Distributed-Embeddings,</strong> whole model on GPU</td>\\n<td>12.1M</td>\\n<td><strong>683x</strong></td>\\n</tr>\\n</tbody>\\n</table>\\n<p>Synthetic models benchmark （见第三节test部分benchmarks测试）</p>\\n<ul>\\n<li>模型规模定义</li>\\n</ul>\\n<table>\\n<thead>\\n<tr>\\n<th>Model</th>\\n<th>Total number of embedding tables</th>\\n<th>Total embedding size (GiB)</th>\\n</tr>\\n</thead>\\n<tbody>\\n<tr>\\n<td>Tiny</td>\\n<td>55</td>\\n<td>4.2</td>\\n</tr>\\n<tr>\\n<td>Small</td>\\n<td>107</td>\\n<td>26.3</td>\\n</tr>\\n<tr>\\n<td>Medium</td>\\n<td>311</td>\\n<td>206.2</td>\\n</tr>\\n<tr>\\n<td>Large</td>\\n<td>612</td>\\n<td>773.8</td>\\n</tr>\\n<tr>\\n<td>Jumbo</td>\\n<td>1,022</td>\\n<td>3,109.5</td>\\n</tr>\\n</tbody>\\n</table>\\n<ul>\\n<li>官方性能(DGX-A100-80GB, batchsize=65536, optimizer=adagrad )</li>\\n</ul>\\n<table>\\n<thead>\\n<tr>\\n<th style=\\"text-align:center\\"><strong>Model</strong></th>\\n<th style=\\"text-align:center\\"><strong>Training step time (ms)</strong></th>\\n<th style=\\"text-align:center\\"><strong>Training step time (ms)</strong></th>\\n<th style=\\"text-align:center\\"><strong>Training step time (ms)</strong></th>\\n<th style=\\"text-align:center\\"><strong>Training step time (ms)</strong></th>\\n<th><strong>Training step time (ms)</strong></th>\\n</tr>\\n</thead>\\n<tbody>\\n<tr>\\n<td style=\\"text-align:center\\"><strong>Model</strong></td>\\n<td style=\\"text-align:center\\"><strong>1 GPU</strong></td>\\n<td style=\\"text-align:center\\"><strong>8 GPU</strong></td>\\n<td style=\\"text-align:center\\"><strong>16 GPU</strong></td>\\n<td style=\\"text-align:center\\"><strong>32 GPU</strong></td>\\n<td><strong>128 GPU</strong></td>\\n</tr>\\n<tr>\\n<td style=\\"text-align:center\\"><strong>Tiny</strong></td>\\n<td style=\\"text-align:center\\">17.6</td>\\n<td style=\\"text-align:center\\">3.6</td>\\n<td style=\\"text-align:center\\">3.2</td>\\n<td style=\\"text-align:center\\"></td>\\n<td></td>\\n</tr>\\n<tr>\\n<td style=\\"text-align:center\\"><strong>Small</strong></td>\\n<td style=\\"text-align:center\\">57.8</td>\\n<td style=\\"text-align:center\\">14.0</td>\\n<td style=\\"text-align:center\\">11.6</td>\\n<td style=\\"text-align:center\\">7.4</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td style=\\"text-align:center\\"><strong>Medium</strong></td>\\n<td style=\\"text-align:center\\"></td>\\n<td style=\\"text-align:center\\">64.4</td>\\n<td style=\\"text-align:center\\">44.9</td>\\n<td style=\\"text-align:center\\">31.1</td>\\n<td>17.2</td>\\n</tr>\\n<tr>\\n<td style=\\"text-align:center\\"><strong>Large</strong></td>\\n<td style=\\"text-align:center\\"></td>\\n<td style=\\"text-align:center\\"></td>\\n<td style=\\"text-align:center\\"></td>\\n<td style=\\"text-align:center\\">65.0</td>\\n<td>33.4</td>\\n</tr>\\n<tr>\\n<td style=\\"text-align:center\\"><strong>Jumbo</strong></td>\\n<td style=\\"text-align:center\\"></td>\\n<td style=\\"text-align:center\\"></td>\\n<td style=\\"text-align:center\\"></td>\\n<td style=\\"text-align:center\\"></td>\\n<td>102.3</td>\\n</tr>\\n</tbody>\\n</table>\\n<ul>\\n<li>实测性能(Tesla T4, batchsize=65536, optimizer=adagrad)</li>\\n</ul>\\n<table>\\n<thead>\\n<tr>\\n<th style=\\"text-align:center\\"><strong>Model</strong></th>\\n<th style=\\"text-align:center\\"><strong>Training step time (ms)</strong></th>\\n<th><strong>Training step time (ms)</strong></th>\\n</tr>\\n</thead>\\n<tbody>\\n<tr>\\n<td style=\\"text-align:center\\"><strong>Model</strong></td>\\n<td style=\\"text-align:center\\">1 GPU</td>\\n<td>4 GPU</td>\\n</tr>\\n<tr>\\n<td style=\\"text-align:center\\"><strong>Tiny</strong></td>\\n<td style=\\"text-align:center\\">42.703</td>\\n<td>82.856</td>\\n</tr>\\n</tbody>\\n</table>\\n<ul>\\n<li>对比TF原生数据并行</li>\\n</ul>\\n<table>\\n<thead>\\n<tr>\\n<th><strong>Solution</strong></th>\\n<th style=\\"text-align:center\\"><strong>Training step time (ms)</strong></th>\\n<th><strong>Training step time (ms)</strong></th>\\n<th><strong>Training step time (ms)</strong></th>\\n<th><strong>Training step time (ms)</strong></th>\\n</tr>\\n</thead>\\n<tbody>\\n<tr>\\n<td><strong>Solution</strong></td>\\n<td style=\\"text-align:center\\"><strong>1 GPU</strong></td>\\n<td><strong>2 GPU</strong></td>\\n<td><strong>4 GPU</strong></td>\\n<td><strong>8 GPU</strong></td>\\n</tr>\\n<tr>\\n<td>NVIDIA Merlin Distributed Embeddings <strong>Model Parallel</strong></td>\\n<td style=\\"text-align:center\\">17.7</td>\\n<td><strong>11.6</strong></td>\\n<td><strong>6.4</strong></td>\\n<td><strong>4.2</strong></td>\\n</tr>\\n<tr>\\n<td>Native TensorFlow <strong>Data Parallel</strong></td>\\n<td style=\\"text-align:center\\">19.9</td>\\n<td>20.2</td>\\n<td>21.2</td>\\n<td>22.3</td>\\n</tr>\\n</tbody>\\n</table>\\n</li>\\n</ul>","autoDesc":true}`);export{w as comp,T as data};
