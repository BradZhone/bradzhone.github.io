import{_ as t}from"./plugin-vue_export-helper-DlAUqK2U.js";import{o as n,c as l,e as d}from"./app-Coh1oo3x.js";const i={},r=d('<h1 id="loss-相关问题" tabindex="-1"><a class="header-anchor" href="#loss-相关问题"><span>Loss 相关问题</span></a></h1><ol><li><p>loss 不收敛</p><ul><li><p>欠拟合</p><ul><li><p>特征：训练集上loss始终和初始大小相当，accuracy很低</p></li><li><p>网络拟合能力不足</p><ul><li>每一个batch使用相同的数据训练，检查loss和acc是否有变化来判断</li><li>改进：增加网络深度（层数），宽度（神经元数量）</li></ul></li><li><p>网络配置问题</p><ul><li><p>权重初始化是否合理</p></li><li><table><thead><tr><th>初始化方式</th><th>备注</th></tr></thead><tbody><tr><td>全零初始化 Zeros</td><td>造成网络对称，所有神经元进行相同参数更新</td></tr><tr><td>全1初始化 Ones</td><td>造成网络对称，所有神经元进行相同参数更新</td></tr><tr><td>初始化为固定值value Constant</td><td>造成网络对称，所有神经元进行相同参数更新</td></tr><tr><td>随机正态分布初始化 RandomNormal</td><td></td></tr><tr><td>随机均匀分布初始化 RandomUniform</td><td></td></tr><tr><td>截尾高斯分布初始化 TruncatedNormal</td><td></td></tr><tr><td>VarianceScaling</td><td></td></tr><tr><td>用随机正交矩阵初始化Orthogonal</td><td></td></tr><tr><td>使用单位矩阵初始化 Identiy</td><td></td></tr><tr><td>LeCun均匀分布初始化方法 lecun_uniform</td><td></td></tr><tr><td>LeCun正态分布初始化方法 lecun_normal</td><td></td></tr><tr><td>Glorot正态分布初始化方法 glorot_normal</td><td></td></tr><tr><td>Glorot均匀分布初始化 glorot_uniform</td><td>一种均值为0，以0为中心的对称区间均匀分布的随机数；接近于0的均匀分布会导致梯度消失</td></tr><tr><td>He正态分布初始化 he_normal</td><td></td></tr><tr><td>He均匀分布初始化 he_uniform</td><td></td></tr></tbody></table></li><li><p>使用适当的激活函数</p></li><li><table><thead><tr><th>激活函数</th><th>备注</th></tr></thead><tbody><tr><td>ReLU</td><td>一般用于卷积层、全连接层</td></tr><tr><td>tanh</td><td>一般用于循环层</td></tr><tr><td>softmax</td><td>一般用于全连接层做输出层</td></tr><tr><td>sigmoid</td><td>不常用</td></tr></tbody></table></li><li><p>使用适当的优化器和学习率</p></li><li><p>未进行归一化导致尺度不平衡，误差增大</p></li></ul></li><li><p>人工构造少量数据反复训练，看是否下降。若不下降，则可能脚本有问题，若下降，则超参可能有问题</p></li><li><p>梯度检查</p></li></ul></li></ul></li><li><p>loss 为nan，inf</p><ul><li>100个iter内出现可能是学习率过高</li><li>若为rnn之类的网络，可能是梯度爆炸，可增加梯度截断（gradient clipping）</li><li>梯度消失：导数特别小，导致连乘接近无穷小（输入数据太小，或输出数据落在激活函数饱和区），使loss不下降，但不会出现nan和inf</li><li>梯度爆炸：导数特别大，导致连乘特别大，超出表示范围（输入数据未归一化（减均值，除方差，加入BN，L2等normalization方法）），可能出现inf</li><li>batchNorm层很多时，检查tensor输入该层后是否变为nan（可能因为训练集和验证集是不同的分布，使移动均值和移动方差为nan，可设置track_runing_stats=False禁用）</li><li>每个batch前梯度要清零<code>optimizer.zero_grad</code></li><li>0做除数，0或负数做对数(loss正常下降，突然出现nan。可对预测数据增加最小值偏移，避免小于等于0)，根号下要大于0，同时要加个Epsilon，因为开根号的导数，变量在分母上，不加Epsilon导数就成了nan</li><li>计算loss的数组越界</li><li>涉及指数计算时算得无穷：$\\frac{e{x}}{e{y}}=\\frac{INF}{INF}=NAN$ ，可减去最大值处理</li><li>label缺失</li><li>输入数据就含有NAN（使用简单的网络读取输入进行检查）</li><li>使用不同精度数据，loss超过精度类型最大范围</li><li>pooling步长大于核尺寸</li></ul></li><li><p>loss 为负</p><ul><li>https://blog.csdn.net/qq_43733107/article/details/128915638</li></ul></li></ol>',2),o=[r];function e(a,s){return n(),l("div",null,o)}const h=t(i,[["render",e],["__file","loss.html.vue"]]),u=JSON.parse(`{"path":"/notes/loss.html","title":"Loss 相关问题","lang":"zh-CN","frontmatter":{"date":"2023-12-29T00:00:00.000Z","tag":["Loss"],"category":["DeepLearning"],"description":"Loss 相关问题 loss 不收敛 欠拟合 特征：训练集上loss始终和初始大小相当，accuracy很低 网络拟合能力不足 每一个batch使用相同的数据训练，检查loss和acc是否有变化来判断 改进：增加网络深度（层数），宽度（神经元数量） 网络配置问题 权重初始化是否合理 使用适当的激活函数 使用适当的优化器和学习率 未进行归一化导致尺度不平...","head":[["meta",{"property":"og:url","content":"https://bradzhone.github.io/notes/loss.html"}],["meta",{"property":"og:site_name","content":"BradZhone's Blog"}],["meta",{"property":"og:title","content":"Loss 相关问题"}],["meta",{"property":"og:description","content":"Loss 相关问题 loss 不收敛 欠拟合 特征：训练集上loss始终和初始大小相当，accuracy很低 网络拟合能力不足 每一个batch使用相同的数据训练，检查loss和acc是否有变化来判断 改进：增加网络深度（层数），宽度（神经元数量） 网络配置问题 权重初始化是否合理 使用适当的激活函数 使用适当的优化器和学习率 未进行归一化导致尺度不平..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"article:author","content":"BradZhone"}],["meta",{"property":"article:tag","content":"Loss"}],["meta",{"property":"article:published_time","content":"2023-12-29T00:00:00.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"Loss 相关问题\\",\\"image\\":[\\"\\"],\\"datePublished\\":\\"2023-12-29T00:00:00.000Z\\",\\"dateModified\\":null,\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"BradZhone\\",\\"url\\":\\"https://github.com/BradZhone\\"}]}"]]},"headers":[],"git":{"createdTime":null,"updatedTime":null,"contributors":[]},"readingTime":{"minutes":2.82,"words":847},"filePathRelative":"notes/loss.md","localizedDate":"2023年12月29日","excerpt":"\\n<ol>\\n<li>\\n<p>loss 不收敛</p>\\n<ul>\\n<li>\\n<p>欠拟合</p>\\n<ul>\\n<li>\\n<p>特征：训练集上loss始终和初始大小相当，accuracy很低</p>\\n</li>\\n<li>\\n<p>网络拟合能力不足</p>\\n<ul>\\n<li>每一个batch使用相同的数据训练，检查loss和acc是否有变化来判断</li>\\n<li>改进：增加网络深度（层数），宽度（神经元数量）</li>\\n</ul>\\n</li>\\n<li>\\n<p>网络配置问题</p>\\n<ul>\\n<li>\\n<p>权重初始化是否合理</p>\\n</li>\\n<li>\\n<table>\\n<thead>\\n<tr>\\n<th>初始化方式</th>\\n<th>备注</th>\\n</tr>\\n</thead>\\n<tbody>\\n<tr>\\n<td>全零初始化 Zeros</td>\\n<td>造成网络对称，所有神经元进行相同参数更新</td>\\n</tr>\\n<tr>\\n<td>全1初始化 Ones</td>\\n<td>造成网络对称，所有神经元进行相同参数更新</td>\\n</tr>\\n<tr>\\n<td>初始化为固定值value Constant</td>\\n<td>造成网络对称，所有神经元进行相同参数更新</td>\\n</tr>\\n<tr>\\n<td>随机正态分布初始化 RandomNormal</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>随机均匀分布初始化 RandomUniform</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>截尾高斯分布初始化 TruncatedNormal</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>VarianceScaling</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>用随机正交矩阵初始化Orthogonal</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>使用单位矩阵初始化 Identiy</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>LeCun均匀分布初始化方法 lecun_uniform</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>LeCun正态分布初始化方法 lecun_normal</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>Glorot正态分布初始化方法 glorot_normal</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>Glorot均匀分布初始化 glorot_uniform</td>\\n<td>一种均值为0，以0为中心的对称区间均匀分布的随机数；接近于0的均匀分布会导致梯度消失</td>\\n</tr>\\n<tr>\\n<td>He正态分布初始化 he_normal</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>He均匀分布初始化 he_uniform</td>\\n<td></td>\\n</tr>\\n</tbody>\\n</table>\\n</li>\\n<li>\\n<p>使用适当的激活函数</p>\\n</li>\\n<li>\\n<table>\\n<thead>\\n<tr>\\n<th>激活函数</th>\\n<th>备注</th>\\n</tr>\\n</thead>\\n<tbody>\\n<tr>\\n<td>ReLU</td>\\n<td>一般用于卷积层、全连接层</td>\\n</tr>\\n<tr>\\n<td>tanh</td>\\n<td>一般用于循环层</td>\\n</tr>\\n<tr>\\n<td>softmax</td>\\n<td>一般用于全连接层做输出层</td>\\n</tr>\\n<tr>\\n<td>sigmoid</td>\\n<td>不常用</td>\\n</tr>\\n</tbody>\\n</table>\\n</li>\\n<li>\\n<p>使用适当的优化器和学习率</p>\\n</li>\\n<li>\\n<p>未进行归一化导致尺度不平衡，误差增大</p>\\n</li>\\n</ul>\\n</li>\\n<li>\\n<p>人工构造少量数据反复训练，看是否下降。若不下降，则可能脚本有问题，若下降，则超参可能有问题</p>\\n</li>\\n<li>\\n<p>梯度检查</p>\\n</li>\\n</ul>\\n</li>\\n</ul>\\n</li>\\n<li>\\n<p>loss 为nan，inf</p>\\n<ul>\\n<li>100个iter内出现可能是学习率过高</li>\\n<li>若为rnn之类的网络，可能是梯度爆炸，可增加梯度截断（gradient clipping）</li>\\n<li>梯度消失：导数特别小，导致连乘接近无穷小（输入数据太小，或输出数据落在激活函数饱和区），使loss不下降，但不会出现nan和inf</li>\\n<li>梯度爆炸：导数特别大，导致连乘特别大，超出表示范围（输入数据未归一化（减均值，除方差，加入BN，L2等normalization方法）），可能出现inf</li>\\n<li>batchNorm层很多时，检查tensor输入该层后是否变为nan（可能因为训练集和验证集是不同的分布，使移动均值和移动方差为nan，可设置track_runing_stats=False禁用）</li>\\n<li>每个batch前梯度要清零<code>optimizer.zero_grad</code></li>\\n<li>0做除数，0或负数做对数(loss正常下降，突然出现nan。可对预测数据增加最小值偏移，避免小于等于0)，根号下要大于0，同时要加个Epsilon，因为开根号的导数，变量在分母上，不加Epsilon导数就成了nan</li>\\n<li>计算loss的数组越界</li>\\n<li>涉及指数计算时算得无穷：$\\\\frac{e{x}}{e{y}}=\\\\frac{INF}{INF}=NAN$ ，可减去最大值处理</li>\\n<li>label缺失</li>\\n<li>输入数据就含有NAN（使用简单的网络读取输入进行检查）</li>\\n<li>使用不同精度数据，loss超过精度类型最大范围</li>\\n<li>pooling步长大于核尺寸</li>\\n</ul>\\n</li>\\n<li>\\n<p>loss 为负</p>\\n<ul>\\n<li>https://blog.csdn.net/qq_43733107/article/details/128915638</li>\\n</ul>\\n</li>\\n</ol>","autoDesc":true}`);export{h as comp,u as data};
