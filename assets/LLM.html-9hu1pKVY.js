import{_ as t}from"./plugin-vue_export-helper-DlAUqK2U.js";import{o as n,c as e,e as a}from"./app-Coh1oo3x.js";const r={},d=a(`<h1 id="llm" tabindex="-1"><a class="header-anchor" href="#llm"><span>LLM</span></a></h1><h2 id="_1-conceptions" tabindex="-1"><a class="header-anchor" href="#_1-conceptions"><span>1. Conceptions</span></a></h2><ul><li><table><thead><tr><th>概念</th><th>内容</th><th>备注</th></tr></thead><tbody><tr><td>token/word segmentation</td><td>将原始文本切分成子单元的过程就叫做 Tokenization。即，按照特定需求把文本中的句子、段落切分成一个字符串序列（其中的元素通常称为token 或叫词语），方便后续的处理分析工作。它是一个离散的文本单元，它可以是单词、标点符号、数字或其他语言元素，这些元素被用作训练和生成文本的基本单位。Token有粒度之分，如：词、字符或subword</td><td>解读资料：https://zhuanlan.zhihu.com/p/444774532</td></tr><tr><td>困惑度 Perplexity (PPL)</td><td>困惑度是一种用来评估语言模型的指标。它衡量了一个语言模型在给定数据集上的预测能力和概率分布的复杂性。较低的困惑度表示模型对数据集的拟合效果更好，也就是说，模型更能准确地预测下一个词或下一个句子。</td><td></td></tr><tr><td>SFT</td><td>监督微调（Supervised Fine-Tuning）</td><td></td></tr><tr><td>RLHF</td><td>基于人类反馈的强化学习 (Reinforcement Learning from Human Feedback)</td><td></td></tr><tr><td>embedding</td><td>（词）嵌入，可用于降维或升维，将特征拉近拉远到一个合适的观察点，将不同特征联系起来</td><td>https://zhuanlan.zhihu.com/p/616419336</td></tr><tr><td>seq2seq</td><td>序列到序列，该技术突破了传统的固定大小输入问题框架，提出了一种全新的端到端的映射方法。技术的核心是 Encoder-Decoder 架构，Encoder 负责将输入序列压缩成指定长度的向量，这个向量就可以看成是这个序列的语义，称为编码。而 Decoder 则负责根据语义向量生成指定的序列，这个过程也称为解码。Seq2Seq 是输出的长度不确定时采用的模型，因此在机器翻译、对话系统、自动文摘等自然语言处理任务中被广泛运用。</td><td>https://zhuanlan.zhihu.com/p/558138527<br>https://zhuanlan.zhihu.com/p/520657912?utm_id=0</td></tr><tr><td>attention</td><td>注意力机制（Attention Mechanism）源于对人类视觉的研究，主要理论原理是：信息处理时选择性地关注所有信息的一部分，同时忽略其他可见的信息。在计算能力有限情况下，注意力机制是解决信息超载问题的主要手段的一种资源分配方案，将计算资源分配给更重要的任务。</td><td>attention机制：https://zhuanlan.zhihu.com/p/46990010<br>atention图解：https://zhuanlan.zhihu.com/p/342235515<br>https://zhuanlan.zhihu.com/p/42724582<br>https://zhuanlan.zhihu.com/p/53682800<br>如何理解attention中的QKV：https://www.zhihu.com/question/298810062<br>transformer中的attention为什么scaled：https://www.zhihu.com/question/339723385<br>attention计算公式中的softmax：https://zhuanlan.zhihu.com/p/157490738<br></td></tr><tr><td>transformer</td><td>Transformer最早起源于论文Attention is all your need，是谷歌云TPU推荐的参考模型。 目前，在NLP领域当中，主要存在三种特征处理器：CNN、RNN以及Transformer，当前Transformer的流行程度已经大过CNN和RNN，它抛弃了传统CNN和RNN神经网络，整个网络结构完全由Attention机制以及前馈神经网络组成。论文中给出Transformer的定义是：Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence aligned RNNs or convolution。</td><td>论文 ：https://arxiv.org/abs/1706.03762<br>源码参考：https://github.com/huggingface/transformers<br>解读资料：https://blog.csdn.net/m0_67505927/article/details/123209347<br>https://blog.csdn.net/qq_52302919/article/details/122207924<br>B站李宏毅的视频，有几节专门讲attention和transformer的，比较清晰：https://www.bilibili.com/video/BV1Wv411h7kN?p=23<br></td></tr><tr><td>Beam search</td><td>集束搜索。一种搜索算法，是对greedy search的一个改进，相对greedy search扩大了搜索空间，但远不及穷举搜索指数级的搜索空间，是二者的一个折中方案。在文本生成任务中常用的解码策略。</td><td>https://zhuanlan.zhihu.com/p/82829880</td></tr><tr><td>AIGC (Artificial Intelligence Generated Content / AI-Generated Content)</td><td>人工智能生成内容，一种利用人工智能技术自动生成文章、音频、视频等多媒体内容的方法。</td><td></td></tr><tr><td>LLM ( Large Language Model)</td><td>大语言模型，使用大量文本数据训练的深度学习模型，可以生成自然语言文本或理解语言文本的含义。特点是规模庞大（包含数十亿参数），能学习到语言数据中的复杂模式。</td><td></td></tr><tr><td>GPT (Generative Pre-trained Transformer)</td><td>生成式预训练Transformer模型, 由OpenAI提出的一系列强大的预训练语言模型，兼具“大规模”和“预训练”两种属性。</td><td></td></tr><tr><td>BERT (Bidirectional Encoder Representations from Transformers)</td><td>基于Transformer的双向编码表征，谷歌提出的一个LLM，对NLP研究产生了重大影响。该模型使用双向方法从一个词的左右两边捕捉上下文，使得各种任务的性能提高，如情感分析和命名实体识别。</td><td></td></tr><tr><td>Bloom ( BigScience Large Open-science Open-access Multi-lingual Language Model)</td><td>BLOOM是一种基于trasnformer架构的解码器（Decoder-Only）自回归大语言模型，由BigScience社区开发和发布。该模型除了176B 之外，还提供了几个较小的模型，其模型参数规模为：560M，1.1B，1.7B，3B 和7.1B。</td><td>论文：https://arxiv.org/abs/2211.05100<br>解读：https://zhuanlan.zhihu.com/p/640016830</td></tr><tr><td>Flash Attention</td><td>FlashAttention 是一种重新排序注意力计算的算法，它无需任何近似即可加速注意力计算并减少内存占用，主要解决Transformer计算速度慢和存储占用高的问题，所以作为目前LLM的模型加速它是一个非常好的解决方案。</td><td>论文：<br>https://arxiv.org/abs/2205.14135<br>解读资料：<br>https://zhuanlan.zhihu.com/p/639228219<br>https://zhuanlan.zhihu.com/p/647364218<br>https://baijiahao.baidu.com/s?id=1774803715921029316&amp;wfr=spider&amp;for=pc<br></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr></tbody></table></li></ul><h2 id="_2-metrics" tabindex="-1"><a class="header-anchor" href="#_2-metrics"><span>2. Metrics</span></a></h2><ul><li><p>参考：https://aicarrier.feishu.cn/wiki/C2NYwAJEqidHUbkfNPgcO6EsnOd</p></li><li><p>功能指标</p><table><thead><tr><th>概念</th><th>内容</th><th>备注</th></tr></thead><tbody><tr><td>Loss</td><td>模型生成文本与参考文本间的差异度量</td><td></td></tr><tr><td>Loss与标准值方差</td><td>$\\frac{(Loss-L_{mean})<sup>2+(L_{std}-L_{mean})</sup>2}{2}$ <br>$L_{std}$: 模型在基准环境中得到的Loss基准值 <br> $L_{mean}$: $Loss$ 与 $L_{std}$ 的平均值</td><td>用于评估不同硬件性能</td></tr><tr><td>PPL</td><td>标记化序列 $X = (x_0,x_1,...,x_t)$ <br> $PPL(X) = exp(-\\frac{1}{t}\\sum_i^t\\log(p_\\theta(x_i</td><td>x_{&lt;i}))$</td></tr></tbody></table></li><li><p>性能指标</p><table><thead><tr><th>概念</th><th>内容</th><th>备注</th></tr></thead><tbody><tr><td>TGS (tokens/gpu/second)</td><td>每秒单块GPU能够通过的token数量 <br> $TGS = \\frac{tokens \\ast GlobalBatchSize}{t_{epoch} \\ast n_{gpu}}$</td><td>大模型训练核心指标</td></tr><tr><td>TFlops</td><td>TeraFlops, 每秒浮点运算次数（亿次）</td><td></td></tr><tr><td>Throughput (Gbps)</td><td>吞吐率</td><td></td></tr><tr><td>BandWidth</td><td>带宽</td><td></td></tr><tr><td>save-model time (s)</td><td>模型保存时间</td><td></td></tr><tr><td>模型初始化时间</td><td>模型初始化加载时间包括权重文件读取时间、转换时间、配置读取时间、显存分配时间、参数设置时间、首次运行时构图及编译时间等推理前置步骤的耗时。</td><td>推理性能指标</td></tr><tr><td>模型初次加载时间</td><td>$模型初次加载时间=Checkpoint加载时间+参数加载时间+Tokenizer加载时间+模型初始化时间$</td><td>推理性能指标</td></tr><tr><td>响应时间</td><td>指用户从发送请求的时刻到用户收到响应结果的时刻所经过的时间；<br>对于大语言模型来说，因响应结果是流式输出，长度不确定，所以我们把这个指标转化为首字延迟（first token latency ）和 推理耗时，分别度量第一个token生成时所消耗的时间，以及平均每个token所需要的时间。</td><td>推理性能指标</td></tr><tr><td>推理耗时</td><td>对于文生图大模型，生成输出图像的耗时记为模型推理耗时，根据使用的batchsize大小可以获得吞吐量QPS（fps=frames/s）<br>$QPS=\\frac{batchsize*1000}{推理耗时}$</td><td>推理性能指标</td></tr><tr><td>Token Throughput</td><td>指每秒生成的token数量，它与响应时间、并发数密切相关，用来衡量服务的承载能力。</td><td>推理性能指标</td></tr></tbody></table></li><li><p>稳定性指标</p><table><thead><tr><th>概念</th><th>内容</th><th>备注</th></tr></thead><tbody><tr><td>前100setp loss波动</td><td>反映模型训练精度稳定性</td><td></td></tr><tr><td>最大模型训练稳定性</td><td>指定模型和规模下，模型训练3天内能否保证不崩溃（训练终止，显存溢出，loss不收敛）</td><td></td></tr><tr><td>断点续训</td><td>根据保存模型参数和优化器状态，从中断位置继续训练</td><td></td></tr></tbody></table></li><li><p>huggingface</p></li><li><p>peft:</p><ul><li>State-of-the-art Parameter-Efficient Fine-Tuning (PEFT) methods, only fine-tune a small number of (extra) model parameters, thereby greatly decreasing the computational and storage costs</li><li>Recent State-of-the-Art PEFT techniques achieve performance comparable to that of full fine-tuning.</li><li>https://github.com/huggingface/peft</li></ul></li><li><p>LoRA:</p><ul><li>Low-Rank Adaptation of Large Language Models</li><li>https://arxiv.org/abs/2106.09685</li></ul></li><li><p>PLM:</p><ul><li>pre-trained language models</li></ul></li><li></li></ul><h2 id="_2-datasets" tabindex="-1"><a class="header-anchor" href="#_2-datasets"><span>2. Datasets</span></a></h2><ul><li><p>TODO: 使用表格统计所有数据集，列出各自的特点，和链接</p></li><li><table><thead><tr><th>Datasets</th><th>Type</th><th>Size</th><th>Usage</th><th>Other</th></tr></thead><tbody><tr><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr></tbody></table></li><li><p>alpaca-data-conversation</p><ul><li></li><li><div class="language-json line-numbers-mode" data-ext="json" data-title="json"><pre class="language-json"><code><span class="token punctuation">[</span> ...
  <span class="token punctuation">{</span>
    <span class="token property">&quot;id&quot;</span><span class="token operator">:</span> <span class="token string">&quot;2&quot;</span><span class="token punctuation">,</span>
    <span class="token property">&quot;conversations&quot;</span><span class="token operator">:</span> <span class="token punctuation">[</span>
      <span class="token punctuation">{</span>
        <span class="token property">&quot;from&quot;</span><span class="token operator">:</span> <span class="token string">&quot;human&quot;</span><span class="token punctuation">,</span>
        <span class="token property">&quot;value&quot;</span><span class="token operator">:</span> <span class="token string">&quot;Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nWhat are the three primary colors?\\n\\n### Response:&quot;</span>
      <span class="token punctuation">}</span><span class="token punctuation">,</span>
      <span class="token punctuation">{</span>
        <span class="token property">&quot;from&quot;</span><span class="token operator">:</span> <span class="token string">&quot;gpt&quot;</span><span class="token punctuation">,</span>
        <span class="token property">&quot;value&quot;</span><span class="token operator">:</span> <span class="token string">&quot;The three primary colors are red, blue, and yellow.&quot;</span>
      <span class="token punctuation">}</span>
    <span class="token punctuation">]</span>
  <span class="token punctuation">}</span><span class="token punctuation">,</span>
  ...
<span class="token punctuation">]</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></li></ul></li><li><p>pile</p><ul><li><p>The Pile is a <strong>825 GiB</strong> diverse, open source language modelling data set that consists of 22 smaller, high-quality datasets combined together.</p></li><li></li><li><div class="language-text line-numbers-mode" data-ext="text" data-title="text"><pre class="language-text"><code>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div></li></ul></li></ul><h2 id="_3-models-network-structures" tabindex="-1"><a class="header-anchor" href="#_3-models-network-structures"><span>3. Models &amp; Network Structures</span></a></h2><ul><li></li></ul><h2 id="_4-distributed-training-frameworks" tabindex="-1"><a class="header-anchor" href="#_4-distributed-training-frameworks"><span>4. Distributed Training Frameworks</span></a></h2><ul><li>megatron</li><li>deepspeed</li><li>colossal AI</li><li></li></ul><h2 id="_5-challenges" tabindex="-1"><a class="header-anchor" href="#_5-challenges"><span>5. Challenges</span></a></h2><ul><li><p>知识过期，幻觉问题（知识编辑，调用外部知识库）</p></li><li><p>记忆能力：</p><ul><li>扩展 Backbone 架构的长度限制：针对 Transformers 固有的序列长度限制问题进行改进。</li><li>总结记忆（Summarizing）：对记忆进行摘要总结，增强代理从记忆中提取关键细节的能力。</li><li>压缩记忆（Compressing）：通过使用向量或适当的数据结构对记忆进行压缩，可以提高记忆检索效率。</li></ul></li><li><p>推理能力（Reasoning）：</p><ul><li>对于智能代理进行决策、分析等复杂任务而言至关重要。具体到 LLMs 上，就是以 思维链（Chain-of-Thought，CoT） 为代表的一系列提示方法。</li></ul></li><li><p>规划（Planning）：</p><ul><li>面对大型挑战时常用的策略。它帮助代理组织思维、设定目标并确定实现这些目标的步骤。在具体实现中，规划可以包含两个步骤： <ul><li>计划制定（Plan Formulation）：代理将复杂任务分解为更易于管理的子任务。例如：一次性分解再按顺序执行、逐步规划并执行、多路规划并选取最优路径等。在一些需要专业知识的场景中，代理可与特定领域的 Planner 模块集成，提升能力。</li><li>计划反思（Plan Reflection）：在制定计划后，可以进行反思并评估其优劣。这种反思一般来自三个方面：借助内部反馈机制；与人类互动获得反馈；从环境中获得反馈。</li></ul></li></ul></li><li><p>迁移性 &amp; 泛化性：</p><ul><li>拥有世界知识的 LLMs 赋予智能代理具备强大的迁移与泛化能力。一个好的代理不是静态的知识库，还应具备动态的学习能力： <ul><li>对未知任务的泛化：随着模型规模与训练数据的增大，LLMs 在解决未知任务上涌现出了惊人的能力。通过指令微调的大模型在 zero-shot 测试中表现良好，在许多任务上都取得了不亚于专家模型的成绩。</li><li>情景学习（In-context Learning）：大模型不仅能够从上下文的少量示例中进行类比学习，这种能力还可以扩展到文本以外的多模态场景，为代理在现实世界中的应用提供了更多可能性。</li><li>持续学习（Continual Learning）：持续学习的主要挑战是灾难性遗忘，即当模型学习新任务时容易丢失过往任务中的知识。专有领域的智能代理应当尽量避免丢失通用领域的知识。</li></ul></li></ul></li><li><p>视觉输入：</p><ul><li><p>LLMs 本身并不具备视觉的感知能力，只能理解离散的文本内容。而视觉输入通常包含有关世界的大量信息，包括对象的属性，空间关系，场景布局等等。常见的方法有：</p><ul><li><p>将视觉输入转为对应的文本描述（Image Captioning）：可以被 LLMs 直接理解，并且可解释性高。</p></li><li><p>对视觉信息进行编码表示：以视觉基础模型 + LLMs 的范式来构成感知模块，通过对齐操作来让模型理解不同模态的内容，可以端到端的方式进行训练。</p></li></ul></li></ul></li><li><p>听觉输入：</p><ul><li>听觉也是人类感知中的重要组成部分。由于 LLMs 有着优秀的工具调用能力，一个直观的想法就是：代理可以将 LLMs 作为控制枢纽，通过级联的方式调用现有的工具集或者专家模型，感知音频信息。此外，音频也可以通过频谱图（Spectrogram）的方式进行直观表示。频谱图可以作为平面图像来展示 2D 信息，因此，一些视觉的处理方法可以迁移到语音领域。</li></ul></li><li><p>工具使用：</p><ul><li>尽管 LLMs 拥有出色的知识储备和专业能力，但在面对具体问题时，也可能会出现鲁棒性问题、幻觉等一系列挑战。与此同时，工具作为使用者能力的扩展，可以在专业性、事实性、可解释性等方面提供帮助。例如，可以通过使用计算器来计算数学问题、使用搜索引擎来搜寻实时信息。</li><li>另外，工具也可以扩展智能代理的行动空间。例如，通过调用语音生成、图像生成等专家模型，来获得多模态的行动方式。因此，如何让代理成为优秀的工具使用者，即学会如何有效地利用工具，是非常重要且有前景的方向。</li><li>目前，主要的工具学习方法包括从演示中学习和从反馈中学习。此外，也可以通过元学习、课程学习等方式来让代理程序在使用各种工具方面具备泛化能力。更进一步，智能代理还可以进一步学习如何「自给自足」地制造工具，从而提高其自主性和独立性。</li></ul></li><li><p>具身行动：</p><ul><li>具身（Embodyment）是指代理与环境交互过程中，理解、改造环境并更新自身状态的能力。具身行动（Embodied Action）被视为虚拟智能与物理现实的互通桥梁。</li><li>传统的基于强化学习的 Agent 在样本效率、泛化性和复杂问题推理等方面存在局限性，而 LLM-based Agents 通过引入大模型丰富的内在知识，使得 Embodied Agent 能够像人类一样主动感知、影响物理环境。根据代理在任务中的自主程度或者说 Action 的复杂程度，可以有以下的原子 Action： <ul><li>Observation 可以帮助智能代理在环境中定位自身位置、感知对象物品和获取其他环境信息；</li><li>Manipulation 则是完成一些具体的抓取、推动等操作任务；</li><li>Navigation 要求智能代理根据任务目标变换自身位置并根据环境信息更新自身状态。</li></ul></li></ul></li><li></li></ul>`,13),i=[d];function s(o,l){return n(),e("div",null,i)}const c=t(r,[["render",s],["__file","LLM.html.vue"]]),u=JSON.parse(`{"path":"/notes/LLM.html","title":"LLM","lang":"zh-CN","frontmatter":{"date":"2023-12-25T00:00:00.000Z","tag":["LLM"],"category":["LLM"],"description":"LLM 1. Conceptions 2. Metrics 参考：https://aicarrier.feishu.cn/wiki/C2NYwAJEqidHUbkfNPgcO6EsnOd 功能指标 性能指标 稳定性指标 huggingface peft: State-of-the-art Parameter-Efficient Fine-Tuning ...","head":[["meta",{"property":"og:url","content":"https://bradzhone.github.io/notes/LLM.html"}],["meta",{"property":"og:site_name","content":"BradZhone's Blog"}],["meta",{"property":"og:title","content":"LLM"}],["meta",{"property":"og:description","content":"LLM 1. Conceptions 2. Metrics 参考：https://aicarrier.feishu.cn/wiki/C2NYwAJEqidHUbkfNPgcO6EsnOd 功能指标 性能指标 稳定性指标 huggingface peft: State-of-the-art Parameter-Efficient Fine-Tuning ..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"article:author","content":"BradZhone"}],["meta",{"property":"article:tag","content":"LLM"}],["meta",{"property":"article:published_time","content":"2023-12-25T00:00:00.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"LLM\\",\\"image\\":[\\"\\"],\\"datePublished\\":\\"2023-12-25T00:00:00.000Z\\",\\"dateModified\\":null,\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"BradZhone\\",\\"url\\":\\"https://github.com/BradZhone\\"}]}"]]},"headers":[{"level":2,"title":"1. Conceptions","slug":"_1-conceptions","link":"#_1-conceptions","children":[]},{"level":2,"title":"2. Metrics","slug":"_2-metrics","link":"#_2-metrics","children":[]},{"level":2,"title":"2. Datasets","slug":"_2-datasets","link":"#_2-datasets","children":[]},{"level":2,"title":"3. Models & Network Structures","slug":"_3-models-network-structures","link":"#_3-models-network-structures","children":[]},{"level":2,"title":"4. Distributed Training Frameworks","slug":"_4-distributed-training-frameworks","link":"#_4-distributed-training-frameworks","children":[]},{"level":2,"title":"5. Challenges","slug":"_5-challenges","link":"#_5-challenges","children":[]}],"git":{"createdTime":null,"updatedTime":null,"contributors":[]},"readingTime":{"minutes":12.22,"words":3666},"filePathRelative":"notes/LLM.md","localizedDate":"2023年12月25日","excerpt":"\\n<h2>1. Conceptions</h2>\\n<ul>\\n<li>\\n<table>\\n<thead>\\n<tr>\\n<th>概念</th>\\n<th>内容</th>\\n<th>备注</th>\\n</tr>\\n</thead>\\n<tbody>\\n<tr>\\n<td>token/word segmentation</td>\\n<td>将原始文本切分成子单元的过程就叫做 Tokenization。即，按照特定需求把文本中的句子、段落切分成一个字符串序列（其中的元素通常称为token 或叫词语），方便后续的处理分析工作。它是一个离散的文本单元，它可以是单词、标点符号、数字或其他语言元素，这些元素被用作训练和生成文本的基本单位。Token有粒度之分，如：词、字符或subword</td>\\n<td>解读资料：https://zhuanlan.zhihu.com/p/444774532</td>\\n</tr>\\n<tr>\\n<td>困惑度 Perplexity (PPL)</td>\\n<td>困惑度是一种用来评估语言模型的指标。它衡量了一个语言模型在给定数据集上的预测能力和概率分布的复杂性。较低的困惑度表示模型对数据集的拟合效果更好，也就是说，模型更能准确地预测下一个词或下一个句子。</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>SFT</td>\\n<td>监督微调（Supervised Fine-Tuning）</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>RLHF</td>\\n<td>基于人类反馈的强化学习 (Reinforcement Learning from Human Feedback)</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>embedding</td>\\n<td>（词）嵌入，可用于降维或升维，将特征拉近拉远到一个合适的观察点，将不同特征联系起来</td>\\n<td>https://zhuanlan.zhihu.com/p/616419336</td>\\n</tr>\\n<tr>\\n<td>seq2seq</td>\\n<td>序列到序列，该技术突破了传统的固定大小输入问题框架，提出了一种全新的端到端的映射方法。技术的核心是 Encoder-Decoder 架构，Encoder 负责将输入序列压缩成指定长度的向量，这个向量就可以看成是这个序列的语义，称为编码。而 Decoder 则负责根据语义向量生成指定的序列，这个过程也称为解码。Seq2Seq 是输出的长度不确定时采用的模型，因此在机器翻译、对话系统、自动文摘等自然语言处理任务中被广泛运用。</td>\\n<td>https://zhuanlan.zhihu.com/p/558138527<br>https://zhuanlan.zhihu.com/p/520657912?utm_id=0</td>\\n</tr>\\n<tr>\\n<td>attention</td>\\n<td>注意力机制（Attention Mechanism）源于对人类视觉的研究，主要理论原理是：信息处理时选择性地关注所有信息的一部分，同时忽略其他可见的信息。在计算能力有限情况下，注意力机制是解决信息超载问题的主要手段的一种资源分配方案，将计算资源分配给更重要的任务。</td>\\n<td>attention机制：https://zhuanlan.zhihu.com/p/46990010<br>atention图解：https://zhuanlan.zhihu.com/p/342235515<br>https://zhuanlan.zhihu.com/p/42724582<br>https://zhuanlan.zhihu.com/p/53682800<br>如何理解attention中的QKV：https://www.zhihu.com/question/298810062<br>transformer中的attention为什么scaled：https://www.zhihu.com/question/339723385<br>attention计算公式中的softmax：https://zhuanlan.zhihu.com/p/157490738<br></td>\\n</tr>\\n<tr>\\n<td>transformer</td>\\n<td>Transformer最早起源于论文Attention is all your need，是谷歌云TPU推荐的参考模型。 目前，在NLP领域当中，主要存在三种特征处理器：CNN、RNN以及Transformer，当前Transformer的流行程度已经大过CNN和RNN，它抛弃了传统CNN和RNN神经网络，整个网络结构完全由Attention机制以及前馈神经网络组成。论文中给出Transformer的定义是：Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence aligned RNNs or convolution。</td>\\n<td>论文 ：https://arxiv.org/abs/1706.03762<br>源码参考：https://github.com/huggingface/transformers<br>解读资料：https://blog.csdn.net/m0_67505927/article/details/123209347<br>https://blog.csdn.net/qq_52302919/article/details/122207924<br>B站李宏毅的视频，有几节专门讲attention和transformer的，比较清晰：https://www.bilibili.com/video/BV1Wv411h7kN?p=23<br></td>\\n</tr>\\n<tr>\\n<td>Beam search</td>\\n<td>集束搜索。一种搜索算法，是对greedy search的一个改进，相对greedy search扩大了搜索空间，但远不及穷举搜索指数级的搜索空间，是二者的一个折中方案。在文本生成任务中常用的解码策略。</td>\\n<td>https://zhuanlan.zhihu.com/p/82829880</td>\\n</tr>\\n<tr>\\n<td>AIGC (Artificial Intelligence Generated Content / AI-Generated Content)</td>\\n<td>人工智能生成内容，一种利用人工智能技术自动生成文章、音频、视频等多媒体内容的方法。</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>LLM ( Large Language Model)</td>\\n<td>大语言模型，使用大量文本数据训练的深度学习模型，可以生成自然语言文本或理解语言文本的含义。特点是规模庞大（包含数十亿参数），能学习到语言数据中的复杂模式。</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>GPT (Generative Pre-trained Transformer)</td>\\n<td>生成式预训练Transformer模型, 由OpenAI提出的一系列强大的预训练语言模型，兼具“大规模”和“预训练”两种属性。</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>BERT (Bidirectional Encoder Representations from Transformers)</td>\\n<td>基于Transformer的双向编码表征，谷歌提出的一个LLM，对NLP研究产生了重大影响。该模型使用双向方法从一个词的左右两边捕捉上下文，使得各种任务的性能提高，如情感分析和命名实体识别。</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>Bloom ( BigScience Large Open-science Open-access Multi-lingual Language Model)</td>\\n<td>BLOOM是一种基于trasnformer架构的解码器（Decoder-Only）自回归大语言模型，由BigScience社区开发和发布。该模型除了176B 之外，还提供了几个较小的模型，其模型参数规模为：560M，1.1B，1.7B，3B 和7.1B。</td>\\n<td>论文：https://arxiv.org/abs/2211.05100<br>解读：https://zhuanlan.zhihu.com/p/640016830</td>\\n</tr>\\n<tr>\\n<td>Flash Attention</td>\\n<td>FlashAttention 是一种重新排序注意力计算的算法，它无需任何近似即可加速注意力计算并减少内存占用，主要解决Transformer计算速度慢和存储占用高的问题，所以作为目前LLM的模型加速它是一个非常好的解决方案。</td>\\n<td>论文：<br>https://arxiv.org/abs/2205.14135<br>解读资料：<br>https://zhuanlan.zhihu.com/p/639228219<br>https://zhuanlan.zhihu.com/p/647364218<br>https://baijiahao.baidu.com/s?id=1774803715921029316&amp;wfr=spider&amp;for=pc<br></td>\\n</tr>\\n<tr>\\n<td></td>\\n<td></td>\\n<td></td>\\n</tr>\\n<tr>\\n<td></td>\\n<td></td>\\n<td></td>\\n</tr>\\n<tr>\\n<td></td>\\n<td></td>\\n<td></td>\\n</tr>\\n<tr>\\n<td></td>\\n<td></td>\\n<td></td>\\n</tr>\\n</tbody>\\n</table>\\n</li>\\n</ul>","autoDesc":true}`);export{c as comp,u as data};
