<!doctype html>
<html lang="zh-CN" data-theme="light">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width,initial-scale=1" />
    <meta name="generator" content="VuePress 2.0.0-rc.9" />
    <meta name="theme" content="VuePress Theme Hope 2.0.0-rc.32" />
    <style>
      html {
        background: var(--bg-color, #fff);
      }

      html[data-theme="dark"] {
        background: var(--bg-color, #1d1e1f);
      }

      body {
        background: var(--bg-color);
      }
    </style>
    <script>
      const userMode = localStorage.getItem("vuepress-theme-hope-scheme");
      const systemDarkMode =
        window.matchMedia &&
        window.matchMedia("(prefers-color-scheme: dark)").matches;

      if (userMode === "dark" || (userMode !== "light" && systemDarkMode)) {
        document.documentElement.setAttribute("data-theme", "dark");
      }
    </script>
    <meta property="og:url" content="https://bradzhone.github.io/notes/LLM.html"><meta property="og:site_name" content="BradZhone's Blog"><meta property="og:title" content="LLM"><meta property="og:description" content="LLM 1. Conceptions 2. Metrics 参考：https://aicarrier.feishu.cn/wiki/C2NYwAJEqidHUbkfNPgcO6EsnOd 功能指标 性能指标 稳定性指标 huggingface peft: State-of-the-art Parameter-Efficient Fine-Tuning ..."><meta property="og:type" content="article"><meta property="og:locale" content="zh-CN"><meta property="article:author" content="BradZhone"><meta property="article:tag" content="LLM"><meta property="article:published_time" content="2023-12-25T00:00:00.000Z"><script type="application/ld+json">{"@context":"https://schema.org","@type":"Article","headline":"LLM","image":[""],"datePublished":"2023-12-25T00:00:00.000Z","dateModified":null,"author":[{"@type":"Person","name":"BradZhone","url":"https://github.com/BradZhone"}]}</script><title>LLM | BradZhone's Blog</title><meta name="description" content="LLM 1. Conceptions 2. Metrics 参考：https://aicarrier.feishu.cn/wiki/C2NYwAJEqidHUbkfNPgcO6EsnOd 功能指标 性能指标 稳定性指标 huggingface peft: State-of-the-art Parameter-Efficient Fine-Tuning ...">
    <link rel="preload" href="/assets/style-BkjCpNEe.css" as="style"><link rel="stylesheet" href="/assets/style-BkjCpNEe.css">
    <link rel="modulepreload" href="/assets/app-Coh1oo3x.js"><link rel="modulepreload" href="/assets/LLM.html-9hu1pKVY.js"><link rel="modulepreload" href="/assets/plugin-vue_export-helper-DlAUqK2U.js">
    <link rel="prefetch" href="/assets/intro.html-D9dwEpHI.js" as="script"><link rel="prefetch" href="/assets/index.html-D5LnkeFh.js" as="script"><link rel="prefetch" href="/assets/CRNN_blog.html-Cb6b7OD4.js" as="script"><link rel="prefetch" href="/assets/cucollection_blog.html-DLix_uWp.js" as="script"><link rel="prefetch" href="/assets/cuda_blog.html-CBZ1Rex-.js" as="script"><link rel="prefetch" href="/assets/distributed_embeddings_blog.html-Cw7oBr-d.js" as="script"><link rel="prefetch" href="/assets/howToBuildThisBlog.html-Dxn0xRj3.js" as="script"><link rel="prefetch" href="/assets/hugectr_blog.html-BrFINEES.js" as="script"><link rel="prefetch" href="/assets/hugectr_src_blog.html-d9qgPAFk.js" as="script"><link rel="prefetch" href="/assets/index.html-DI16Z1C4.js" as="script"><link rel="prefetch" href="/assets/torchrec_cn_embedding_note.html-YEVLbIAq.js" as="script"><link rel="prefetch" href="/assets/warpcore_blog.html-Cz_5IJvA.js" as="script"><link rel="prefetch" href="/assets/c___note.html-BYbhq9Nq.js" as="script"><link rel="prefetch" href="/assets/deep_learning.html-b-b3zdJE.js" as="script"><link rel="prefetch" href="/assets/linux_command.html-G4dNyKFL.js" as="script"><link rel="prefetch" href="/assets/loss.html-CJxfnubw.js" as="script"><link rel="prefetch" href="/assets/markdown.html-DJcQDvEC.js" as="script"><link rel="prefetch" href="/assets/metrics.html-DKJGeFbq.js" as="script"><link rel="prefetch" href="/assets/PLAN_Z.html-B9yJBwFA.js" as="script"><link rel="prefetch" href="/assets/precision.html-B_dd_MdY.js" as="script"><link rel="prefetch" href="/assets/index.html-D1wTsZoY.js" as="script"><link rel="prefetch" href="/assets/torchrec_note.html-B-i_O6mg.js" as="script"><link rel="prefetch" href="/assets/uml_note.html-IaIR5y-b.js" as="script"><link rel="prefetch" href="/assets/index.html-Cgwhpoct.js" as="script"><link rel="prefetch" href="/assets/404.html-DYVdKbdY.js" as="script"><link rel="prefetch" href="/assets/index.html-1jQ9-Uww.js" as="script"><link rel="prefetch" href="/assets/index.html-BEDqx9YQ.js" as="script"><link rel="prefetch" href="/assets/index.html-BltlhrdK.js" as="script"><link rel="prefetch" href="/assets/index.html-C0yITs6x.js" as="script"><link rel="prefetch" href="/assets/index.html-maJbTduo.js" as="script"><link rel="prefetch" href="/assets/index.html-BPQ9UPx_.js" as="script"><link rel="prefetch" href="/assets/index.html-BQB6pLKG.js" as="script"><link rel="prefetch" href="/assets/index.html-Be8HnZrW.js" as="script"><link rel="prefetch" href="/assets/index.html-DzX6xTUq.js" as="script"><link rel="prefetch" href="/assets/index.html-BxZ6QV-X.js" as="script"><link rel="prefetch" href="/assets/index.html-Dl0UMf-I.js" as="script"><link rel="prefetch" href="/assets/index.html-xBRd_OsE.js" as="script"><link rel="prefetch" href="/assets/index.html-BTkUZ9Ws.js" as="script"><link rel="prefetch" href="/assets/index.html-D57wSldI.js" as="script"><link rel="prefetch" href="/assets/index.html-Dqr1foaP.js" as="script"><link rel="prefetch" href="/assets/index.html-CSGzSmAq.js" as="script"><link rel="prefetch" href="/assets/index.html-BnBAmG4V.js" as="script"><link rel="prefetch" href="/assets/index.html-d2G8e39M.js" as="script"><link rel="prefetch" href="/assets/index.html-CpCodvQl.js" as="script"><link rel="prefetch" href="/assets/index.html-hEB5WJuD.js" as="script"><link rel="prefetch" href="/assets/index.html-DX2Ijbyk.js" as="script"><link rel="prefetch" href="/assets/index.html-Da6Vp1bl.js" as="script"><link rel="prefetch" href="/assets/index.html-CGr0tdaC.js" as="script"><link rel="prefetch" href="/assets/index.html-D5fEOhj5.js" as="script"><link rel="prefetch" href="/assets/index.html-C1vK6J6C.js" as="script"><link rel="prefetch" href="/assets/index.html-DgD_mq8U.js" as="script"><link rel="prefetch" href="/assets/index.html-Demc7CPi.js" as="script"><link rel="prefetch" href="/assets/index.html-D31kcDwM.js" as="script"><link rel="prefetch" href="/assets/index.html-COTqqkDG.js" as="script"><link rel="prefetch" href="/assets/index.html-DtZDr4LJ.js" as="script"><link rel="prefetch" href="/assets/index.html-BYDcpP-H.js" as="script"><link rel="prefetch" href="/assets/index.html-QpKn8zOy.js" as="script"><link rel="prefetch" href="/assets/index.html-Fd1nQRc_.js" as="script"><link rel="prefetch" href="/assets/index.html-BEVLRn0k.js" as="script"><link rel="prefetch" href="/assets/giscus-7BMGhbDA.js" as="script"><link rel="prefetch" href="/assets/photoswipe.esm-SzV8tJDW.js" as="script">
  </head>
  <body>
    <div id="app"><!--[--><!--[--><!--[--><span tabindex="-1"></span><a href="#main-content" class="vp-skip-link sr-only">跳至主要內容</a><!--]--><div class="theme-container has-toc"><!--[--><header id="navbar" class="vp-navbar"><div class="vp-navbar-start"><button type="button" class="vp-toggle-sidebar-button" title="Toggle Sidebar"><span class="icon"></span></button><!--[--><!----><!--]--><!--[--><a class="route-link vp-brand" href="/"><img class="vp-nav-logo light" src="/light.png" alt><img class="vp-nav-logo dark" src="/dark.png" alt><span class="vp-site-name hide-in-pad">BradZhone&#39;s Blog</span></a><!--]--><!--[--><!----><!--]--></div><div class="vp-navbar-center"><!--[--><!----><!--]--><!--[--><nav class="vp-nav-links"><div class="nav-item hide-in-mobile"><a class="route-link nav-link" href="/" aria-label="Home"><span class="font-icon icon fa-fw fa-sm fas fa-home" style=""></span>Home<!----></a></div><div class="nav-item hide-in-mobile"><a class="route-link nav-link" href="/blogs/" aria-label="Blogs"><span class="font-icon icon fa-fw fa-sm fas fa-blog" style=""></span>Blogs<!----></a></div><div class="nav-item hide-in-mobile"><a class="route-link nav-link active" href="/notes/" aria-label="Notes"><span class="font-icon icon fa-fw fa-sm fas fa-note-sticky" style=""></span>Notes<!----></a></div><div class="nav-item hide-in-mobile"><a class="route-link nav-link" href="/thinking/" aria-label="Thinking"><span class="font-icon icon fa-fw fa-sm fas fa-lightbulb" style=""></span>Thinking<!----></a></div><div class="nav-item hide-in-mobile"><a class="route-link nav-link" href="/intro.html" aria-label="About Me"><span class="font-icon icon fa-fw fa-sm fas fa-star" style=""></span>About Me<!----></a></div></nav><!--]--><!--[--><!----><!--]--></div><div class="vp-navbar-end"><!--[--><!----><!--]--><!--[--><!----><!----><div class="nav-item hide-in-mobile"><button type="button" id="appearance-switch"><svg xmlns="http://www.w3.org/2000/svg" class="icon auto-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="auto icon" style="display:none;"><path d="M512 992C246.92 992 32 777.08 32 512S246.92 32 512 32s480 214.92 480 480-214.92 480-480 480zm0-840c-198.78 0-360 161.22-360 360 0 198.84 161.22 360 360 360s360-161.16 360-360c0-198.78-161.22-360-360-360zm0 660V212c165.72 0 300 134.34 300 300 0 165.72-134.28 300-300 300z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" class="icon dark-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="dark icon" style="display:none;"><path d="M524.8 938.667h-4.267a439.893 439.893 0 0 1-313.173-134.4 446.293 446.293 0 0 1-11.093-597.334A432.213 432.213 0 0 1 366.933 90.027a42.667 42.667 0 0 1 45.227 9.386 42.667 42.667 0 0 1 10.24 42.667 358.4 358.4 0 0 0 82.773 375.893 361.387 361.387 0 0 0 376.747 82.774 42.667 42.667 0 0 1 54.187 55.04 433.493 433.493 0 0 1-99.84 154.88 438.613 438.613 0 0 1-311.467 128z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" class="icon light-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="light icon" style="display:block;"><path d="M952 552h-80a40 40 0 0 1 0-80h80a40 40 0 0 1 0 80zM801.88 280.08a41 41 0 0 1-57.96-57.96l57.96-58a41.04 41.04 0 0 1 58 58l-58 57.96zM512 752a240 240 0 1 1 0-480 240 240 0 0 1 0 480zm0-560a40 40 0 0 1-40-40V72a40 40 0 0 1 80 0v80a40 40 0 0 1-40 40zm-289.88 88.08-58-57.96a41.04 41.04 0 0 1 58-58l57.96 58a41 41 0 0 1-57.96 57.96zM192 512a40 40 0 0 1-40 40H72a40 40 0 0 1 0-80h80a40 40 0 0 1 40 40zm30.12 231.92a41 41 0 0 1 57.96 57.96l-57.96 58a41.04 41.04 0 0 1-58-58l58-57.96zM512 832a40 40 0 0 1 40 40v80a40 40 0 0 1-80 0v-80a40 40 0 0 1 40-40zm289.88-88.08 58 57.96a41.04 41.04 0 0 1-58 58l-57.96-58a41 41 0 0 1 57.96-57.96z"></path></svg></button></div><!----><!--]--><!--[--><!----><!--]--><button type="button" class="vp-toggle-navbar-button" aria-label="Toggle Navbar" aria-expanded="false" aria-controls="nav-screen"><span><span class="vp-top"></span><span class="vp-middle"></span><span class="vp-bottom"></span></span></button></div></header><!----><!--]--><!----><div class="toggle-sidebar-wrapper"><span class="arrow start"></span></div><aside id="sidebar" class="vp-sidebar"><!--[--><!----><!--]--><ul class="vp-sidebar-links"><li><section class="vp-sidebar-group"><p class="vp-sidebar-header clickable"><span class="font-icon icon fa-fw fa-sm fas fa-blog" style=""></span><a class="route-link nav-link vp-sidebar-title" href="/blogs/" aria-label="Blogs"><!---->Blogs<!----></a><!----></p><ul class="vp-sidebar-links"><li><a class="route-link nav-link vp-sidebar-link vp-sidebar-page" href="/blogs/howToBuildThisBlog.html" aria-label="如何搭建本博客"><!---->如何搭建本博客<!----></a></li><li><a class="route-link nav-link vp-sidebar-link vp-sidebar-page" href="/blogs/CRNN_blog.html" aria-label="CRNN网络调研适配记录"><!---->CRNN网络调研适配记录<!----></a></li><li><a class="route-link nav-link vp-sidebar-link vp-sidebar-page" href="/blogs/cucollection_blog.html" aria-label="cuCollections性能测试"><!---->cuCollections性能测试<!----></a></li><li><a class="route-link nav-link vp-sidebar-link vp-sidebar-page" href="/blogs/cuda_blog.html" aria-label="CUDA学习笔记"><!---->CUDA学习笔记<!----></a></li><li><a class="route-link nav-link vp-sidebar-link vp-sidebar-page" href="/blogs/distributed_embeddings_blog.html" aria-label="Distributed_embeddings"><!---->Distributed_embeddings<!----></a></li><li><a class="route-link nav-link vp-sidebar-link vp-sidebar-page" href="/blogs/hugectr_blog.html" aria-label="HugeCTR 学习笔记"><!---->HugeCTR 学习笔记<!----></a></li><li><a class="route-link nav-link vp-sidebar-link vp-sidebar-page" href="/blogs/hugectr_src_blog.html" aria-label="HugeCTR源码阅读笔记"><!---->HugeCTR源码阅读笔记<!----></a></li><li><a class="route-link nav-link vp-sidebar-link vp-sidebar-page" href="/blogs/torchrec_cn_embedding_note.html" aria-label="torchrec cn_embedding模块设计方案"><!---->torchrec cn_embedding模块设计方案<!----></a></li><li><a class="route-link nav-link vp-sidebar-link vp-sidebar-page" href="/blogs/warpcore_blog.html" aria-label="WARPCORE 学习笔记"><!---->WARPCORE 学习笔记<!----></a></li></ul></section></li><li><section class="vp-sidebar-group"><p class="vp-sidebar-header clickable active"><span class="font-icon icon fa-fw fa-sm fas fa-note-sticky" style=""></span><a class="route-link nav-link active vp-sidebar-title" href="/notes/" aria-label="Notes"><!---->Notes<!----></a><!----></p><ul class="vp-sidebar-links"><li><a class="route-link nav-link vp-sidebar-link vp-sidebar-page" href="/notes/c___note.html" aria-label="C++ note"><!---->C++ note<!----></a></li><li><a class="route-link nav-link vp-sidebar-link vp-sidebar-page" href="/notes/deep_learning.html" aria-label="DL相关"><!---->DL相关<!----></a></li><li><a class="route-link nav-link vp-sidebar-link vp-sidebar-page" href="/notes/precision.html" aria-label="Floating Point precision formats"><!---->Floating Point precision formats<!----></a></li><li><a class="route-link nav-link vp-sidebar-link vp-sidebar-page" href="/notes/linux_command.html" aria-label="Linux 快捷指令"><!---->Linux 快捷指令<!----></a></li><li><a class="route-link nav-link active vp-sidebar-link vp-sidebar-page active" href="/notes/LLM.html" aria-label="LLM"><!---->LLM<!----></a></li><li><a class="route-link nav-link vp-sidebar-link vp-sidebar-page" href="/notes/loss.html" aria-label="Loss 相关问题"><!---->Loss 相关问题<!----></a></li><li><a class="route-link nav-link vp-sidebar-link vp-sidebar-page" href="/notes/markdown.html" aria-label="Markdown 语法"><!---->Markdown 语法<!----></a></li><li><a class="route-link nav-link vp-sidebar-link vp-sidebar-page" href="/notes/metrics.html" aria-label="Metrics"><!---->Metrics<!----></a></li><li><a class="route-link nav-link vp-sidebar-link vp-sidebar-page" href="/notes/PLAN_Z.html" aria-label="TODO LIST"><!---->TODO LIST<!----></a></li><li><a class="route-link nav-link vp-sidebar-link vp-sidebar-page" href="/notes/torchrec_note.html" aria-label="Torchrec调研"><!---->Torchrec调研<!----></a></li><li><a class="route-link nav-link vp-sidebar-link vp-sidebar-page" href="/notes/uml_note.html" aria-label="UML学习笔记"><!---->UML学习笔记<!----></a></li></ul></section></li><li><section class="vp-sidebar-group"><p class="vp-sidebar-header clickable"><span class="font-icon icon fa-fw fa-sm fas fa-lightbulb" style=""></span><a class="route-link nav-link vp-sidebar-title" href="/thinking/" aria-label="Thinking"><!---->Thinking<!----></a><!----></p><ul class="vp-sidebar-links"></ul></section></li></ul><!--[--><!----><!--]--></aside><!--[--><main id="main-content" class="vp-page"><!--[--><!--[--><!----><!--]--><!----><nav class="vp-breadcrumb disable"></nav><div class="vp-page-title"><h1><!---->LLM</h1><div class="page-info"><span class="page-author-info" aria-label="作者🖊" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon author-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="author icon"><path d="M649.6 633.6c86.4-48 147.2-144 147.2-249.6 0-160-128-288-288-288s-288 128-288 288c0 108.8 57.6 201.6 147.2 249.6-121.6 48-214.4 153.6-240 288-3.2 9.6 0 19.2 6.4 25.6 3.2 9.6 12.8 12.8 22.4 12.8h704c9.6 0 19.2-3.2 25.6-12.8 6.4-6.4 9.6-16 6.4-25.6-25.6-134.4-121.6-240-243.2-288z"></path></svg><span><a class="page-author-item" href="https://github.com/BradZhone" target="_blank" rel="noopener noreferrer">BradZhone</a></span><span property="author" content="BradZhone"></span></span><!----><span class="page-date-info" aria-label="写作日期📅" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon calendar-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="calendar icon"><path d="M716.4 110.137c0-18.753-14.72-33.473-33.472-33.473-18.753 0-33.473 14.72-33.473 33.473v33.473h66.993v-33.473zm-334.87 0c0-18.753-14.72-33.473-33.473-33.473s-33.52 14.72-33.52 33.473v33.473h66.993v-33.473zm468.81 33.52H716.4v100.465c0 18.753-14.72 33.473-33.472 33.473a33.145 33.145 0 01-33.473-33.473V143.657H381.53v100.465c0 18.753-14.72 33.473-33.473 33.473a33.145 33.145 0 01-33.473-33.473V143.657H180.6A134.314 134.314 0 0046.66 277.595v535.756A134.314 134.314 0 00180.6 947.289h669.74a134.36 134.36 0 00133.94-133.938V277.595a134.314 134.314 0 00-133.94-133.938zm33.473 267.877H147.126a33.145 33.145 0 01-33.473-33.473c0-18.752 14.72-33.473 33.473-33.473h736.687c18.752 0 33.472 14.72 33.472 33.473a33.145 33.145 0 01-33.472 33.473z"></path></svg><span><!----></span><meta property="datePublished" content="2023-12-25T00:00:00.000Z"></span><!----><span class="page-reading-time-info" aria-label="阅读时间⌛" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon timer-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="timer icon"><path d="M799.387 122.15c4.402-2.978 7.38-7.897 7.38-13.463v-1.165c0-8.933-7.38-16.312-16.312-16.312H256.33c-8.933 0-16.311 7.38-16.311 16.312v1.165c0 5.825 2.977 10.874 7.637 13.592 4.143 194.44 97.22 354.963 220.201 392.763-122.204 37.542-214.893 196.511-220.2 389.397-4.661 5.049-7.638 11.651-7.638 19.03v5.825h566.49v-5.825c0-7.379-2.849-13.981-7.509-18.9-5.049-193.016-97.867-351.985-220.2-389.527 123.24-37.67 216.446-198.453 220.588-392.892zM531.16 450.445v352.632c117.674 1.553 211.787 40.778 211.787 88.676H304.097c0-48.286 95.149-87.382 213.728-88.676V450.445c-93.077-3.107-167.901-81.297-167.901-177.093 0-8.803 6.99-15.793 15.793-15.793 8.803 0 15.794 6.99 15.794 15.793 0 80.261 63.69 145.635 142.01 145.635s142.011-65.374 142.011-145.635c0-8.803 6.99-15.793 15.794-15.793s15.793 6.99 15.793 15.793c0 95.019-73.789 172.82-165.96 177.093z"></path></svg><span>大约 12 分钟</span><meta property="timeRequired" content="PT12M"></span><span class="page-category-info" aria-label="分类🌈" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon category-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="category icon"><path d="M148.41 106.992h282.176c22.263 0 40.31 18.048 40.31 40.31V429.48c0 22.263-18.047 40.31-40.31 40.31H148.41c-22.263 0-40.311-18.047-40.311-40.31V147.302c0-22.263 18.048-40.31 40.311-40.31zM147.556 553.478H429.73c22.263 0 40.311 18.048 40.311 40.31v282.176c0 22.263-18.048 40.312-40.31 40.312H147.555c-22.263 0-40.311-18.049-40.311-40.312V593.79c0-22.263 18.048-40.311 40.31-40.311zM593.927 106.992h282.176c22.263 0 40.31 18.048 40.31 40.31V429.48c0 22.263-18.047 40.31-40.31 40.31H593.927c-22.263 0-40.311-18.047-40.311-40.31V147.302c0-22.263 18.048-40.31 40.31-40.31zM730.22 920.502H623.926c-40.925 0-74.22-33.388-74.22-74.425V623.992c0-41.038 33.387-74.424 74.425-74.424h222.085c41.038 0 74.424 33.226 74.424 74.067v114.233c0 10.244-8.304 18.548-18.547 18.548s-18.548-8.304-18.548-18.548V623.635c0-20.388-16.746-36.974-37.33-36.974H624.13c-20.585 0-37.331 16.747-37.331 37.33v222.086c0 20.585 16.654 37.331 37.126 37.331H730.22c10.243 0 18.547 8.304 18.547 18.547 0 10.244-8.304 18.547-18.547 18.547z"></path></svg><!--[--><span class="page-category-item category6 clickable" role="navigation">LLM</span><!--]--><meta property="articleSection" content="LLM"></span><span class="page-tag-info" aria-label="标签🏷" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon tag-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="tag icon"><path d="M939.902 458.563L910.17 144.567c-1.507-16.272-14.465-29.13-30.737-30.737L565.438 84.098h-.402c-3.215 0-5.726 1.005-7.634 2.913l-470.39 470.39a10.004 10.004 0 000 14.164l365.423 365.424c1.909 1.908 4.42 2.913 7.132 2.913s5.223-1.005 7.132-2.913l470.39-470.39c2.01-2.11 3.014-5.023 2.813-8.036zm-240.067-72.121c-35.458 0-64.286-28.828-64.286-64.286s28.828-64.285 64.286-64.285 64.286 28.828 64.286 64.285-28.829 64.286-64.286 64.286z"></path></svg><!--[--><span class="page-tag-item tag6 clickable" role="navigation">LLM</span><!--]--><meta property="keywords" content="LLM"></span></div><hr></div><div class="vp-toc-placeholder"><aside id="toc"><!--[--><!----><!--]--><div class="vp-toc-header">此页内容<button type="button" class="print-button" title="打印"><svg xmlns="http://www.w3.org/2000/svg" class="icon print-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="print icon"><path d="M819.2 364.8h-44.8V128c0-17.067-14.933-32-32-32H281.6c-17.067 0-32 14.933-32 32v236.8h-44.8C145.067 364.8 96 413.867 96 473.6v192c0 59.733 49.067 108.8 108.8 108.8h44.8V896c0 17.067 14.933 32 32 32h460.8c17.067 0 32-14.933 32-32V774.4h44.8c59.733 0 108.8-49.067 108.8-108.8v-192c0-59.733-49.067-108.8-108.8-108.8zM313.6 160h396.8v204.8H313.6V160zm396.8 704H313.6V620.8h396.8V864zM864 665.6c0 25.6-19.2 44.8-44.8 44.8h-44.8V588.8c0-17.067-14.933-32-32-32H281.6c-17.067 0-32 14.933-32 32v121.6h-44.8c-25.6 0-44.8-19.2-44.8-44.8v-192c0-25.6 19.2-44.8 44.8-44.8h614.4c25.6 0 44.8 19.2 44.8 44.8v192z"></path></svg></button><div class="arrow end"></div></div><div class="vp-toc-wrapper"><ul class="vp-toc-list"><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#_1-conceptions">1. Conceptions</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#_2-metrics">2. Metrics</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#_2-datasets">2. Datasets</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#_3-models-network-structures">3. Models &amp; Network Structures</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#_4-distributed-training-frameworks">4. Distributed Training Frameworks</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#_5-challenges">5. Challenges</a></li><!----><!--]--></ul><div class="vp-toc-marker" style="top:-1.7rem;"></div></div><!--[--><!----><!--]--></aside></div><!--[--><!----><!--]--><div class="theme-hope-content"><h1 id="llm" tabindex="-1"><a class="header-anchor" href="#llm"><span>LLM</span></a></h1><h2 id="_1-conceptions" tabindex="-1"><a class="header-anchor" href="#_1-conceptions"><span>1. Conceptions</span></a></h2><ul><li><table><thead><tr><th>概念</th><th>内容</th><th>备注</th></tr></thead><tbody><tr><td>token/word segmentation</td><td>将原始文本切分成子单元的过程就叫做 Tokenization。即，按照特定需求把文本中的句子、段落切分成一个字符串序列（其中的元素通常称为token 或叫词语），方便后续的处理分析工作。它是一个离散的文本单元，它可以是单词、标点符号、数字或其他语言元素，这些元素被用作训练和生成文本的基本单位。Token有粒度之分，如：词、字符或subword</td><td>解读资料：https://zhuanlan.zhihu.com/p/444774532</td></tr><tr><td>困惑度 Perplexity (PPL)</td><td>困惑度是一种用来评估语言模型的指标。它衡量了一个语言模型在给定数据集上的预测能力和概率分布的复杂性。较低的困惑度表示模型对数据集的拟合效果更好，也就是说，模型更能准确地预测下一个词或下一个句子。</td><td></td></tr><tr><td>SFT</td><td>监督微调（Supervised Fine-Tuning）</td><td></td></tr><tr><td>RLHF</td><td>基于人类反馈的强化学习 (Reinforcement Learning from Human Feedback)</td><td></td></tr><tr><td>embedding</td><td>（词）嵌入，可用于降维或升维，将特征拉近拉远到一个合适的观察点，将不同特征联系起来</td><td>https://zhuanlan.zhihu.com/p/616419336</td></tr><tr><td>seq2seq</td><td>序列到序列，该技术突破了传统的固定大小输入问题框架，提出了一种全新的端到端的映射方法。技术的核心是 Encoder-Decoder 架构，Encoder 负责将输入序列压缩成指定长度的向量，这个向量就可以看成是这个序列的语义，称为编码。而 Decoder 则负责根据语义向量生成指定的序列，这个过程也称为解码。Seq2Seq 是输出的长度不确定时采用的模型，因此在机器翻译、对话系统、自动文摘等自然语言处理任务中被广泛运用。</td><td>https://zhuanlan.zhihu.com/p/558138527<br>https://zhuanlan.zhihu.com/p/520657912?utm_id=0</td></tr><tr><td>attention</td><td>注意力机制（Attention Mechanism）源于对人类视觉的研究，主要理论原理是：信息处理时选择性地关注所有信息的一部分，同时忽略其他可见的信息。在计算能力有限情况下，注意力机制是解决信息超载问题的主要手段的一种资源分配方案，将计算资源分配给更重要的任务。</td><td>attention机制：https://zhuanlan.zhihu.com/p/46990010<br>atention图解：https://zhuanlan.zhihu.com/p/342235515<br>https://zhuanlan.zhihu.com/p/42724582<br>https://zhuanlan.zhihu.com/p/53682800<br>如何理解attention中的QKV：https://www.zhihu.com/question/298810062<br>transformer中的attention为什么scaled：https://www.zhihu.com/question/339723385<br>attention计算公式中的softmax：https://zhuanlan.zhihu.com/p/157490738<br></td></tr><tr><td>transformer</td><td>Transformer最早起源于论文Attention is all your need，是谷歌云TPU推荐的参考模型。 目前，在NLP领域当中，主要存在三种特征处理器：CNN、RNN以及Transformer，当前Transformer的流行程度已经大过CNN和RNN，它抛弃了传统CNN和RNN神经网络，整个网络结构完全由Attention机制以及前馈神经网络组成。论文中给出Transformer的定义是：Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence aligned RNNs or convolution。</td><td>论文 ：https://arxiv.org/abs/1706.03762<br>源码参考：https://github.com/huggingface/transformers<br>解读资料：https://blog.csdn.net/m0_67505927/article/details/123209347<br>https://blog.csdn.net/qq_52302919/article/details/122207924<br>B站李宏毅的视频，有几节专门讲attention和transformer的，比较清晰：https://www.bilibili.com/video/BV1Wv411h7kN?p=23<br></td></tr><tr><td>Beam search</td><td>集束搜索。一种搜索算法，是对greedy search的一个改进，相对greedy search扩大了搜索空间，但远不及穷举搜索指数级的搜索空间，是二者的一个折中方案。在文本生成任务中常用的解码策略。</td><td>https://zhuanlan.zhihu.com/p/82829880</td></tr><tr><td>AIGC (Artificial Intelligence Generated Content / AI-Generated Content)</td><td>人工智能生成内容，一种利用人工智能技术自动生成文章、音频、视频等多媒体内容的方法。</td><td></td></tr><tr><td>LLM ( Large Language Model)</td><td>大语言模型，使用大量文本数据训练的深度学习模型，可以生成自然语言文本或理解语言文本的含义。特点是规模庞大（包含数十亿参数），能学习到语言数据中的复杂模式。</td><td></td></tr><tr><td>GPT (Generative Pre-trained Transformer)</td><td>生成式预训练Transformer模型, 由OpenAI提出的一系列强大的预训练语言模型，兼具“大规模”和“预训练”两种属性。</td><td></td></tr><tr><td>BERT (Bidirectional Encoder Representations from Transformers)</td><td>基于Transformer的双向编码表征，谷歌提出的一个LLM，对NLP研究产生了重大影响。该模型使用双向方法从一个词的左右两边捕捉上下文，使得各种任务的性能提高，如情感分析和命名实体识别。</td><td></td></tr><tr><td>Bloom ( BigScience Large Open-science Open-access Multi-lingual Language Model)</td><td>BLOOM是一种基于trasnformer架构的解码器（Decoder-Only）自回归大语言模型，由BigScience社区开发和发布。该模型除了176B 之外，还提供了几个较小的模型，其模型参数规模为：560M，1.1B，1.7B，3B 和7.1B。</td><td>论文：https://arxiv.org/abs/2211.05100<br>解读：https://zhuanlan.zhihu.com/p/640016830</td></tr><tr><td>Flash Attention</td><td>FlashAttention 是一种重新排序注意力计算的算法，它无需任何近似即可加速注意力计算并减少内存占用，主要解决Transformer计算速度慢和存储占用高的问题，所以作为目前LLM的模型加速它是一个非常好的解决方案。</td><td>论文：<br>https://arxiv.org/abs/2205.14135<br>解读资料：<br>https://zhuanlan.zhihu.com/p/639228219<br>https://zhuanlan.zhihu.com/p/647364218<br>https://baijiahao.baidu.com/s?id=1774803715921029316&amp;wfr=spider&amp;for=pc<br></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr></tbody></table></li></ul><h2 id="_2-metrics" tabindex="-1"><a class="header-anchor" href="#_2-metrics"><span>2. Metrics</span></a></h2><ul><li><p>参考：https://aicarrier.feishu.cn/wiki/C2NYwAJEqidHUbkfNPgcO6EsnOd</p></li><li><p>功能指标</p><table><thead><tr><th>概念</th><th>内容</th><th>备注</th></tr></thead><tbody><tr><td>Loss</td><td>模型生成文本与参考文本间的差异度量</td><td></td></tr><tr><td>Loss与标准值方差</td><td>$\frac{(Loss-L_{mean})<sup>2+(L_{std}-L_{mean})</sup>2}{2}$ <br>$L_{std}$: 模型在基准环境中得到的Loss基准值 <br> $L_{mean}$: $Loss$ 与 $L_{std}$ 的平均值</td><td>用于评估不同硬件性能</td></tr><tr><td>PPL</td><td>标记化序列 $X = (x_0,x_1,...,x_t)$ <br> $PPL(X) = exp(-\frac{1}{t}\sum_i^t\log(p_\theta(x_i</td><td>x_{&lt;i}))$</td></tr></tbody></table></li><li><p>性能指标</p><table><thead><tr><th>概念</th><th>内容</th><th>备注</th></tr></thead><tbody><tr><td>TGS (tokens/gpu/second)</td><td>每秒单块GPU能够通过的token数量 <br> $TGS = \frac{tokens \ast GlobalBatchSize}{t_{epoch} \ast n_{gpu}}$</td><td>大模型训练核心指标</td></tr><tr><td>TFlops</td><td>TeraFlops, 每秒浮点运算次数（亿次）</td><td></td></tr><tr><td>Throughput (Gbps)</td><td>吞吐率</td><td></td></tr><tr><td>BandWidth</td><td>带宽</td><td></td></tr><tr><td>save-model time (s)</td><td>模型保存时间</td><td></td></tr><tr><td>模型初始化时间</td><td>模型初始化加载时间包括权重文件读取时间、转换时间、配置读取时间、显存分配时间、参数设置时间、首次运行时构图及编译时间等推理前置步骤的耗时。</td><td>推理性能指标</td></tr><tr><td>模型初次加载时间</td><td>$模型初次加载时间=Checkpoint加载时间+参数加载时间+Tokenizer加载时间+模型初始化时间$</td><td>推理性能指标</td></tr><tr><td>响应时间</td><td>指用户从发送请求的时刻到用户收到响应结果的时刻所经过的时间；<br>对于大语言模型来说，因响应结果是流式输出，长度不确定，所以我们把这个指标转化为首字延迟（first token latency ）和 推理耗时，分别度量第一个token生成时所消耗的时间，以及平均每个token所需要的时间。</td><td>推理性能指标</td></tr><tr><td>推理耗时</td><td>对于文生图大模型，生成输出图像的耗时记为模型推理耗时，根据使用的batchsize大小可以获得吞吐量QPS（fps=frames/s）<br>$QPS=\frac{batchsize*1000}{推理耗时}$</td><td>推理性能指标</td></tr><tr><td>Token Throughput</td><td>指每秒生成的token数量，它与响应时间、并发数密切相关，用来衡量服务的承载能力。</td><td>推理性能指标</td></tr></tbody></table></li><li><p>稳定性指标</p><table><thead><tr><th>概念</th><th>内容</th><th>备注</th></tr></thead><tbody><tr><td>前100setp loss波动</td><td>反映模型训练精度稳定性</td><td></td></tr><tr><td>最大模型训练稳定性</td><td>指定模型和规模下，模型训练3天内能否保证不崩溃（训练终止，显存溢出，loss不收敛）</td><td></td></tr><tr><td>断点续训</td><td>根据保存模型参数和优化器状态，从中断位置继续训练</td><td></td></tr></tbody></table></li><li><p>huggingface</p></li><li><p>peft:</p><ul><li>State-of-the-art Parameter-Efficient Fine-Tuning (PEFT) methods, only fine-tune a small number of (extra) model parameters, thereby greatly decreasing the computational and storage costs</li><li>Recent State-of-the-Art PEFT techniques achieve performance comparable to that of full fine-tuning.</li><li>https://github.com/huggingface/peft</li></ul></li><li><p>LoRA:</p><ul><li>Low-Rank Adaptation of Large Language Models</li><li>https://arxiv.org/abs/2106.09685</li></ul></li><li><p>PLM:</p><ul><li>pre-trained language models</li></ul></li><li></li></ul><h2 id="_2-datasets" tabindex="-1"><a class="header-anchor" href="#_2-datasets"><span>2. Datasets</span></a></h2><ul><li><p>TODO: 使用表格统计所有数据集，列出各自的特点，和链接</p></li><li><table><thead><tr><th>Datasets</th><th>Type</th><th>Size</th><th>Usage</th><th>Other</th></tr></thead><tbody><tr><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr></tbody></table></li><li><p>alpaca-data-conversation</p><ul><li></li><li><div class="language-json line-numbers-mode" data-ext="json" data-title="json"><pre class="language-json"><code><span class="token punctuation">[</span> ...
  <span class="token punctuation">{</span>
    <span class="token property">&quot;id&quot;</span><span class="token operator">:</span> <span class="token string">&quot;2&quot;</span><span class="token punctuation">,</span>
    <span class="token property">&quot;conversations&quot;</span><span class="token operator">:</span> <span class="token punctuation">[</span>
      <span class="token punctuation">{</span>
        <span class="token property">&quot;from&quot;</span><span class="token operator">:</span> <span class="token string">&quot;human&quot;</span><span class="token punctuation">,</span>
        <span class="token property">&quot;value&quot;</span><span class="token operator">:</span> <span class="token string">&quot;Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\nWhat are the three primary colors?\n\n### Response:&quot;</span>
      <span class="token punctuation">}</span><span class="token punctuation">,</span>
      <span class="token punctuation">{</span>
        <span class="token property">&quot;from&quot;</span><span class="token operator">:</span> <span class="token string">&quot;gpt&quot;</span><span class="token punctuation">,</span>
        <span class="token property">&quot;value&quot;</span><span class="token operator">:</span> <span class="token string">&quot;The three primary colors are red, blue, and yellow.&quot;</span>
      <span class="token punctuation">}</span>
    <span class="token punctuation">]</span>
  <span class="token punctuation">}</span><span class="token punctuation">,</span>
  ...
<span class="token punctuation">]</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></li></ul></li><li><p>pile</p><ul><li><p>The Pile is a <strong>825 GiB</strong> diverse, open source language modelling data set that consists of 22 smaller, high-quality datasets combined together.</p></li><li></li><li><div class="language-text line-numbers-mode" data-ext="text" data-title="text"><pre class="language-text"><code>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div></li></ul></li></ul><h2 id="_3-models-network-structures" tabindex="-1"><a class="header-anchor" href="#_3-models-network-structures"><span>3. Models &amp; Network Structures</span></a></h2><ul><li></li></ul><h2 id="_4-distributed-training-frameworks" tabindex="-1"><a class="header-anchor" href="#_4-distributed-training-frameworks"><span>4. Distributed Training Frameworks</span></a></h2><ul><li>megatron</li><li>deepspeed</li><li>colossal AI</li><li></li></ul><h2 id="_5-challenges" tabindex="-1"><a class="header-anchor" href="#_5-challenges"><span>5. Challenges</span></a></h2><ul><li><p>知识过期，幻觉问题（知识编辑，调用外部知识库）</p></li><li><p>记忆能力：</p><ul><li>扩展 Backbone 架构的长度限制：针对 Transformers 固有的序列长度限制问题进行改进。</li><li>总结记忆（Summarizing）：对记忆进行摘要总结，增强代理从记忆中提取关键细节的能力。</li><li>压缩记忆（Compressing）：通过使用向量或适当的数据结构对记忆进行压缩，可以提高记忆检索效率。</li></ul></li><li><p>推理能力（Reasoning）：</p><ul><li>对于智能代理进行决策、分析等复杂任务而言至关重要。具体到 LLMs 上，就是以 思维链（Chain-of-Thought，CoT） 为代表的一系列提示方法。</li></ul></li><li><p>规划（Planning）：</p><ul><li>面对大型挑战时常用的策略。它帮助代理组织思维、设定目标并确定实现这些目标的步骤。在具体实现中，规划可以包含两个步骤： <ul><li>计划制定（Plan Formulation）：代理将复杂任务分解为更易于管理的子任务。例如：一次性分解再按顺序执行、逐步规划并执行、多路规划并选取最优路径等。在一些需要专业知识的场景中，代理可与特定领域的 Planner 模块集成，提升能力。</li><li>计划反思（Plan Reflection）：在制定计划后，可以进行反思并评估其优劣。这种反思一般来自三个方面：借助内部反馈机制；与人类互动获得反馈；从环境中获得反馈。</li></ul></li></ul></li><li><p>迁移性 &amp; 泛化性：</p><ul><li>拥有世界知识的 LLMs 赋予智能代理具备强大的迁移与泛化能力。一个好的代理不是静态的知识库，还应具备动态的学习能力： <ul><li>对未知任务的泛化：随着模型规模与训练数据的增大，LLMs 在解决未知任务上涌现出了惊人的能力。通过指令微调的大模型在 zero-shot 测试中表现良好，在许多任务上都取得了不亚于专家模型的成绩。</li><li>情景学习（In-context Learning）：大模型不仅能够从上下文的少量示例中进行类比学习，这种能力还可以扩展到文本以外的多模态场景，为代理在现实世界中的应用提供了更多可能性。</li><li>持续学习（Continual Learning）：持续学习的主要挑战是灾难性遗忘，即当模型学习新任务时容易丢失过往任务中的知识。专有领域的智能代理应当尽量避免丢失通用领域的知识。</li></ul></li></ul></li><li><p>视觉输入：</p><ul><li><p>LLMs 本身并不具备视觉的感知能力，只能理解离散的文本内容。而视觉输入通常包含有关世界的大量信息，包括对象的属性，空间关系，场景布局等等。常见的方法有：</p><ul><li><p>将视觉输入转为对应的文本描述（Image Captioning）：可以被 LLMs 直接理解，并且可解释性高。</p></li><li><p>对视觉信息进行编码表示：以视觉基础模型 + LLMs 的范式来构成感知模块，通过对齐操作来让模型理解不同模态的内容，可以端到端的方式进行训练。</p></li></ul></li></ul></li><li><p>听觉输入：</p><ul><li>听觉也是人类感知中的重要组成部分。由于 LLMs 有着优秀的工具调用能力，一个直观的想法就是：代理可以将 LLMs 作为控制枢纽，通过级联的方式调用现有的工具集或者专家模型，感知音频信息。此外，音频也可以通过频谱图（Spectrogram）的方式进行直观表示。频谱图可以作为平面图像来展示 2D 信息，因此，一些视觉的处理方法可以迁移到语音领域。</li></ul></li><li><p>工具使用：</p><ul><li>尽管 LLMs 拥有出色的知识储备和专业能力，但在面对具体问题时，也可能会出现鲁棒性问题、幻觉等一系列挑战。与此同时，工具作为使用者能力的扩展，可以在专业性、事实性、可解释性等方面提供帮助。例如，可以通过使用计算器来计算数学问题、使用搜索引擎来搜寻实时信息。</li><li>另外，工具也可以扩展智能代理的行动空间。例如，通过调用语音生成、图像生成等专家模型，来获得多模态的行动方式。因此，如何让代理成为优秀的工具使用者，即学会如何有效地利用工具，是非常重要且有前景的方向。</li><li>目前，主要的工具学习方法包括从演示中学习和从反馈中学习。此外，也可以通过元学习、课程学习等方式来让代理程序在使用各种工具方面具备泛化能力。更进一步，智能代理还可以进一步学习如何「自给自足」地制造工具，从而提高其自主性和独立性。</li></ul></li><li><p>具身行动：</p><ul><li>具身（Embodyment）是指代理与环境交互过程中，理解、改造环境并更新自身状态的能力。具身行动（Embodied Action）被视为虚拟智能与物理现实的互通桥梁。</li><li>传统的基于强化学习的 Agent 在样本效率、泛化性和复杂问题推理等方面存在局限性，而 LLM-based Agents 通过引入大模型丰富的内在知识，使得 Embodied Agent 能够像人类一样主动感知、影响物理环境。根据代理在任务中的自主程度或者说 Action 的复杂程度，可以有以下的原子 Action： <ul><li>Observation 可以帮助智能代理在环境中定位自身位置、感知对象物品和获取其他环境信息；</li><li>Manipulation 则是完成一些具体的抓取、推动等操作任务；</li><li>Navigation 要求智能代理根据任务目标变换自身位置并根据环境信息更新自身状态。</li></ul></li></ul></li><li></li></ul></div><!--[--><!----><!--]--><footer class="page-meta"><!----><div class="meta-item git-info"><!----><!----></div></footer><nav class="vp-page-nav"><a class="route-link nav-link prev" href="/notes/linux_command.html" aria-label="Linux 快捷指令"><div class="hint"><span class="arrow start"></span>上一页</div><div class="link"><!---->Linux 快捷指令</div></a><a class="route-link nav-link next" href="/notes/loss.html" aria-label="Loss 相关问题"><div class="hint">下一页<span class="arrow end"></span></div><div class="link">Loss 相关问题<!----></div></a></nav><div id="vp-comment" class="giscus-wrapper input-top" style="display:block;"><svg xmlns="http://www.w3.org/2000/svg" width="32" height="32" preserveAspectRatio="xMidYMid" viewBox="0 0 100 100"><circle cx="28" cy="75" r="11" fill="currentColor"><animate attributeName="fill-opacity" begin="0s" dur="1s" keyTimes="0;0.2;1" repeatCount="indefinite" values="0;1;1"></animate></circle><path fill="none" stroke="#88baf0" stroke-width="10" d="M28 47a28 28 0 0 1 28 28"><animate attributeName="stroke-opacity" begin="0.1s" dur="1s" keyTimes="0;0.2;1" repeatCount="indefinite" values="0;1;1"></animate></path><path fill="none" stroke="#88baf0" stroke-width="10" d="M28 25a50 50 0 0 1 50 50"><animate attributeName="stroke-opacity" begin="0.2s" dur="1s" keyTimes="0;0.2;1" repeatCount="indefinite" values="0;1;1"></animate></path></svg></div><!--[--><!----><!--]--><!--]--></main><!--]--><footer class="vp-footer-wrapper"><div class="vp-footer">BradZhone's Blog</div><div class="vp-copyright">Copyright © 2024 BradZhone </div></footer></div><!--]--><!--[--><!----><!--]--><!--]--></div>
    <script type="module" src="/assets/app-Coh1oo3x.js" defer></script>
  </body>
</html>
