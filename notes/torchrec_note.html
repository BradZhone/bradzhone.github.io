<!doctype html>
<html lang="zh-CN" data-theme="light">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width,initial-scale=1" />
    <meta name="generator" content="VuePress 2.0.0-rc.9" />
    <meta name="theme" content="VuePress Theme Hope 2.0.0-rc.32" />
    <style>
      html {
        background: var(--bg-color, #fff);
      }

      html[data-theme="dark"] {
        background: var(--bg-color, #1d1e1f);
      }

      body {
        background: var(--bg-color);
      }
    </style>
    <script>
      const userMode = localStorage.getItem("vuepress-theme-hope-scheme");
      const systemDarkMode =
        window.matchMedia &&
        window.matchMedia("(prefers-color-scheme: dark)").matches;

      if (userMode === "dark" || (userMode !== "light" && systemDarkMode)) {
        document.documentElement.setAttribute("data-theme", "dark");
      }
    </script>
    <meta property="og:url" content="https://bradzhone.github.io/notes/torchrec_note.html"><meta property="og:site_name" content="BradZhone's Blog"><meta property="og:title" content="Torchrecè°ƒç ”"><meta property="og:description" content="Torchrecè°ƒç ” 0.0 æœªè¿ç§»ä¾èµ–åº“æ±‡æ€» torch torch.distributed._shard torch.fx._compatibility torch.distributed._composable torch.distributed.optim._apply_optimizer_in_backward torch.distribut..."><meta property="og:type" content="article"><meta property="og:locale" content="zh-CN"><meta property="article:author" content="BradZhone"><meta property="article:tag" content="Torchrec"><meta property="article:published_time" content="2023-06-21T00:00:00.000Z"><script type="application/ld+json">{"@context":"https://schema.org","@type":"Article","headline":"Torchrecè°ƒç ”","image":[""],"datePublished":"2023-06-21T00:00:00.000Z","dateModified":null,"author":[{"@type":"Person","name":"BradZhone","url":"https://github.com/BradZhone"}]}</script><title>Torchrecè°ƒç ” | BradZhone's Blog</title><meta name="description" content="Torchrecè°ƒç ” 0.0 æœªè¿ç§»ä¾èµ–åº“æ±‡æ€» torch torch.distributed._shard torch.fx._compatibility torch.distributed._composable torch.distributed.optim._apply_optimizer_in_backward torch.distribut...">
    <link rel="preload" href="/assets/style-BkjCpNEe.css" as="style"><link rel="stylesheet" href="/assets/style-BkjCpNEe.css">
    <link rel="modulepreload" href="/assets/app-Coh1oo3x.js"><link rel="modulepreload" href="/assets/torchrec_note.html-B-i_O6mg.js"><link rel="modulepreload" href="/assets/plugin-vue_export-helper-DlAUqK2U.js">
    <link rel="prefetch" href="/assets/intro.html-D9dwEpHI.js" as="script"><link rel="prefetch" href="/assets/index.html-D5LnkeFh.js" as="script"><link rel="prefetch" href="/assets/CRNN_blog.html-Cb6b7OD4.js" as="script"><link rel="prefetch" href="/assets/cucollection_blog.html-DLix_uWp.js" as="script"><link rel="prefetch" href="/assets/cuda_blog.html-CBZ1Rex-.js" as="script"><link rel="prefetch" href="/assets/distributed_embeddings_blog.html-Cw7oBr-d.js" as="script"><link rel="prefetch" href="/assets/howToBuildThisBlog.html-Dxn0xRj3.js" as="script"><link rel="prefetch" href="/assets/hugectr_blog.html-BrFINEES.js" as="script"><link rel="prefetch" href="/assets/hugectr_src_blog.html-d9qgPAFk.js" as="script"><link rel="prefetch" href="/assets/index.html-DI16Z1C4.js" as="script"><link rel="prefetch" href="/assets/torchrec_cn_embedding_note.html-YEVLbIAq.js" as="script"><link rel="prefetch" href="/assets/warpcore_blog.html-Cz_5IJvA.js" as="script"><link rel="prefetch" href="/assets/c___note.html-BYbhq9Nq.js" as="script"><link rel="prefetch" href="/assets/deep_learning.html-b-b3zdJE.js" as="script"><link rel="prefetch" href="/assets/linux_command.html-G4dNyKFL.js" as="script"><link rel="prefetch" href="/assets/LLM.html-9hu1pKVY.js" as="script"><link rel="prefetch" href="/assets/loss.html-CJxfnubw.js" as="script"><link rel="prefetch" href="/assets/markdown.html-DJcQDvEC.js" as="script"><link rel="prefetch" href="/assets/metrics.html-DKJGeFbq.js" as="script"><link rel="prefetch" href="/assets/PLAN_Z.html-B9yJBwFA.js" as="script"><link rel="prefetch" href="/assets/precision.html-B_dd_MdY.js" as="script"><link rel="prefetch" href="/assets/index.html-D1wTsZoY.js" as="script"><link rel="prefetch" href="/assets/uml_note.html-IaIR5y-b.js" as="script"><link rel="prefetch" href="/assets/index.html-Cgwhpoct.js" as="script"><link rel="prefetch" href="/assets/404.html-DYVdKbdY.js" as="script"><link rel="prefetch" href="/assets/index.html-1jQ9-Uww.js" as="script"><link rel="prefetch" href="/assets/index.html-BEDqx9YQ.js" as="script"><link rel="prefetch" href="/assets/index.html-BltlhrdK.js" as="script"><link rel="prefetch" href="/assets/index.html-C0yITs6x.js" as="script"><link rel="prefetch" href="/assets/index.html-maJbTduo.js" as="script"><link rel="prefetch" href="/assets/index.html-BPQ9UPx_.js" as="script"><link rel="prefetch" href="/assets/index.html-BQB6pLKG.js" as="script"><link rel="prefetch" href="/assets/index.html-Be8HnZrW.js" as="script"><link rel="prefetch" href="/assets/index.html-DzX6xTUq.js" as="script"><link rel="prefetch" href="/assets/index.html-BxZ6QV-X.js" as="script"><link rel="prefetch" href="/assets/index.html-Dl0UMf-I.js" as="script"><link rel="prefetch" href="/assets/index.html-xBRd_OsE.js" as="script"><link rel="prefetch" href="/assets/index.html-BTkUZ9Ws.js" as="script"><link rel="prefetch" href="/assets/index.html-D57wSldI.js" as="script"><link rel="prefetch" href="/assets/index.html-Dqr1foaP.js" as="script"><link rel="prefetch" href="/assets/index.html-CSGzSmAq.js" as="script"><link rel="prefetch" href="/assets/index.html-BnBAmG4V.js" as="script"><link rel="prefetch" href="/assets/index.html-d2G8e39M.js" as="script"><link rel="prefetch" href="/assets/index.html-CpCodvQl.js" as="script"><link rel="prefetch" href="/assets/index.html-hEB5WJuD.js" as="script"><link rel="prefetch" href="/assets/index.html-DX2Ijbyk.js" as="script"><link rel="prefetch" href="/assets/index.html-Da6Vp1bl.js" as="script"><link rel="prefetch" href="/assets/index.html-CGr0tdaC.js" as="script"><link rel="prefetch" href="/assets/index.html-D5fEOhj5.js" as="script"><link rel="prefetch" href="/assets/index.html-C1vK6J6C.js" as="script"><link rel="prefetch" href="/assets/index.html-DgD_mq8U.js" as="script"><link rel="prefetch" href="/assets/index.html-Demc7CPi.js" as="script"><link rel="prefetch" href="/assets/index.html-D31kcDwM.js" as="script"><link rel="prefetch" href="/assets/index.html-COTqqkDG.js" as="script"><link rel="prefetch" href="/assets/index.html-DtZDr4LJ.js" as="script"><link rel="prefetch" href="/assets/index.html-BYDcpP-H.js" as="script"><link rel="prefetch" href="/assets/index.html-QpKn8zOy.js" as="script"><link rel="prefetch" href="/assets/index.html-Fd1nQRc_.js" as="script"><link rel="prefetch" href="/assets/index.html-BEVLRn0k.js" as="script"><link rel="prefetch" href="/assets/giscus-7BMGhbDA.js" as="script"><link rel="prefetch" href="/assets/photoswipe.esm-SzV8tJDW.js" as="script">
  </head>
  <body>
    <div id="app"><!--[--><!--[--><!--[--><span tabindex="-1"></span><a href="#main-content" class="vp-skip-link sr-only">è·³è‡³ä¸»è¦å…§å®¹</a><!--]--><div class="theme-container has-toc"><!--[--><header id="navbar" class="vp-navbar"><div class="vp-navbar-start"><button type="button" class="vp-toggle-sidebar-button" title="Toggle Sidebar"><span class="icon"></span></button><!--[--><!----><!--]--><!--[--><a class="route-link vp-brand" href="/"><img class="vp-nav-logo light" src="/light.png" alt><img class="vp-nav-logo dark" src="/dark.png" alt><span class="vp-site-name hide-in-pad">BradZhone&#39;s Blog</span></a><!--]--><!--[--><!----><!--]--></div><div class="vp-navbar-center"><!--[--><!----><!--]--><!--[--><nav class="vp-nav-links"><div class="nav-item hide-in-mobile"><a class="route-link nav-link" href="/" aria-label="Home"><span class="font-icon icon fa-fw fa-sm fas fa-home" style=""></span>Home<!----></a></div><div class="nav-item hide-in-mobile"><a class="route-link nav-link" href="/blogs/" aria-label="Blogs"><span class="font-icon icon fa-fw fa-sm fas fa-blog" style=""></span>Blogs<!----></a></div><div class="nav-item hide-in-mobile"><a class="route-link nav-link active" href="/notes/" aria-label="Notes"><span class="font-icon icon fa-fw fa-sm fas fa-note-sticky" style=""></span>Notes<!----></a></div><div class="nav-item hide-in-mobile"><a class="route-link nav-link" href="/thinking/" aria-label="Thinking"><span class="font-icon icon fa-fw fa-sm fas fa-lightbulb" style=""></span>Thinking<!----></a></div><div class="nav-item hide-in-mobile"><a class="route-link nav-link" href="/intro.html" aria-label="About Me"><span class="font-icon icon fa-fw fa-sm fas fa-star" style=""></span>About Me<!----></a></div></nav><!--]--><!--[--><!----><!--]--></div><div class="vp-navbar-end"><!--[--><!----><!--]--><!--[--><!----><!----><div class="nav-item hide-in-mobile"><button type="button" id="appearance-switch"><svg xmlns="http://www.w3.org/2000/svg" class="icon auto-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="auto icon" style="display:none;"><path d="M512 992C246.92 992 32 777.08 32 512S246.92 32 512 32s480 214.92 480 480-214.92 480-480 480zm0-840c-198.78 0-360 161.22-360 360 0 198.84 161.22 360 360 360s360-161.16 360-360c0-198.78-161.22-360-360-360zm0 660V212c165.72 0 300 134.34 300 300 0 165.72-134.28 300-300 300z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" class="icon dark-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="dark icon" style="display:none;"><path d="M524.8 938.667h-4.267a439.893 439.893 0 0 1-313.173-134.4 446.293 446.293 0 0 1-11.093-597.334A432.213 432.213 0 0 1 366.933 90.027a42.667 42.667 0 0 1 45.227 9.386 42.667 42.667 0 0 1 10.24 42.667 358.4 358.4 0 0 0 82.773 375.893 361.387 361.387 0 0 0 376.747 82.774 42.667 42.667 0 0 1 54.187 55.04 433.493 433.493 0 0 1-99.84 154.88 438.613 438.613 0 0 1-311.467 128z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" class="icon light-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="light icon" style="display:block;"><path d="M952 552h-80a40 40 0 0 1 0-80h80a40 40 0 0 1 0 80zM801.88 280.08a41 41 0 0 1-57.96-57.96l57.96-58a41.04 41.04 0 0 1 58 58l-58 57.96zM512 752a240 240 0 1 1 0-480 240 240 0 0 1 0 480zm0-560a40 40 0 0 1-40-40V72a40 40 0 0 1 80 0v80a40 40 0 0 1-40 40zm-289.88 88.08-58-57.96a41.04 41.04 0 0 1 58-58l57.96 58a41 41 0 0 1-57.96 57.96zM192 512a40 40 0 0 1-40 40H72a40 40 0 0 1 0-80h80a40 40 0 0 1 40 40zm30.12 231.92a41 41 0 0 1 57.96 57.96l-57.96 58a41.04 41.04 0 0 1-58-58l58-57.96zM512 832a40 40 0 0 1 40 40v80a40 40 0 0 1-80 0v-80a40 40 0 0 1 40-40zm289.88-88.08 58 57.96a41.04 41.04 0 0 1-58 58l-57.96-58a41 41 0 0 1 57.96-57.96z"></path></svg></button></div><!----><!--]--><!--[--><!----><!--]--><button type="button" class="vp-toggle-navbar-button" aria-label="Toggle Navbar" aria-expanded="false" aria-controls="nav-screen"><span><span class="vp-top"></span><span class="vp-middle"></span><span class="vp-bottom"></span></span></button></div></header><!----><!--]--><!----><div class="toggle-sidebar-wrapper"><span class="arrow start"></span></div><aside id="sidebar" class="vp-sidebar"><!--[--><!----><!--]--><ul class="vp-sidebar-links"><li><section class="vp-sidebar-group"><p class="vp-sidebar-header clickable"><span class="font-icon icon fa-fw fa-sm fas fa-blog" style=""></span><a class="route-link nav-link vp-sidebar-title" href="/blogs/" aria-label="Blogs"><!---->Blogs<!----></a><!----></p><ul class="vp-sidebar-links"><li><a class="route-link nav-link vp-sidebar-link vp-sidebar-page" href="/blogs/howToBuildThisBlog.html" aria-label="å¦‚ä½•æ­å»ºæœ¬åšå®¢"><!---->å¦‚ä½•æ­å»ºæœ¬åšå®¢<!----></a></li><li><a class="route-link nav-link vp-sidebar-link vp-sidebar-page" href="/blogs/CRNN_blog.html" aria-label="CRNNç½‘ç»œè°ƒç ”é€‚é…è®°å½•"><!---->CRNNç½‘ç»œè°ƒç ”é€‚é…è®°å½•<!----></a></li><li><a class="route-link nav-link vp-sidebar-link vp-sidebar-page" href="/blogs/cucollection_blog.html" aria-label="cuCollectionsæ€§èƒ½æµ‹è¯•"><!---->cuCollectionsæ€§èƒ½æµ‹è¯•<!----></a></li><li><a class="route-link nav-link vp-sidebar-link vp-sidebar-page" href="/blogs/cuda_blog.html" aria-label="CUDAå­¦ä¹ ç¬”è®°"><!---->CUDAå­¦ä¹ ç¬”è®°<!----></a></li><li><a class="route-link nav-link vp-sidebar-link vp-sidebar-page" href="/blogs/distributed_embeddings_blog.html" aria-label="Distributed_embeddings"><!---->Distributed_embeddings<!----></a></li><li><a class="route-link nav-link vp-sidebar-link vp-sidebar-page" href="/blogs/hugectr_blog.html" aria-label="HugeCTR å­¦ä¹ ç¬”è®°"><!---->HugeCTR å­¦ä¹ ç¬”è®°<!----></a></li><li><a class="route-link nav-link vp-sidebar-link vp-sidebar-page" href="/blogs/hugectr_src_blog.html" aria-label="HugeCTRæºç é˜…è¯»ç¬”è®°"><!---->HugeCTRæºç é˜…è¯»ç¬”è®°<!----></a></li><li><a class="route-link nav-link vp-sidebar-link vp-sidebar-page" href="/blogs/torchrec_cn_embedding_note.html" aria-label="torchrec cn_embeddingæ¨¡å—è®¾è®¡æ–¹æ¡ˆ"><!---->torchrec cn_embeddingæ¨¡å—è®¾è®¡æ–¹æ¡ˆ<!----></a></li><li><a class="route-link nav-link vp-sidebar-link vp-sidebar-page" href="/blogs/warpcore_blog.html" aria-label="WARPCORE å­¦ä¹ ç¬”è®°"><!---->WARPCORE å­¦ä¹ ç¬”è®°<!----></a></li></ul></section></li><li><section class="vp-sidebar-group"><p class="vp-sidebar-header clickable active"><span class="font-icon icon fa-fw fa-sm fas fa-note-sticky" style=""></span><a class="route-link nav-link active vp-sidebar-title" href="/notes/" aria-label="Notes"><!---->Notes<!----></a><!----></p><ul class="vp-sidebar-links"><li><a class="route-link nav-link vp-sidebar-link vp-sidebar-page" href="/notes/c___note.html" aria-label="C++ note"><!---->C++ note<!----></a></li><li><a class="route-link nav-link vp-sidebar-link vp-sidebar-page" href="/notes/deep_learning.html" aria-label="DLç›¸å…³"><!---->DLç›¸å…³<!----></a></li><li><a class="route-link nav-link vp-sidebar-link vp-sidebar-page" href="/notes/precision.html" aria-label="Floating Point precision formats"><!---->Floating Point precision formats<!----></a></li><li><a class="route-link nav-link vp-sidebar-link vp-sidebar-page" href="/notes/linux_command.html" aria-label="Linux å¿«æ·æŒ‡ä»¤"><!---->Linux å¿«æ·æŒ‡ä»¤<!----></a></li><li><a class="route-link nav-link vp-sidebar-link vp-sidebar-page" href="/notes/LLM.html" aria-label="LLM"><!---->LLM<!----></a></li><li><a class="route-link nav-link vp-sidebar-link vp-sidebar-page" href="/notes/loss.html" aria-label="Loss ç›¸å…³é—®é¢˜"><!---->Loss ç›¸å…³é—®é¢˜<!----></a></li><li><a class="route-link nav-link vp-sidebar-link vp-sidebar-page" href="/notes/markdown.html" aria-label="Markdown è¯­æ³•"><!---->Markdown è¯­æ³•<!----></a></li><li><a class="route-link nav-link vp-sidebar-link vp-sidebar-page" href="/notes/metrics.html" aria-label="Metrics"><!---->Metrics<!----></a></li><li><a class="route-link nav-link vp-sidebar-link vp-sidebar-page" href="/notes/PLAN_Z.html" aria-label="TODO LIST"><!---->TODO LIST<!----></a></li><li><a class="route-link nav-link active vp-sidebar-link vp-sidebar-page active" href="/notes/torchrec_note.html" aria-label="Torchrecè°ƒç ”"><!---->Torchrecè°ƒç ”<!----></a></li><li><a class="route-link nav-link vp-sidebar-link vp-sidebar-page" href="/notes/uml_note.html" aria-label="UMLå­¦ä¹ ç¬”è®°"><!---->UMLå­¦ä¹ ç¬”è®°<!----></a></li></ul></section></li><li><section class="vp-sidebar-group"><p class="vp-sidebar-header clickable"><span class="font-icon icon fa-fw fa-sm fas fa-lightbulb" style=""></span><a class="route-link nav-link vp-sidebar-title" href="/thinking/" aria-label="Thinking"><!---->Thinking<!----></a><!----></p><ul class="vp-sidebar-links"></ul></section></li></ul><!--[--><!----><!--]--></aside><!--[--><main id="main-content" class="vp-page"><!--[--><!--[--><!----><!--]--><!----><nav class="vp-breadcrumb disable"></nav><div class="vp-page-title"><h1><!---->Torchrecè°ƒç ”</h1><div class="page-info"><span class="page-author-info" aria-label="ä½œè€…ğŸ–Š" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon author-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="author icon"><path d="M649.6 633.6c86.4-48 147.2-144 147.2-249.6 0-160-128-288-288-288s-288 128-288 288c0 108.8 57.6 201.6 147.2 249.6-121.6 48-214.4 153.6-240 288-3.2 9.6 0 19.2 6.4 25.6 3.2 9.6 12.8 12.8 22.4 12.8h704c9.6 0 19.2-3.2 25.6-12.8 6.4-6.4 9.6-16 6.4-25.6-25.6-134.4-121.6-240-243.2-288z"></path></svg><span><a class="page-author-item" href="https://github.com/BradZhone" target="_blank" rel="noopener noreferrer">BradZhone</a></span><span property="author" content="BradZhone"></span></span><!----><span class="page-date-info" aria-label="å†™ä½œæ—¥æœŸğŸ“…" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon calendar-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="calendar icon"><path d="M716.4 110.137c0-18.753-14.72-33.473-33.472-33.473-18.753 0-33.473 14.72-33.473 33.473v33.473h66.993v-33.473zm-334.87 0c0-18.753-14.72-33.473-33.473-33.473s-33.52 14.72-33.52 33.473v33.473h66.993v-33.473zm468.81 33.52H716.4v100.465c0 18.753-14.72 33.473-33.472 33.473a33.145 33.145 0 01-33.473-33.473V143.657H381.53v100.465c0 18.753-14.72 33.473-33.473 33.473a33.145 33.145 0 01-33.473-33.473V143.657H180.6A134.314 134.314 0 0046.66 277.595v535.756A134.314 134.314 0 00180.6 947.289h669.74a134.36 134.36 0 00133.94-133.938V277.595a134.314 134.314 0 00-133.94-133.938zm33.473 267.877H147.126a33.145 33.145 0 01-33.473-33.473c0-18.752 14.72-33.473 33.473-33.473h736.687c18.752 0 33.472 14.72 33.472 33.473a33.145 33.145 0 01-33.472 33.473z"></path></svg><span><!----></span><meta property="datePublished" content="2023-06-21T00:00:00.000Z"></span><!----><span class="page-reading-time-info" aria-label="é˜…è¯»æ—¶é—´âŒ›" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon timer-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="timer icon"><path d="M799.387 122.15c4.402-2.978 7.38-7.897 7.38-13.463v-1.165c0-8.933-7.38-16.312-16.312-16.312H256.33c-8.933 0-16.311 7.38-16.311 16.312v1.165c0 5.825 2.977 10.874 7.637 13.592 4.143 194.44 97.22 354.963 220.201 392.763-122.204 37.542-214.893 196.511-220.2 389.397-4.661 5.049-7.638 11.651-7.638 19.03v5.825h566.49v-5.825c0-7.379-2.849-13.981-7.509-18.9-5.049-193.016-97.867-351.985-220.2-389.527 123.24-37.67 216.446-198.453 220.588-392.892zM531.16 450.445v352.632c117.674 1.553 211.787 40.778 211.787 88.676H304.097c0-48.286 95.149-87.382 213.728-88.676V450.445c-93.077-3.107-167.901-81.297-167.901-177.093 0-8.803 6.99-15.793 15.793-15.793 8.803 0 15.794 6.99 15.794 15.793 0 80.261 63.69 145.635 142.01 145.635s142.011-65.374 142.011-145.635c0-8.803 6.99-15.793 15.794-15.793s15.793 6.99 15.793 15.793c0 95.019-73.789 172.82-165.96 177.093z"></path></svg><span>å¤§çº¦ 26 åˆ†é’Ÿ</span><meta property="timeRequired" content="PT26M"></span><span class="page-category-info" aria-label="åˆ†ç±»ğŸŒˆ" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon category-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="category icon"><path d="M148.41 106.992h282.176c22.263 0 40.31 18.048 40.31 40.31V429.48c0 22.263-18.047 40.31-40.31 40.31H148.41c-22.263 0-40.311-18.047-40.311-40.31V147.302c0-22.263 18.048-40.31 40.311-40.31zM147.556 553.478H429.73c22.263 0 40.311 18.048 40.311 40.31v282.176c0 22.263-18.048 40.312-40.31 40.312H147.555c-22.263 0-40.311-18.049-40.311-40.312V593.79c0-22.263 18.048-40.311 40.31-40.311zM593.927 106.992h282.176c22.263 0 40.31 18.048 40.31 40.31V429.48c0 22.263-18.047 40.31-40.31 40.31H593.927c-22.263 0-40.311-18.047-40.311-40.31V147.302c0-22.263 18.048-40.31 40.31-40.31zM730.22 920.502H623.926c-40.925 0-74.22-33.388-74.22-74.425V623.992c0-41.038 33.387-74.424 74.425-74.424h222.085c41.038 0 74.424 33.226 74.424 74.067v114.233c0 10.244-8.304 18.548-18.547 18.548s-18.548-8.304-18.548-18.548V623.635c0-20.388-16.746-36.974-37.33-36.974H624.13c-20.585 0-37.331 16.747-37.331 37.33v222.086c0 20.585 16.654 37.331 37.126 37.331H730.22c10.243 0 18.547 8.304 18.547 18.547 0 10.244-8.304 18.547-18.547 18.547z"></path></svg><!--[--><span class="page-category-item category8 clickable" role="navigation">æ¨èç³»ç»Ÿ</span><!--]--><meta property="articleSection" content="æ¨èç³»ç»Ÿ"></span><span class="page-tag-info" aria-label="æ ‡ç­¾ğŸ·" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon tag-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="tag icon"><path d="M939.902 458.563L910.17 144.567c-1.507-16.272-14.465-29.13-30.737-30.737L565.438 84.098h-.402c-3.215 0-5.726 1.005-7.634 2.913l-470.39 470.39a10.004 10.004 0 000 14.164l365.423 365.424c1.909 1.908 4.42 2.913 7.132 2.913s5.223-1.005 7.132-2.913l470.39-470.39c2.01-2.11 3.014-5.023 2.813-8.036zm-240.067-72.121c-35.458 0-64.286-28.828-64.286-64.286s28.828-64.285 64.286-64.285 64.286 28.828 64.286 64.285-28.829 64.286-64.286 64.286z"></path></svg><!--[--><span class="page-tag-item tag1 clickable" role="navigation">Torchrec</span><!--]--><meta property="keywords" content="Torchrec"></span></div><hr></div><div class="vp-toc-placeholder"><aside id="toc"><!--[--><!----><!--]--><div class="vp-toc-header">æ­¤é¡µå†…å®¹<button type="button" class="print-button" title="æ‰“å°"><svg xmlns="http://www.w3.org/2000/svg" class="icon print-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="print icon"><path d="M819.2 364.8h-44.8V128c0-17.067-14.933-32-32-32H281.6c-17.067 0-32 14.933-32 32v236.8h-44.8C145.067 364.8 96 413.867 96 473.6v192c0 59.733 49.067 108.8 108.8 108.8h44.8V896c0 17.067 14.933 32 32 32h460.8c17.067 0 32-14.933 32-32V774.4h44.8c59.733 0 108.8-49.067 108.8-108.8v-192c0-59.733-49.067-108.8-108.8-108.8zM313.6 160h396.8v204.8H313.6V160zm396.8 704H313.6V620.8h396.8V864zM864 665.6c0 25.6-19.2 44.8-44.8 44.8h-44.8V588.8c0-17.067-14.933-32-32-32H281.6c-17.067 0-32 14.933-32 32v121.6h-44.8c-25.6 0-44.8-19.2-44.8-44.8v-192c0-25.6 19.2-44.8 44.8-44.8h614.4c25.6 0 44.8 19.2 44.8 44.8v192z"></path></svg></button><div class="arrow end"></div></div><div class="vp-toc-wrapper"><ul class="vp-toc-list"><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#_0-0-æœªè¿ç§»ä¾èµ–åº“æ±‡æ€»">0.0 æœªè¿ç§»ä¾èµ–åº“æ±‡æ€»</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#_0-1-qa">0.1 QA</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#_0-2-è¿ç§»è®¡åˆ’">0.2 è¿ç§»è®¡åˆ’</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#_1-ä»‹ç»">1. ä»‹ç»</a></li><li><ul class="vp-toc-list"><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#_1-1-å®‰è£…-æµ‹è¯•">1.1 å®‰è£…&amp;æµ‹è¯•</a></li><!----><!--]--></ul></li><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#_2-data-preprocess">2. Data Preprocess</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#_3-torchrec-benchmarks">3. Torchrec Benchmarks</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#_4-dlrm-benchmarks">4. DLRM Benchmarks</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#_6-dlrm-benchmarksæµ‹è¯•ç»“æœ">6. DLRM Benchmarksæµ‹è¯•ç»“æœ</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#_7-torchrecä¸hugectræ¯”è¾ƒ">7. Torchrecä¸HugeCTRæ¯”è¾ƒ</a></li><!----><!--]--></ul><div class="vp-toc-marker" style="top:-1.7rem;"></div></div><!--[--><!----><!--]--></aside></div><!--[--><!----><!--]--><div class="theme-hope-content"><h1 id="torchrecè°ƒç ”" tabindex="-1"><a class="header-anchor" href="#torchrecè°ƒç ”"><span>Torchrecè°ƒç ”</span></a></h1><h2 id="_0-0-æœªè¿ç§»ä¾èµ–åº“æ±‡æ€»" tabindex="-1"><a class="header-anchor" href="#_0-0-æœªè¿ç§»ä¾èµ–åº“æ±‡æ€»"><span>0.0 æœªè¿ç§»ä¾èµ–åº“æ±‡æ€»</span></a></h2><ol><li>torch <ul><li>torch.distributed._shard</li><li>torch.fx._compatibility</li><li>torch.distributed._composable</li><li>torch.distributed.optim._apply_optimizer_in_backward</li><li>torch.distributed.fsdp</li><li>torch._C.distributed_c10d.ProcessGroupNCCL</li><li>torch.distributed._composable.contract</li></ul></li><li>fbgemm-gpu</li></ol><h2 id="_0-1-qa" tabindex="-1"><a class="header-anchor" href="#_0-1-qa"><span>0.1 QA</span></a></h2><ul><li>å‚è€ƒèµ„æ–™ï¼š <ul><li>https://pytorch.org/torchrec/ ï¼ˆtorchrecæ–‡æ¡£ï¼‰</li><li>https://github.com/facebookresearch/dlrm/tree/main/torchrec_dlrm ï¼ˆdlrmï¼‰</li><li>https://blog.csdn.net/u013701860/article/details/51140762 ï¼ˆuvmï¼‰</li><li>https://www.sohu.com/a/560275515_100093134ï¼ˆæ•´ä½“ä»‹ç»ï¼‰</li><li>https://zhuanlan.zhihu.com/p/619060815ï¼ˆæ•´ä½“ä»‹ç»ï¼‰</li></ul></li></ul><ol><li><!----><ol><li>DMPï¼ˆDistributed Model Parallelï¼‰ï¼Œ EBCï¼ˆEmbedding Bag Collectionsï¼‰, KJT(Keyed Jagged Tensor), Planner, Sharderç­‰</li><li>ä¸»è¦æ˜¯å¯¹æ¨¡å‹å¹¶è¡Œï¼Œembeddingåˆ‡åˆ†æ–¹å¼ä»¥åŠè‡ªåŠ¨åˆ¶å®šåˆ†ç‰‡è®¡åˆ’ï¼Œç¨€ç–ï¼Œé‡åŒ–ç­‰çš„å®ç°</li></ol></li><li><!----><ol><li><p>ä½¿ç”¨EBCæ•°æ®ç»“æ„å­˜å‚¨embedding</p></li><li><p>ä½¿ç”¨fused ebcï¼ˆè§åç»­æ•°æ®ç»“æ„å¯¹æ¯”ä»‹ç»ï¼‰</p></li><li><p>æ”¯æŒåŠ¨æ€è¡¨</p><ol><li>åŠ¨æ€è¡¨æ˜¯cpuè¿˜æ˜¯gpuå®ç°çš„? <ol><li>CPU\GPUæ··åˆå®ç°ï¼Œtorchrecæ—©æœŸç‰ˆæœ¬ä¸æ”¯æŒåŠ¨æ€æ‰©å®¹ï¼Œå¯¹è¶…ç™¾äº¿è§„æ¨¡æ¨¡å‹æ— æ³•æ”¯æŒï¼Œåç»­æ˜¯å¾®ä¿¡å›¢é˜Ÿå¼€å‘åŠ¨æ€è¡¨åŠŸèƒ½å22.9æœˆæ•´åˆè¿›æ–°ç‰ˆæœ¬ï¼ˆè§<a href="https://zhuanlan.zhihu.com/p/619060815" target="_blank" rel="noopener noreferrer">ä»‹ç»<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>ï¼‰</li><li>ä¹Ÿç±»ä¼¼äºhugectrçš„gpu cacheæœºåˆ¶ï¼Œcpuç»´æŠ¤ä¸€ä¸ªæ˜ å°„è¡¨ï¼Œæ˜ å°„ä¸åŒembåœ¨gpuä¸­çš„åˆ†å¸ƒæƒ…å†µï¼Œå°†å¸¸ç”¨çš„embæ”¾æ˜¾å­˜ä¸­ï¼Œæœªå‘½ä¸­çš„ä»pså†…å­˜æï¼Œä½¿ç”¨çš„é©±é€ç­–ç•¥æ¯”è¾ƒæœ‰æ„æ€ï¼Œç»“åˆäº†LRUå’ŒLFU</li><li>dynamic embç›®å‰æ˜¯ä»¥torchrec_dynamic_embeddingæ‰©å±•åº“çš„å½¢å¼æ•´åˆè¿›torchrecé¡¹ç›®ä¸­çš„ï¼Œåœ¨torchrec/contrib/dynamic_embeddingä¸‹ï¼Œéœ€è¦å•ç‹¬ç¼–è¯‘ï¼›æ•´åˆè¿›torchrec/torchrec/csrcä¸­çš„ä»£ç æ˜¯ä»cotribè·¯å¾„ä¸‹è¿è¿‡æ¥çš„ï¼Œæ·»åŠ äº†æ›´å¤šbenchmarkså’Œunittestsï¼Œä½†æ˜¯python apiè²Œä¼¼è¿˜æ²¡å¼€å‘å®Œæˆï¼Œå»ºè®®ä½¿ç”¨contribè·¯å¾„ä¸‹çš„æºç å®‰è£…æ‰©å±•</li></ol></li><li>é©±é€ç­–ç•¥ <ol><li>ä½¿ç”¨LRUå’ŒLFUæ··åˆçš„é©±é€ç­–ç•¥, æœ€åè®°å½•åœ¨é˜Ÿåˆ—ä¸­çš„å†…å®¹è¢«é©±é€</li><li>é¢‘æ¬¡ä½¿ç”¨æŒ‡æ•°ä½è®°å½•ï¼Œ5bitsï¼Œæ¦‚ç‡ç®—æ³•ï¼Œæ¯æ¬¡å–é¢‘æ¬¡ä½éšæœºæ•°ï¼Œå…¨ä¸ºé›¶æ˜¯é¢‘æ¬¡æŒ‡æ•°åŠ ä¸€</li><li>æ—¶é—´ä½¿ç”¨27bitsè®°å½•</li><li>é¢‘æ¬¡ä½åœ¨æ—¶é—´ä½å‰ï¼ŒLFUä¼˜å…ˆçº§æ¯”LRUé«˜ï¼ˆå€¼è¶Šå°è¯´æ˜ä½¿ç”¨çš„è¶Šå°‘ï¼Œåˆ™æ¢å‡ºï¼‰</li><li>ä½¿ç”¨é˜Ÿåˆ—æ‰¹é‡é©±é€æ˜¾å­˜ä¸­çš„emb</li><li><img src="/assets/evict_strategy-1683885152669-14-9LymrW2h.png" alt="evict_strategy" tabindex="0" loading="lazy"><figcaption>evict_strategy</figcaption></li></ol></li></ol></li><li><p>æ”¯æŒå¤šç§embeddingåˆ‡åˆ†æ–¹å¼ï¼š</p><p>row-wise, column-wise, table-wise</p><p>åŠä¸‰ç§æ–¹å¼çš„æ··åˆ</p><ol><li>ä½¿ç”¨å¤šç§ä¸åŒçš„åˆ†ç‰‡æ–¹å¼çš„åŸå› ï¼š <ol><li>åœ¨å®é™…åœºæ™¯ä¸­ï¼Œä¸åŒç‰¹å¾çš„è®¿é—®é¢‘ç‡æ˜¯ä¸åŒçš„ï¼Œå‘ˆç°å¹‚ç‡åˆ†å¸ƒ</li><li>è‹¥åªä½¿ç”¨row-wiseï¼Œåˆ™æ¯ä¸ªç‰¹å¾åŸŸå¯¹åº”çš„emb tableå°†ä¼šåˆ†å¸ƒåˆ°ä¸åŒgpuä¸Šï¼Œç”±äºç‰¹å¾çš„å¹‚ç‡åˆ†å¸ƒï¼Œä»ä¸åŒgpuä¸­è·å–emb vectorçš„è§„æ¨¡å¯èƒ½ä¼šä¸å‡è¡¡ï¼Œå½±å“é€šä¿¡æ€§èƒ½</li><li>è€Œä½¿ç”¨col-wiseï¼Œåˆ™æ¯æ‰¾ä¸€ä¸ªç‰¹å¾å¯¹åº”çš„emb vectoréƒ½è¦ä»æ‰€æœ‰gpuä¸­è·å–æ•°æ®æ‹¼æ¥æˆå®Œæ•´çš„æ•°æ®ï¼Œä½¿é€šä¿¡é‡å‡è¡¡</li><li>ç”¨æˆ·å¯æ ¹æ®ä¸šåŠ¡åœºæ™¯è‡ªå®šä¹‰åˆ†ç‰‡æ–¹å¼</li></ol></li></ol></li></ol></li><li><!----><ol><li>å®ç°äº†DMPï¼Œå¯¹ç½‘ç»œsparseéƒ¨åˆ†ä½œæ¨¡å‹å¹¶è¡Œï¼Œå¯¹denseéƒ¨åˆ†ä½œDDP</li><li>piplineï¼šæ”¯æŒæ•°æ®è¯»å–ã€åˆ†å‘ã€è®­ç»ƒæµæ°´ <ol><li>torchrec.distributed.train_pipeline.TrainPipelineBase ä½¿ç”¨ä¸¤ä¸ªæµ <ul><li>the current (default) stream: æ‰§è¡Œå‰çº¿åå‘ä¼˜åŒ–è®¡ç®—</li><li>self._memcpy_stream:æ‰§è¡Œinputä»hoståˆ°GPU</li></ul></li><li>torchrec.distributed.train_pipeline.<strong>TrainPipelineSparseDist</strong>, éšè—all2allå»¶è¿Ÿï¼ŒåŒæ—¶ä¿ç•™å‰å‘/åå‘çš„è®­ç»ƒé¡ºåº <ul><li>stage 3: forward, backward - uses default CUDA stream</li><li>stage 2: ShardedModule.input_dist() - uses data_dist CUDA stream</li><li>stage 1: device transfer - uses memcpy CUDA stream</li></ul></li></ol></li></ol></li><li><!----><ol><li>ä¼ ç»Ÿæ¨èç³»ç»Ÿçš„psæ¶æ„ <ol><li><img src="/assets/72bada1fb3b644b68e0c32d0c2c0401e-BgkF98s3.png" alt="img" style="zoom:15%;"></li><li>ä»¥cpuä¸ºä¸­å¿ƒï¼Œpsåškvå­˜å‚¨ï¼Œworkeråœ¨æ¯ä¸ªiterä»psææ‰€éœ€emb vectorï¼Œå‰å‘åå‘åå°†æ›´æ–°åçš„ç‰¹å¾æ¨é€å›ps</li><li>emb tableåœ¨cpuä¸Š</li><li>è®­ç»ƒè¿‡ç¨‹éœ€è¦ç­‰å¾…é€šä¿¡ï¼Œéš¾ä»¥ç»„æˆæµæ°´çº¿ï¼Œæ— æ³•å……åˆ†åˆ©ç”¨ç®—åŠ›</li></ol></li><li>torchrecçš„psæ¶æ„ <ol><li><img src="/assets/5b33518e573e4d2793b492e1b1f428e0-D437P5NJ.png" alt="img" style="zoom:15%;"></li><li>ä»¥gpuä¸ºä¸­å¿ƒï¼Œåˆ©ç”¨gpué—´é«˜å¸¦å®½é€šä¿¡äº¤æ¢æ‰€éœ€emb vectorï¼ˆæ•°æ®ã€æ¨¡å‹æ··åˆå¹¶è¡Œï¼‰ï¼Œèƒ½å‡å°‘ä¸psäº¤æ¢æ•°æ®çš„é€šä¿¡å¼€é”€</li><li>emb shardsï¼ˆemb tableçš„å­é›†ï¼‰åˆ†å¸ƒå¼å­˜å‚¨åœ¨å¤šä¸ªgpuä¸Šï¼ˆæˆ–uvmå†…å­˜ä¸­ï¼‰ï¼ˆè§„æ¨¡ä¸ç®—å¤ªå¤§ï¼‰</li><li>è¿˜å¯æ”¯æŒåŠ¨æ€è¡¨ï¼Œåˆ©ç”¨pså­˜å‚¨gpuæ”¾ä¸ä¸‹çš„embï¼ˆè¶…å¤§è§„æ¨¡ï¼‰</li></ol></li><li>pså’Œuvmçš„å…³ç³» <ol><li>uvmæ˜¯åœ¨cpuå†…å­˜ä¸­å¼€è¾Ÿä¸€å—ç©ºé—´å½“ä½œæ˜¯å¯¹gpuæ˜¾å­˜çš„æ‰©å±•ï¼Œcpuã€gpuéƒ½èƒ½ä½¿ç”¨åŒæ ·åœ°æŒ‡é’ˆè®¿é—®å…¶ä¸­çš„æ•°æ®ï¼Œä¾¿äºå¼€å‘ï¼Œä½†å®é™…ä¸Šè¿˜æ˜¯ä¼šåšcpuã€gpué—´æ•°æ®çš„éšå¼ä¼ è¾“</li><li>ä½¿ç”¨uvmæ˜¯åœ¨emb tableè§„æ¨¡ä¸ç®—ç‰¹åˆ«å¤§ï¼Œä½†gpuæ˜¾å­˜æ— æ³•å®Œå…¨æ”¾å…¥æ‰€æœ‰emb tableçš„æƒ…å†µä¸‹ä½¿ç”¨çš„ï¼Œä¸€å®šç¨‹åº¦ä¸Šæ‰©å±•æ˜¾å­˜å¤§å°ï¼ˆé€»è¾‘å¤§å°ï¼‰</li><li>è€Œä½¿ç”¨è¶…å¤§è§„æ¨¡çš„emb tableæ—¶ï¼Œå°±éœ€è¦pså­˜å‚¨æ›´å¤šçš„emb tableï¼Œå†é…åˆåŠ¨æ€embå’Œç¼“å­˜æ¥å®ç°</li></ol></li></ol></li><li><!----><ol><li>uvmåªæ˜¯å¯¹æ˜¾å­˜ä¸€å®šç¨‹åº¦çš„æ‰©å±•ï¼Œä¾¿äºå¼€å‘ä¸ç»´æŠ¤ï¼Œä¸èƒ½å®Œå…¨è§£å†³æ˜¾å­˜ä¸å¤Ÿç”¨çš„æƒ…å†µï¼Œè€Œå®é™…ä¸Šè¿˜æ˜¯ä¼šå‘ç”Ÿcpuã€gpué—´çš„éšå¼æ•°æ®æ‹·è´ï¼Œç”šè‡³ä½¿ç”¨streamå’ŒcudaMemcpyAsyncæ€§èƒ½ä¼šæ¯”ç”¨uvmæ›´é«˜ï¼ˆå‚è€ƒ<a href="https://blog.csdn.net/u013701860/article/details/51140762" target="_blank" rel="noopener noreferrer">é“¾æ¥<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>ï¼‰</li><li>ä½¿ç”¨sharderæ˜¯ä¸ºäº†å¯¹embeddingåšæ¨¡å‹å¹¶è¡Œï¼Œæ˜¯ä¸ºäº†æ‘†è„±ä»¥cpuä¸ºä¸­å¿ƒçš„ä¼ ç»Ÿpsæ¶æ„ï¼Œæé«˜gpuä½¿ç”¨æ€§èƒ½ï¼Œä¸åŒçš„åˆ†ç‰‡æ–¹å¼ä¹Ÿæ˜¯ä¸ºäº†è§£å†³æ˜¾å­˜æ— æ³•å­˜ä¸‹æ‰€æœ‰emb tableçš„é—®é¢˜</li><li>ä¸åŒçš„åˆ†ç‰‡æ–¹å¼æ˜¯è€ƒé‡äº†ä¸åŒçš„ç³»ç»Ÿè¿è¡Œç¯å¢ƒä»¥åŠç”¨æˆ·éœ€æ±‚ï¼Œä»¥æ±‚æœ€å¤§åŒ–æ€§èƒ½ï¼Œä¸ªäººç†è§£uvmæ˜¯ä¸ºåˆ†ç‰‡æä¾›äº†è¾…åŠ©å’Œç®€åŒ–å¼€å‘ç»´æŠ¤ï¼Œå¹¶ä¸æ˜¯åªè¦ç”¨äº†uvmå°±èƒ½è§£å†³æ˜¾å­˜é—®é¢˜ï¼Œå°±ä¸éœ€è¦æ¨¡å‹å¹¶è¡Œèƒ½ç›´æ¥ä»uvmä¸­è¯»embåšæ•°æ®å¹¶è¡Œäº†ï¼Œè¿™æ ·åè€Œé€€åŒ–ä¸ºæ—©æœŸçš„psæ¶æ„ï¼Œæ— æ³•å……åˆ†åˆ©ç”¨gpué«˜å¸¦å®½</li></ol></li><li><!----><ol><li><p>EBC&amp;FusedEBC</p><ol><li><p>Embedding å’ŒEmbeddingBagçš„åŒºåˆ«</p><ol><li><p>ä½¿ç”¨embeddingbagä¸»è¦æ˜¯ä¸ºäº†è§£å†³multi-hotçš„æƒ…å†µ, è€Œä¸”è¿˜èƒ½åŒæ—¶æ”¯æŒone-hot</p></li><li><p>embeddingæ˜¯ç›´æ¥æŸ¥è¯¢å‡ºå¤šä¸ªemb vec, è€Œembeddingbagæ˜¯å°†æŸ¥è¯¢å‡ºçš„å¤šä¸ªemb vecåšæ± åŒ–(æ­¤å¤„ä¸ºç›¸åŠ )åè¾“å‡ºä¸€ä¸ªæœ€ç»ˆçš„emb vec, è¿™æ ·å°±èƒ½è§£å†³multio-hotçš„æƒ…å†µ(å¯¹äºä¸€ä¸ªç‰¹å¾åŸŸ,ä¸ä½†æ”¯æŒå•é€‰,è¿˜æ”¯æŒå¤šé€‰)</p><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code><span class="token keyword">import</span> torch

vocab_size <span class="token operator">=</span> <span class="token number">5</span> <span class="token comment"># feature_slot size</span>
embedding_dim <span class="token operator">=</span> <span class="token number">3</span>

<span class="token comment"># å¤§å°ç›¸åŒçš„emb å’Œemb_bag</span>
embedding <span class="token operator">=</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>num_embeddings<span class="token operator">=</span>vocab_size<span class="token punctuation">,</span> embedding_dim<span class="token operator">=</span>embedding_dim<span class="token punctuation">)</span>
embedding_bag <span class="token operator">=</span> nn<span class="token punctuation">.</span>EmbeddingBag<span class="token punctuation">(</span>num_embeddings<span class="token operator">=</span>vocab_size<span class="token punctuation">,</span> embedding_dim<span class="token operator">=</span>embedding_dim<span class="token punctuation">,</span> mode<span class="token operator">=</span><span class="token string">&#39;sum&#39;</span><span class="token punctuation">)</span>

<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> embedding<span class="token punctuation">.</span>weight
Parameter containing<span class="token punctuation">:</span>
tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span> <span class="token number">0.2669</span><span class="token punctuation">,</span>  <span class="token number">0.0411</span><span class="token punctuation">,</span>  <span class="token number">1.8483</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">0.1264</span><span class="token punctuation">,</span>  <span class="token number">0.4678</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.7871</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">0.9744</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1.1333</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">2.0062</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span> <span class="token number">0.3138</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1.0656</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.6442</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span> <span class="token number">0.1350</span><span class="token punctuation">,</span>  <span class="token number">0.1416</span><span class="token punctuation">,</span>  <span class="token number">0.0687</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
        
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> embedding_bag<span class="token punctuation">.</span>weight
Parameter containing<span class="token punctuation">:</span>
tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">0.9410</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1.2599</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.5800</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1.3137</span><span class="token punctuation">,</span>  <span class="token number">0.2207</span><span class="token punctuation">,</span>  <span class="token number">0.1835</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span> <span class="token number">0.1689</span><span class="token punctuation">,</span>  <span class="token number">2.0827</span><span class="token punctuation">,</span>  <span class="token number">0.7237</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span> <span class="token number">0.2223</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.5492</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.6188</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span> <span class="token number">0.4136</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1.1578</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.7838</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
        
input_ids <span class="token operator">=</span> torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> embedding<span class="token punctuation">(</span>input_ids<span class="token punctuation">)</span>
tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">0.1264</span><span class="token punctuation">,</span>  <span class="token number">0.4678</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.7871</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
         <span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">0.9744</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1.1333</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">2.0062</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
         <span class="token punctuation">[</span> <span class="token number">0.3138</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1.0656</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.6442</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> grad_fn<span class="token operator">=</span><span class="token operator">&lt;</span>EmbeddingBackward0<span class="token operator">&gt;</span><span class="token punctuation">)</span>
         
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> embedding_bag<span class="token punctuation">(</span>input_ids<span class="token punctuation">)</span>
tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">0.9225</span><span class="token punctuation">,</span>  <span class="token number">1.7543</span><span class="token punctuation">,</span>  <span class="token number">0.2883</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> grad_fn<span class="token operator">=</span><span class="token operator">&lt;</span>EmbeddingBagBackward0<span class="token operator">&gt;</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></li></ol></li><li><p>EmbeddingBagå’ŒEmbeddingBagCollectionçš„åŒºåˆ«</p><ol><li><p>EBCç”¨äºç®¡ç†å¤šä¸ªEmbeddingBags</p><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code>B <span class="token operator">=</span> <span class="token number">2</span>
D <span class="token operator">=</span> <span class="token number">8</span>
dense_in_features <span class="token operator">=</span> <span class="token number">100</span>

eb1_config <span class="token operator">=</span> EmbeddingBagConfig<span class="token punctuation">(</span>
    name<span class="token operator">=</span><span class="token string">&quot;t1&quot;</span><span class="token punctuation">,</span> embedding_dim<span class="token operator">=</span>D<span class="token punctuation">,</span> num_embeddings<span class="token operator">=</span><span class="token number">100</span><span class="token punctuation">,</span> feature_names<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">&quot;f1&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;f3&quot;</span><span class="token punctuation">]</span>
<span class="token punctuation">)</span>
eb2_config <span class="token operator">=</span> EmbeddingBagConfig<span class="token punctuation">(</span>
    name<span class="token operator">=</span><span class="token string">&quot;t2&quot;</span><span class="token punctuation">,</span>
    embedding_dim<span class="token operator">=</span>D<span class="token punctuation">,</span>
    num_embeddings<span class="token operator">=</span><span class="token number">100</span><span class="token punctuation">,</span>
    feature_names<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">&quot;f2&quot;</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
<span class="token punctuation">)</span>

ebc <span class="token operator">=</span> EmbeddingBagCollection<span class="token punctuation">(</span>tables<span class="token operator">=</span><span class="token punctuation">[</span>eb1_config<span class="token punctuation">,</span> eb2_config<span class="token punctuation">]</span><span class="token punctuation">)</span>

<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> ebc
EmbeddingBagCollection<span class="token punctuation">(</span>
  <span class="token punctuation">(</span>embedding_bags<span class="token punctuation">)</span><span class="token punctuation">:</span> ModuleDict<span class="token punctuation">(</span>
    <span class="token punctuation">(</span>t1<span class="token punctuation">)</span><span class="token punctuation">:</span> EmbeddingBag<span class="token punctuation">(</span><span class="token number">100</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> mode<span class="token operator">=</span><span class="token string">&#39;sum&#39;</span><span class="token punctuation">)</span>
    <span class="token punctuation">(</span>t2<span class="token punctuation">)</span><span class="token punctuation">:</span> EmbeddingBag<span class="token punctuation">(</span><span class="token number">100</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> mode<span class="token operator">=</span><span class="token string">&#39;sum&#39;</span><span class="token punctuation">)</span>
  <span class="token punctuation">)</span>
<span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></li></ol></li><li><p>EBCå’ŒFusedEBCçš„åŒºåˆ«</p><ol><li><img src="/assets/mmexport1683360092606-BksPmBkX.png" alt="mmexport1683360092606" style="zoom:25%;"></li><li><img src="/assets/mmexport1683360093936-BPDzbGUc.png" alt="mmexport1683360093936" style="zoom:25%;"></li><li>fused ebcæ˜¯å¯¹æ™®é€šebcä½œäº†ç®—å­æ–¹é¢çš„èåˆä¼˜åŒ–ï¼ˆä¾èµ–fbgemmåº“ï¼‰ï¼Œå¦‚ï¼Œå¯ä½¿ç”¨ä¸€ä¸ªkernelå®ç°å¤šä¸ªembçš„æŸ¥è¯¢</li><li>åœ¨æŸ¥è¡¨æ—¶ï¼Œå¸¸è§„çš„æ–¹æ³•æ˜¯ä¸€ä¸ªwarpå†…çš„æ‰€æœ‰çº¿ç¨‹è¿ç»­è¯»å–ä¸€ç»„å¾…æŸ¥è¯¢keyï¼Œå†ä»emb tableä¸­éšæœºè¯»å–å¯¹åº”emb vectorï¼Œæ— æ³•å……åˆ†åˆ©ç”¨æ˜¾å­˜å¸¦å®½ï¼›</li><li>fused ebcçš„æŸ¥è¡¨æ–¹å¼æ˜¯ä½¿ç”¨cudaçš„shfl_syncï¼Œä¸€ä¸ªwarpå†…çš„æ‰€æœ‰threadåŒæ—¶æ‹·è´åŒä¸€æ¡emb vectorï¼Œè¿™æ ·ä¸¤ä¸ªè¿‡ç¨‹éƒ½æ˜¯è¿ç»­è¯»</li></ol></li></ol></li><li><p>KJT</p><ol><li><p>kjtæ•°æ®ç»“æ„æ˜¯åœ¨cpuè¿˜æ˜¯gpuä½¿ç”¨?ä¸ºä½•ä¼šç”¨åˆ°ä¸åŒé•¿åº¦çš„tensor?</p><ol><li><p>ä½¿ç”¨ä¸åŒé•¿åº¦çš„tensoræ˜¯ä¸ºäº†è¡¨ç¤ºå¦‚multi-hot, ç¼ºçœå€¼ï¼Œæ¯ä¸ªbatchä¸­å¯¹åº”ç‰¹å¾åŸŸç‰¹å¾å‡ºç°çš„æƒ…å†µ, åœ¨cpu, gpuéƒ½ä¼šä½¿ç”¨</p></li><li><p>torchrecä¸­æ˜¯ä½¿ç”¨EBCç»“æ„(åŸºäºtorchçš„embeddingbag)å­˜å‚¨å¤šä¸ªç‰¹å¾åŸŸçš„åµŒå…¥è¡¨çš„, ç„¶åä½¿ç”¨kjtæ•°æ®ç»“æ„ä½œä¸ºå¾…æŸ¥è¯¢keyçš„å¼ é‡, ä¼ å…¥ebcå¾—åˆ°å¯¹åº”çš„åµŒå…¥å‘é‡</p><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code><span class="token comment"># ç¤ºä¾‹ï¼š</span>
<span class="token comment">#|------------|------------|</span>
<span class="token comment">#| product ID | user ID    |</span>
<span class="token comment">#|------------|------------|</span>
<span class="token comment">#| [101, 202] | [404]      |</span>
<span class="token comment">#| []         | [505]      |</span>
<span class="token comment">#| [303]      | [606]      |</span>
<span class="token comment">#|------------|------------|</span>

mb <span class="token operator">=</span> torchrec<span class="token punctuation">.</span>KeyedJaggedTensor<span class="token punctuation">(</span>
    keys <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">&quot;product&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;user&quot;</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    values <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">101</span><span class="token punctuation">,</span> <span class="token number">202</span><span class="token punctuation">,</span> <span class="token number">303</span><span class="token punctuation">,</span> <span class="token number">404</span><span class="token punctuation">,</span> <span class="token number">505</span><span class="token punctuation">,</span> <span class="token number">606</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    lengths <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>int64<span class="token punctuation">)</span><span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
<span class="token punctuation">)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span>mb<span class="token punctuation">.</span>to<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">&quot;cpu&quot;</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> KeyedJaggedTensor<span class="token punctuation">(</span><span class="token punctuation">{</span>
    <span class="token string">&quot;product&quot;</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">101</span><span class="token punctuation">,</span> <span class="token number">202</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">303</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    <span class="token string">&quot;user&quot;</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">404</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">505</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">606</span><span class="token punctuation">]</span><span class="token punctuation">]</span>
<span class="token punctuation">}</span><span class="token punctuation">)</span>


<span class="token comment"># sparse å‚æ•°å­˜å‚¨</span>
eb1_config <span class="token operator">=</span> EmbeddingBagConfig<span class="token punctuation">(</span>
name<span class="token operator">=</span><span class="token string">&quot;t1&quot;</span><span class="token punctuation">,</span> embedding_dim<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> num_embeddings<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">,</span> feature_names<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">&quot;f1&quot;</span><span class="token punctuation">]</span>
<span class="token punctuation">)</span>
eb2_config <span class="token operator">=</span> EmbeddingBagConfig<span class="token punctuation">(</span>
name<span class="token operator">=</span><span class="token string">&quot;t2&quot;</span><span class="token punctuation">,</span> embedding_dim<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> num_embeddings<span class="token operator">=</span><span class="token number">12</span><span class="token punctuation">,</span> feature_names<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">&quot;f2&quot;</span><span class="token punctuation">]</span>
<span class="token punctuation">)</span>

ebc <span class="token operator">=</span> EmbeddingBagCollection<span class="token punctuation">(</span>tables<span class="token operator">=</span><span class="token punctuation">[</span>eb1_config<span class="token punctuation">]</span><span class="token punctuation">)</span>
sparse_arch <span class="token operator">=</span> SparseArch<span class="token punctuation">(</span>ebc<span class="token punctuation">)</span>

<span class="token comment">#     0       1        2  &lt;-- batch</span>
<span class="token comment"># 0   [0,1] None    [2]</span>
<span class="token comment"># 1   [3]    [4]    [5,6,7]</span>
<span class="token comment"># feature</span>

features <span class="token operator">=</span> KeyedJaggedTensor<span class="token punctuation">.</span>from_offsets_sync<span class="token punctuation">(</span>
   keys<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">&quot;f1&quot;</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
   values<span class="token operator">=</span>torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token number">6</span><span class="token punctuation">,</span><span class="token number">7</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
   offsets<span class="token operator">=</span>torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
   <span class="token punctuation">)</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> sparse_arch<span class="token punctuation">(</span>features<span class="token punctuation">)</span>
tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">0.0635</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.6799</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1.8670</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token punctuation">[</span> <span class="token number">0.0000</span><span class="token punctuation">,</span>  <span class="token number">0.0000</span><span class="token punctuation">,</span>  <span class="token number">0.0000</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">0.8205</span><span class="token punctuation">,</span>  <span class="token number">0.8911</span><span class="token punctuation">,</span>  <span class="token number">0.5688</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token punctuation">[</span> <span class="token number">1.7768</span><span class="token punctuation">,</span>  <span class="token number">0.3763</span><span class="token punctuation">,</span>  <span class="token number">0.9286</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1.6304</span><span class="token punctuation">,</span>  <span class="token number">1.8683</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.0427</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1.6682</span><span class="token punctuation">,</span>  <span class="token number">2.4401</span><span class="token punctuation">,</span>  <span class="token number">0.6767</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> grad_fn<span class="token operator">=</span><span class="token operator">&lt;</span>ReshapeAliasBackward0<span class="token operator">&gt;</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></li></ol></li></ol></li></ol></li><li><!----><ol><li>embeddingåˆ†ç‰‡æ–¹å¼çš„åŒºåˆ«: <ul><li>hugectrçš„distributedslotembeddinghashæ˜¯å°†æ¯ä¸ªslotåˆ’åˆ†åˆ°æ‰€æœ‰GPUä¸Š,ç›¸å½“äºtorchrecçš„row-wise sharding</li><li>hugectrçš„localizedslotembeddinghashæ˜¯å°†æŸä¸ªå®Œæ•´çš„slotåˆ†é…åˆ°æŸä¸ªGPUä¸Š,ç›¸å½“äºtorchrecçš„table-wise sharding</li><li>é™¤æ­¤ä¹‹å¤–, torchrecè¿˜<strong>æ”¯æŒæ›´å¤šçš„åˆ‡åˆ†æ–¹å¼</strong>, æ»¡è¶³ç”¨æˆ·å®šåˆ¶åŒ–ç½‘ç»œæ‹“æ‰‘çš„éœ€æ±‚, ä¸”èƒ½å¤Ÿ<strong>è‡ªåŠ¨é€‰æ‹©</strong>åœ¨å½“å‰è¿è¡Œç¯å¢ƒä¸‹æœ€ä¼˜çš„åˆ†ç‰‡æ–¹å¼</li></ul></li><li>åœ¨åˆ†å¸ƒå¼ä¸Šéƒ½æ”¯æŒ<strong>æ•°æ®æ¨¡å‹æ··åˆå¹¶è¡Œ</strong>, denseç½‘ç»œéƒ¨åˆ†çš„å¤„ç†éƒ½æ˜¯æ•°æ®å¹¶è¡Œ, embeddingéƒ½æ˜¯æ¨¡å‹å¹¶è¡Œ, ä¸”å‰å‘åå‘çš„è®¡ç®—è¿‡ç¨‹ä¹Ÿéƒ½åœ¨GPUä¸Šè¿›è¡Œ, æœ€å¤§ç¨‹åº¦åˆ©ç”¨äº†GPUçš„å¹¶è¡Œè®¡ç®—æ€§èƒ½</li><li>åœ¨æ•°æ®åŠ è½½ä¸Šä¹Ÿéƒ½æ”¯æŒæ•°æ®è¯»å–,åˆ†å‘,è®­ç»ƒæµæ°´</li><li>æ•°æ®é¢„å¤„ç†: <ul><li>torchrecæ˜¯å…ˆè¡¥å…¨ç¼ºçœå€¼,ç„¶åé‡æ–°æ˜ å°„key idåˆ°è¿ç»­idç©ºé—´,å†å°†å‡ºç°æ¬¡æ•°å°‘äºé˜ˆå€¼3çš„keyæ˜ å°„åˆ°key_id=1,å†åšshuffle</li><li>hugecträ¹Ÿæ˜¯å…ˆè¡¥å…¨ç¼ºçœå€¼,é‡æ–°æ˜ å°„åˆ°è¿ç»­id, å†å°†å‡ºç°æ¬¡æ•°å°‘äºé˜ˆå€¼6çš„keyæ˜ å°„åˆ°æŸä¸€ç‰¹å®šid,å†åšshuffle,**æœ€åè¿˜å¯ä»¥ä½¿ç”¨feature cross(ç‰¹å¾ç»„åˆ)**è¿›ä¸€æ­¥å‡å°‘ç‰¹å¾æ•°é‡,æå‡ç»„åˆç‰¹å¾çš„è¡¨è¾¾èƒ½åŠ›</li></ul></li></ol></li><li><!----><ol><li>ç”¨åœ¨æ¨¡å‹å¹¶è¡Œæ¨¡å—ä¸­, ä¸ºFSDPæ–¹å¼æä¾›æ”¯æŒ, å®é™…ä¸Šåªç”¨åœ¨testè„šæœ¬ä¸­, dlrmç½‘ç»œæœªç”¨åˆ°</li></ol></li><li><!----><ol><li>ç®€å•æ€»ç»“,å°±æ˜¯æ ¹æ®è¾“å…¥ç½‘ç»œç»“æ„,å‚æ•°è§„æ¨¡, è¿è¡Œçš„è®¾å¤‡ç¯å¢ƒçš„æ‹“æ‰‘,è®¾å¤‡çš„å¸¦å®½,æ˜¾å­˜, æ•°æ®ç±»å‹ç­‰ä¿¡æ¯, ç©·ä¸¾æ‰€æœ‰åˆ‡åˆ†æ–¹æ¡ˆ,ç„¶åæŒ‘å‡ºæ‰€æœ‰å¯è¡Œçš„æ–¹æ¡ˆåšä¸€ä¸ªæ¨¡æ‹Ÿæ€§èƒ½è¯„ä¼°, æ ¹æ®è¯„ä¼°çš„ç»“æœå†æŒ‘å‡ºæ€§èƒ½æœ€å¥½çš„æ–¹æ¡ˆå‡ºæ¥</li><li>æ‰§è¡Œæµç¨‹&amp;ç»†èŠ‚ï¼š <ol><li>è®¾ç½®Topology, ç”ŸæˆShardingPlan, Planneræœ‰ä¸¤ä¸ªé˜¶æ®µ: <ol><li>Planning stage:åœ¨ç»™å®šsharderså’Œè¿è¡Œç¯å¢ƒ(Topology)çš„å‰æä¸‹å†³å®šå¦‚ä½•åˆ‡åˆ†æ¨¡å‹,è¾“å‡ºShardingPlan, ä¼šç»™å‡ºä»€ä¹ˆåˆ‡åˆ†æ–¹å¼,ä½¿ç”¨ä»€ä¹ˆcompute kernel, perf, rank, è¯„ä¼°å°†ä¼šå ç”¨å¤šå°‘HBMå­˜å‚¨ç©ºé—´ <ol><li>Topologyç±»: ä»£è¡¨ç½‘ç»œè®¾å¤‡é›†ç¾¤ç»„ç»‡æ–¹å¼, å¯è®¾ç½®è®¾å¤‡ç±»å‹, world size, æ¯ä¸ªè®¾å¤‡çš„å­˜å‚¨ç©ºé—´(hbm,ddr)å¤§å°,å¸¦å®½ç­‰.</li><li>Shard: embçš„å­è¡¨, ä¸»è¦è®°å½•äº†sizeå’Œoffset</li><li>enumeratorç±»ç”¨äºåˆ—ä¸¾æ‰€æœ‰å¯è¡Œçš„æ–¹æ¡ˆï¼Œä¹‹åestimatorç±»å†å¯¹ä¸åŒçš„åˆ‡åˆ†æ–¹å¼åšperf /storage estimation, ä¹‹åå°†æ–¹æ¡ˆä¼ å…¥proposersæŒ‰ç…§ä¸åŒç­–ç•¥å¯¹æ‰€æœ‰æ–¹æ¡ˆçš„è¯„ä¼°æ€§èƒ½ä½œä¼˜åŠ£æ’åº, æœ€åé€‰æ‹©æœ€ä¼˜çš„æ–¹æ¡ˆ</li></ol></li><li>Sharding stage:æ ¹æ®ShardingPlanä½¿ç”¨ç»™å®šçš„sharderåˆ‡åˆ†æ¨¡å‹,éœ€è¦åœ¨è¿è¡Œç¯å¢ƒä¸­æ‰§è¡Œ <ol><li>Partitioner, ç”¨äºåˆ‡åˆ†shards, æœ‰å¤šç§ç­–ç•¥,å¦‚Greedy, BLDM, Linearç­‰(ç›®å‰åªæœ‰Greedyçš„å®ç°)</li></ol></li></ol></li><li>åœ¨rank0ä¸Šåˆ¶å®šplanç„¶åbroadcaståˆ°æ‰€æœ‰è®¾å¤‡ä¸Š</li><li>éœ€è¦é¢„ç•™å‡ºkjtå’Œdenseéƒ¨åˆ†çš„ç©ºé—´å‡ºæ¥ä¸è€ƒè™‘åœ¨shardså æ®çš„ç©ºé—´ä¹‹å†…</li><li>é¢„ä¼°embedding wall time perf å’Œå³°å€¼å†…å­˜ï¼Œæ˜¯æŒ‰ç…§ç»éªŒè®¾å®šäº†ä¸€ç³»åˆ—å¦‚å¸¦å®½ä¸€ç±»çš„å¸¸é‡æ¥é¢„ä¼°çš„ï¼ˆè§constants.pyï¼‰ï¼Œä¼šè®¡ç®—æ¯ä¸€ä¸ªshardsçš„æ€§èƒ½</li><li>ä½¿ç”¨ParameterConstaintsé€‰æ‹©shardingç±»å‹æä¾›poolingå› å­ <ol><li>èƒ½å¤Ÿå¸®åŠ©planneræ›´å‡†ç¡®çš„è¯„ä¼°æ€§èƒ½</li><li>ç”¨æˆ·è®¾ç½®ä¸€äº›shardingé™åˆ¶é¡¹æ¥æŒ‡å¯¼planner, å¦‚,é™å®šä»€ä¹ˆåˆ’åˆ†æ–¹å¼, æœ€å°‘åˆ’åˆ†å¤šå°‘å—, ä½¿ç”¨ä»€ä¹ˆè®¡ç®—kernelç­‰</li></ol></li><li>è¶…è¿‡GPUæ˜¾å­˜æ—¶è‡ªåŠ¨è§¦å‘UVM Caching</li></ol></li></ol></li><li><!----><ol><li>ä½¿ç”¨torch.utils.data.DataLoaderä»torchrec.datasets.criteo.InMemoryBinaryCriteoIterDataPipeä¸­è¯»å–æ•°æ®, ä»¥torchrec.datasets.utils.Batchæ•°æ®ç»“æ„å­˜å…¥ç”¨äºæ¯ä¸ªiter</li><li>æ²¡æœ‰åƒhugecträ½¿ç”¨keyset listæ–‡ä»¶ç›´æ¥é¢„å…ˆåˆ’åˆ†å¥½æ¯ä¸ªpasséœ€è¦ä½¿ç”¨ä»€ä¹ˆembçš„æ­¥éª¤</li></ol></li><li><!----><ol><li><p>torchrecå¯¹dlrmç½‘ç»œç»“æ„åšäº†å¯¹åº”å®ç°ä»¥åŠä¼˜åŒ–ç‰ˆæœ¬ï¼Œä¸»è¦å®ç°äº†SparseArch, DenseArch, InteractionArch, OverArchï¼Œ åˆ†åˆ«å¯¹åº”ç½‘ç»œç»“æ„ä¸­çš„Embedding, Bottom MLP, Pairwise interaction, concat&amp;Top MLP<img src="/assets/2023-05-06%2014-52-59%20%E7%9A%84%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE-1683884972652-10-_uxnbtuX.png" alt="2023-05-06 14-52-59 çš„å±å¹•æˆªå›¾" style="zoom:67%;"></p></li><li><p><a href="https://github.com/facebookresearch/dlrm/tree/main/torchrec_dlrm" target="_blank" rel="noopener noreferrer">é¡¹ç›®<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>ä¸­æœ‰torchrecå¯¹åº”çš„ç½‘ç»œè®­ç»ƒè„šæœ¬</p></li><li><p>é¦–å…ˆè¯»å–ç½‘ç»œå‚æ•°ï¼Œä½¿ç”¨torch.distributedè®¾ç½®å¥½ç½‘ç»œæ‹“æ‰‘, åç«¯å¯é€‰ncclå’Œgloo</p><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code>rank <span class="token operator">=</span> <span class="token builtin">int</span><span class="token punctuation">(</span>os<span class="token punctuation">.</span>environ<span class="token punctuation">[</span><span class="token string">&quot;LOCAL_RANK&quot;</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token keyword">if</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    device<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>device <span class="token operator">=</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f&quot;cuda:</span><span class="token interpolation"><span class="token punctuation">{</span>rank<span class="token punctuation">}</span></span><span class="token string">&quot;</span></span><span class="token punctuation">)</span>
    backend <span class="token operator">=</span> <span class="token string">&quot;nccl&quot;</span>
    torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>set_device<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
<span class="token keyword">else</span><span class="token punctuation">:</span>
    device<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>device <span class="token operator">=</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">&quot;cpu&quot;</span><span class="token punctuation">)</span>
    backend <span class="token operator">=</span> <span class="token string">&quot;gloo&quot;</span>

<span class="token keyword">if</span> rank <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>
        <span class="token string">&quot;PARAMS: (lr, batch_size, warmup_steps, decay_start, decay_steps): &quot;</span>
        <span class="token string-interpolation"><span class="token string">f&quot;</span><span class="token interpolation"><span class="token punctuation">{</span><span class="token punctuation">(</span>args<span class="token punctuation">.</span>learning_rate<span class="token punctuation">,</span> args<span class="token punctuation">.</span>batch_size<span class="token punctuation">,</span> args<span class="token punctuation">.</span>lr_warmup_steps<span class="token punctuation">,</span> args<span class="token punctuation">.</span>lr_decay_start<span class="token punctuation">,</span> args<span class="token punctuation">.</span>lr_decay_steps<span class="token punctuation">)</span><span class="token punctuation">}</span></span><span class="token string">&quot;</span></span>
    <span class="token punctuation">)</span>
dist<span class="token punctuation">.</span>init_process_group<span class="token punctuation">(</span>backend<span class="token operator">=</span>backend<span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></li><li><p>ç„¶åè®¾ç½®dataloaderï¼Œå®šä¹‰æ¨¡å‹ç»“æ„ï¼Œéœ€è¦æ ¹æ®ä½¿ç”¨çš„æ•°æ®é›†å®šä¹‰EBCï¼Œå®šä¹‰ä¸åŒç½‘ç»œå±‚çš„å¤§å°</p><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code>train_dataloader <span class="token operator">=</span> get_dataloader<span class="token punctuation">(</span>args<span class="token punctuation">,</span> backend<span class="token punctuation">,</span> <span class="token string">&quot;train&quot;</span><span class="token punctuation">)</span>
val_dataloader <span class="token operator">=</span> get_dataloader<span class="token punctuation">(</span>args<span class="token punctuation">,</span> backend<span class="token punctuation">,</span> <span class="token string">&quot;val&quot;</span><span class="token punctuation">)</span>
test_dataloader <span class="token operator">=</span> get_dataloader<span class="token punctuation">(</span>args<span class="token punctuation">,</span> backend<span class="token punctuation">,</span> <span class="token string">&quot;test&quot;</span><span class="token punctuation">)</span>

eb_configs <span class="token operator">=</span> <span class="token punctuation">[</span>
    EmbeddingBagConfig<span class="token punctuation">(</span>
        name<span class="token operator">=</span><span class="token string-interpolation"><span class="token string">f&quot;t_</span><span class="token interpolation"><span class="token punctuation">{</span>feature_name<span class="token punctuation">}</span></span><span class="token string">&quot;</span></span><span class="token punctuation">,</span>
        embedding_dim<span class="token operator">=</span>args<span class="token punctuation">.</span>embedding_dim<span class="token punctuation">,</span>
        num_embeddings<span class="token operator">=</span>none_throws<span class="token punctuation">(</span>args<span class="token punctuation">.</span>num_embeddings_per_feature<span class="token punctuation">)</span><span class="token punctuation">[</span>feature_idx<span class="token punctuation">]</span>
        <span class="token keyword">if</span> args<span class="token punctuation">.</span>num_embeddings <span class="token keyword">is</span> <span class="token boolean">None</span>
        <span class="token keyword">else</span> args<span class="token punctuation">.</span>num_embeddings<span class="token punctuation">,</span>
        feature_names<span class="token operator">=</span><span class="token punctuation">[</span>feature_name<span class="token punctuation">]</span><span class="token punctuation">,</span>
    <span class="token punctuation">)</span>
    <span class="token keyword">for</span> feature_idx<span class="token punctuation">,</span> feature_name <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>DEFAULT_CAT_NAMES<span class="token punctuation">)</span>
<span class="token punctuation">]</span>
sharded_module_kwargs <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>

dlrm_model <span class="token operator">=</span> DLRM<span class="token punctuation">(</span>
    embedding_bag_collection<span class="token operator">=</span>EmbeddingBagCollection<span class="token punctuation">(</span>
        tables<span class="token operator">=</span>eb_configs<span class="token punctuation">,</span> device<span class="token operator">=</span>torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">&quot;meta&quot;</span><span class="token punctuation">)</span>
    <span class="token punctuation">)</span><span class="token punctuation">,</span>
    dense_in_features<span class="token operator">=</span><span class="token builtin">len</span><span class="token punctuation">(</span>DEFAULT_INT_NAMES<span class="token punctuation">)</span><span class="token punctuation">,</span>
    dense_arch_layer_sizes<span class="token operator">=</span>args<span class="token punctuation">.</span>dense_arch_layer_sizes<span class="token punctuation">,</span>
    over_arch_layer_sizes<span class="token operator">=</span>args<span class="token punctuation">.</span>over_arch_layer_sizes<span class="token punctuation">,</span>
    dense_device<span class="token operator">=</span>device<span class="token punctuation">,</span>
<span class="token punctuation">)</span>
train_model <span class="token operator">=</span> DLRMTrain<span class="token punctuation">(</span>dlrm_model<span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></li><li><p>æŒ‡å®šembedding_optimizerï¼Œ ä½¿ç”¨adagradæˆ–sgd</p><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code>embedding_optimizer <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>Adagrad <span class="token keyword">if</span> args<span class="token punctuation">.</span>adagrad <span class="token keyword">else</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>SGD
apply_optimizer_in_backward<span class="token punctuation">(</span>
        embedding_optimizer<span class="token punctuation">,</span>
        train_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>sparse_arch<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        optimizer_kwargs<span class="token punctuation">,</span>
    <span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></li><li><p>å®šä¹‰plannerï¼Œè·å–plan</p><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code>planner <span class="token operator">=</span> EmbeddingShardingPlanner<span class="token punctuation">(</span>
    topology<span class="token operator">=</span>Topology<span class="token punctuation">(</span>
        local_world_size<span class="token operator">=</span>get_local_size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        world_size<span class="token operator">=</span>dist<span class="token punctuation">.</span>get_world_size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        compute_device<span class="token operator">=</span>device<span class="token punctuation">.</span><span class="token builtin">type</span><span class="token punctuation">,</span>
    <span class="token punctuation">)</span><span class="token punctuation">,</span>
    batch_size<span class="token operator">=</span>args<span class="token punctuation">.</span>batch_size<span class="token punctuation">,</span>
    <span class="token comment"># If experience OOM, increase the percentage. see</span>
    <span class="token comment"># https://pytorch.org/torchrec/torchrec.distributed.planner.html#torchrec.distributed.planner.storage_reservations.HeuristicalStorageReservation</span>
    storage_reservation<span class="token operator">=</span>HeuristicalStorageReservation<span class="token punctuation">(</span>percentage<span class="token operator">=</span><span class="token number">0.05</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
<span class="token punctuation">)</span>
plan <span class="token operator">=</span> planner<span class="token punctuation">.</span>collective_plan<span class="token punctuation">(</span>
    train_model<span class="token punctuation">,</span> get_default_sharders<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> dist<span class="token punctuation">.</span>GroupMember<span class="token punctuation">.</span>WORLD
<span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></li><li><p>å°†æ¨¡å‹ç»“æ„ã€è®¾å¤‡ã€planä¼ å…¥DMPå°è£…ä¸ºmodel</p><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code>model <span class="token operator">=</span> DistributedModelParallel<span class="token punctuation">(</span>
        module<span class="token operator">=</span>train_model<span class="token punctuation">,</span>
        device<span class="token operator">=</span>device<span class="token punctuation">,</span>
        plan<span class="token operator">=</span>plan<span class="token punctuation">,</span>
    <span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></li><li><p>æŒ‡å®šdense_optimizeråï¼Œå†ä¸embedding_optimizeråˆå¹¶ä¸ºæœ€ç»ˆoptimizer</p><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code>dense_optimizer <span class="token operator">=</span> KeyedOptimizerWrapper<span class="token punctuation">(</span>
    <span class="token builtin">dict</span><span class="token punctuation">(</span>in_backward_optimizer_filter<span class="token punctuation">(</span>model<span class="token punctuation">.</span>named_parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    optimizer_with_params<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
<span class="token punctuation">)</span>
optimizer <span class="token operator">=</span> CombinedOptimizer<span class="token punctuation">(</span><span class="token punctuation">[</span>model<span class="token punctuation">.</span>fused_optimizer<span class="token punctuation">,</span> dense_optimizer<span class="token punctuation">]</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></li><li><p>å®šä¹‰lr_scheduler</p><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code>lr_scheduler <span class="token operator">=</span> LRPolicyScheduler<span class="token punctuation">(</span>
        optimizer<span class="token punctuation">,</span> args<span class="token punctuation">.</span>lr_warmup_steps<span class="token punctuation">,</span> args<span class="token punctuation">.</span>lr_decay_start<span class="token punctuation">,</span> args<span class="token punctuation">.</span>lr_decay_steps
    <span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></li><li><p>è®­ç»ƒ</p><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code><span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>args<span class="token punctuation">.</span>epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>
    _train<span class="token punctuation">(</span>
        pipeline<span class="token punctuation">,</span>
        train_dataloader<span class="token punctuation">,</span>
        val_dataloader<span class="token punctuation">,</span>
        epoch<span class="token punctuation">,</span>
        lr_scheduler<span class="token punctuation">,</span>
        args<span class="token punctuation">.</span>print_lr<span class="token punctuation">,</span>
        args<span class="token punctuation">.</span>validation_freq_within_epoch<span class="token punctuation">,</span>
        args<span class="token punctuation">.</span>limit_train_batches<span class="token punctuation">,</span>
        args<span class="token punctuation">.</span>limit_val_batches<span class="token punctuation">,</span>
    <span class="token punctuation">)</span>
    val_auroc <span class="token operator">=</span> _evaluate<span class="token punctuation">(</span>args<span class="token punctuation">.</span>limit_val_batches<span class="token punctuation">,</span> pipeline<span class="token punctuation">,</span> val_dataloader<span class="token punctuation">,</span> <span class="token string">&quot;val&quot;</span><span class="token punctuation">)</span>
    results<span class="token punctuation">.</span>val_aurocs<span class="token punctuation">.</span>append<span class="token punctuation">(</span>val_auroc<span class="token punctuation">)</span>

test_auroc <span class="token operator">=</span> _evaluate<span class="token punctuation">(</span>args<span class="token punctuation">.</span>limit_test_batches<span class="token punctuation">,</span> pipeline<span class="token punctuation">,</span> test_dataloader<span class="token punctuation">,</span> <span class="token string">&quot;test&quot;</span><span class="token punctuation">)</span>
results<span class="token punctuation">.</span>test_auroc <span class="token operator">=</span> test_auroc
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ol><li><p>æŒ‰batchè¯»å–æ•°æ®è¿›è¡Œè®­ç»ƒ</p><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code><span class="token keyword">def</span> <span class="token function">_train</span><span class="token punctuation">(</span>
    pipeline<span class="token punctuation">:</span> TrainPipelineSparseDist<span class="token punctuation">,</span>
    train_dataloader<span class="token punctuation">:</span> DataLoader<span class="token punctuation">,</span>
    val_dataloader<span class="token punctuation">:</span> DataLoader<span class="token punctuation">,</span>
    epoch<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">,</span>
    lr_scheduler<span class="token punctuation">,</span>
    print_lr<span class="token punctuation">:</span> <span class="token builtin">bool</span><span class="token punctuation">,</span>
    validation_freq<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span><span class="token builtin">int</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    limit_train_batches<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span><span class="token builtin">int</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    limit_val_batches<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span><span class="token builtin">int</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">&gt;</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">&quot;&quot;&quot;
    Trains model for 1 epoch. Helper function for train_val_test.

    Args:
        pipeline (TrainPipelineSparseDist): data pipeline.
        train_dataloader (DataLoader): Training set&#39;s dataloader.
        val_dataloader (DataLoader): Validation set&#39;s dataloader.
        epoch (int): The number of complete passes through the training set so far.
        lr_scheduler (LRPolicyScheduler): Learning rate scheduler.
        print_lr (bool): Whether to print the learning rate every training step.
        validation_freq (Optional[int]): The number of training steps between validation runs within an epoch.
        limit_train_batches (Optional[int]): Limits the training set to the first `limit_train_batches` batches.
        limit_val_batches (Optional[int]): Limits the validation set to the first `limit_val_batches` batches.

    Returns:
        None.
    &quot;&quot;&quot;</span>
    pipeline<span class="token punctuation">.</span>_model<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span>

    iterator <span class="token operator">=</span> itertools<span class="token punctuation">.</span>islice<span class="token punctuation">(</span><span class="token builtin">iter</span><span class="token punctuation">(</span>train_dataloader<span class="token punctuation">)</span><span class="token punctuation">,</span> limit_train_batches<span class="token punctuation">)</span>

    is_rank_zero <span class="token operator">=</span> dist<span class="token punctuation">.</span>get_rank<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">0</span>
    <span class="token keyword">if</span> is_rank_zero<span class="token punctuation">:</span>
        pbar <span class="token operator">=</span> tqdm<span class="token punctuation">(</span>
            <span class="token builtin">iter</span><span class="token punctuation">(</span><span class="token builtin">int</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            desc<span class="token operator">=</span><span class="token string-interpolation"><span class="token string">f&quot;Epoch </span><span class="token interpolation"><span class="token punctuation">{</span>epoch<span class="token punctuation">}</span></span><span class="token string">&quot;</span></span><span class="token punctuation">,</span>
            total<span class="token operator">=</span><span class="token builtin">len</span><span class="token punctuation">(</span>train_dataloader<span class="token punctuation">)</span><span class="token punctuation">,</span>
            disable<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>
        <span class="token punctuation">)</span>

    start_it <span class="token operator">=</span> <span class="token number">0</span>
    n <span class="token operator">=</span> <span class="token punctuation">(</span>
        validation_freq
        <span class="token keyword">if</span> validation_freq
        <span class="token keyword">else</span> limit_train_batches
        <span class="token keyword">if</span> limit_train_batches
        <span class="token keyword">else</span> <span class="token builtin">len</span><span class="token punctuation">(</span>train_dataloader<span class="token punctuation">)</span>
    <span class="token punctuation">)</span>
    <span class="token keyword">for</span> batched_iterator <span class="token keyword">in</span> batched<span class="token punctuation">(</span>iterator<span class="token punctuation">,</span> n<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">for</span> it <span class="token keyword">in</span> itertools<span class="token punctuation">.</span>count<span class="token punctuation">(</span>start_it<span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token keyword">try</span><span class="token punctuation">:</span>
                <span class="token keyword">if</span> is_rank_zero <span class="token keyword">and</span> print_lr<span class="token punctuation">:</span>
                    <span class="token keyword">for</span> i<span class="token punctuation">,</span> g <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>pipeline<span class="token punctuation">.</span>_optimizer<span class="token punctuation">.</span>param_groups<span class="token punctuation">)</span><span class="token punctuation">:</span>
                        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f&quot;lr: </span><span class="token interpolation"><span class="token punctuation">{</span>it<span class="token punctuation">}</span></span><span class="token string"> </span><span class="token interpolation"><span class="token punctuation">{</span>i<span class="token punctuation">}</span></span><span class="token string"> </span><span class="token interpolation"><span class="token punctuation">{</span>g<span class="token punctuation">[</span><span class="token string">&#39;lr&#39;</span><span class="token punctuation">]</span><span class="token punctuation">:</span><span class="token format-spec">.6f</span><span class="token punctuation">}</span></span><span class="token string">&quot;</span></span><span class="token punctuation">)</span>
                pipeline<span class="token punctuation">.</span>progress<span class="token punctuation">(</span>batched_iterator<span class="token punctuation">)</span>
                lr_scheduler<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
                <span class="token keyword">if</span> is_rank_zero<span class="token punctuation">:</span>
                    pbar<span class="token punctuation">.</span>update<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>
            <span class="token keyword">except</span> StopIteration<span class="token punctuation">:</span>
                <span class="token keyword">if</span> is_rank_zero<span class="token punctuation">:</span>
                    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;Total number of iterations:&quot;</span><span class="token punctuation">,</span> it<span class="token punctuation">)</span>
                start_it <span class="token operator">=</span> it
                <span class="token keyword">break</span>

        <span class="token keyword">if</span> validation_freq <span class="token keyword">and</span> start_it <span class="token operator">%</span> validation_freq <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>
            _evaluate<span class="token punctuation">(</span>limit_val_batches<span class="token punctuation">,</span> pipeline<span class="token punctuation">,</span> val_dataloader<span class="token punctuation">,</span> <span class="token string">&quot;val&quot;</span><span class="token punctuation">)</span>
            pipeline<span class="token punctuation">.</span>_model<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></li></ol></li></ol></li></ol><h2 id="_0-2-è¿ç§»è®¡åˆ’" tabindex="-1"><a class="header-anchor" href="#_0-2-è¿ç§»è®¡åˆ’"><span>0.2 è¿ç§»è®¡åˆ’</span></a></h2><ol><li>å°†collectionæ•´åˆè¿›æ¡†æ¶ä¸­embç›¸å…³éƒ¨åˆ† <ol><li>torchrecçš„dynamic embå¹¶éæ˜¯çœŸçš„åŠ¨æ€è¡¨ï¼Œåªæ˜¯å®ç°äº†gpu cacheç”¨äºå­˜å‚¨æ›´å¤§çš„embï¼Œå¹¶ä¸èƒ½åŠ¨æ€æ‰©å¢gpuå†…è¡¨å¤§å°</li></ol></li><li>æœªæ”¯æŒä¾èµ–åº“è§£å†³æ–¹æ³• <ol><li>è‹¥pytorchç»„èƒ½æä¾›ä¸´æ—¶ç‰ˆæœ¬æ”¯æŒå°±æœ€å¥½äº†</li><li>ç›®å‰æƒ³åˆ°çš„æ–¹å¼æ˜¯å…ˆå°†é«˜ç‰ˆæœ¬pytorchä¸­çš„å¯¹åº”ä¾èµ–æ¨¡å—è§£è€¦å‡ºæ¥æ”¾åˆ°ç°åœ¨çš„pytorch1.9ç‰ˆæœ¬ä¸­é‡æ–°æºç å®‰è£…</li><li>æˆ–æ˜¯ç°å®ç°cpuç‰ˆæœ¬</li></ol></li><li>è¿ç§»åDLRMç½‘ç»œæ€§èƒ½æµ‹è¯•ã€çƒ­ç‚¹åˆ†æä»¥åŠè°ƒä¼˜</li></ol><h2 id="_1-ä»‹ç»" tabindex="-1"><a class="header-anchor" href="#_1-ä»‹ç»"><span>1. ä»‹ç»</span></a></h2><ul><li><p>ç›¸å…³é¡¹ç›®ï¼š</p><ul><li>https://github.com/pytorch/torchrec</li><li>https://github.com/facebookresearch/dlrm/tree/main/torchrec_dlrm</li></ul></li><li><p>TorchRec æ˜¯PyTorchä¸‹å¤§è§„æ¨¡æ¨èç³»ç»Ÿ (RecSys) è®­ç»ƒæ¡†æ¶ï¼Œèƒ½æä¾›æ¨èç³»ç»Ÿæ‰€éœ€çš„é€šç”¨ç¨€ç–æ€§å’Œå¹¶è¡Œæ€§åŸè¯­ï¼Œå…è®¸ä½¿ç”¨è·¨å¤šä¸ª GPU åˆ†ç‰‡çš„å¤§å‹åµŒå…¥è¡¨æ¥è®­ç»ƒæ¨¡å‹</p><ul><li>æ”¯æŒæ··åˆæ•°æ®å¹¶è¡Œ/æ¨¡å‹å¹¶è¡Œï¼Œå¤šè®¾å¤‡/å¤šèŠ‚ç‚¹è®­ç»ƒ</li><li>https://nvidia-merlin.github.io/HugeCTR/main/hugectr_embedding_training_cache.htmlhttps://nvidia-merlin.github.io/HugeCTR/main/hugectr_embedding_training_cache.htmlhttps://nvidia-merlin.github.io/HugeCTR/main/hugectr_embedding_training_cache.htmlhttps://nvidia-merlin.github.io/HugeCTR/main/hugectr_embedding_training_cache.htmlhttps://nvidia-merlin.github.io/HugeCTR/main/hugectr_embedding_training_cache.htmlhttps://nvidia-merlin.github.io/HugeCTR/main/hugectr_embedding_training_cache.htmlhttps://nvidia-merlin.github.io/HugeCTR/main/hugectr_embedding_training_cache.htmlhttps://nvidia-merlin.github.io/HugeCTR/main/hugectr_embedding_training_cache.htmlå¯ä»¥ä½¿ç”¨ä¸åŒçš„åˆ†ç‰‡ç­–ç•¥å¯¹åµŒå…¥è¡¨è¿›è¡Œåˆ†ç‰‡ï¼ŒåŒ…æ‹¬data-parallel, table-wise, row-wise, table-wise-row-wise, å’Œ column-wise sharding</li><li>TorchRec planner å¯ä»¥è‡ªåŠ¨ä¸ºæ¨¡å‹ç”Ÿæˆä¼˜åŒ–çš„åˆ†ç‰‡è®¡åˆ’</li><li>æ”¯æŒæ•°æ®åŠ è½½ï¼ˆå¤åˆ¶åˆ° GPUï¼‰ã€è®¾å¤‡é—´é€šä¿¡å’Œè®¡ç®—ï¼ˆå‰å‘ã€åå‘ï¼‰é‡å çš„æµæ°´çº¿ï¼Œä»¥æé«˜æ€§èƒ½</li><li>ç”± FBGEMM æä¾›å¯¹RecSysçš„ä¼˜åŒ–kernelï¼ŒFBGEMMï¼ˆFacebook é€šç”¨çŸ©é˜µä¹˜æ³•ï¼‰æ˜¯ç”¨äºæœåŠ¡å™¨ç«¯æ¨ç†çš„ä½ç²¾åº¦ã€é«˜æ€§èƒ½çŸ©é˜µ-çŸ©é˜µä¹˜æ³•å’Œå·ç§¯åº“</li><li>æ”¯æŒé™ä½ç²¾åº¦çš„è®­ç»ƒæ¨ç†é‡åŒ–</li></ul></li><li><p>ç¯å¢ƒ</p><div class="language-text line-numbers-mode" data-ext="text" data-title="text"><pre class="language-text"><code>Python &gt;= 3.7
CUDA &gt;= 11.0
nvcr.io/nvidia/pytorch:23.02-py3
nvcr.io/nvidia/pytorch:22.08-py3
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></li><li><p>ä¾èµ–åº“</p><div class="language-txt line-numbers-mode" data-ext="txt" data-title="txt"><pre class="language-txt"><code>black
cmake
fbgemm-gpu-nightly
hypothesis
iopath
numpy
pandas
pyre-extensions
scikit-build
tabulate
torchmetrics
torchx
tqdm
usort
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></li></ul><h3 id="_1-1-å®‰è£…-æµ‹è¯•" tabindex="-1"><a class="header-anchor" href="#_1-1-å®‰è£…-æµ‹è¯•"><span>1.1 å®‰è£…&amp;æµ‹è¯•</span></a></h3><ul><li><div class="language-bash line-numbers-mode" data-ext="sh" data-title="sh"><pre class="language-bash"><code> <span class="token comment"># å»ºè®®ä½¿ç”¨pipå®‰è£…</span>
 pip <span class="token function">install</span> torchrec_nightly --force-reinstall
 
 <span class="token comment"># ä½¿ç”¨æºç å®‰è£…ä¼šå‡ºç°fbgemm-gpuç‰ˆæœ¬é—®é¢˜</span>
 pip <span class="token function">install</span> <span class="token parameter variable">-r</span> requirements.txt
 python setup.py <span class="token function">install</span> develop
 python setup.py --cpu-only <span class="token function">install</span> develop
 
 <span class="token comment"># å®‰è£…åæµ‹è¯•</span>
 <span class="token comment"># torchx run -s local_cwd dist.ddp -j 1x2 --gpu 2 --script test_installation.py</span>
 
 <span class="token comment"># torchrun --nnodes 1 --nproc_per_node 2 --rdzv_backend c10d --rdzv_endpoint localhost --rdzv_id 54321 --role trainer test_installation.py</span>
 
 python <span class="token parameter variable">-m</span> torch.distributed.run <span class="token parameter variable">--nnodes</span> <span class="token number">1</span> <span class="token parameter variable">--nproc_per_node</span> <span class="token number">2</span> <span class="token parameter variable">--rdzv_backend</span> c10d <span class="token parameter variable">--rdzv_endpoint</span> localhost <span class="token parameter variable">--rdzv_id</span> <span class="token number">54321</span> <span class="token parameter variable">--role</span> trainer test_installation.py <span class="token parameter variable">--cpu_only</span>
 
 python <span class="token parameter variable">-m</span> torch.distributed.launch <span class="token punctuation">\</span>
     <span class="token parameter variable">--nproc_per_node</span> <span class="token number">2</span> <span class="token punctuation">\</span>
     <span class="token parameter variable">--nnodes</span> <span class="token number">1</span> <span class="token punctuation">\</span>
     <span class="token parameter variable">--node_rank</span> <span class="token number">0</span><span class="token punctuation">\</span>
     <span class="token parameter variable">--master_addr</span> localhost <span class="token punctuation">\</span>
     <span class="token parameter variable">--master_port</span> <span class="token number">54321</span><span class="token punctuation">\</span>
     --use-env test_installation.py
 
 <span class="token comment">#dlrm</span>
 python <span class="token parameter variable">-m</span> torch.distributed.run <span class="token parameter variable">--nnodes</span> <span class="token number">1</span> <span class="token parameter variable">--nproc_per_node</span> <span class="token number">2</span> <span class="token parameter variable">--rdzv_backend</span> c10d <span class="token parameter variable">--rdzv_endpoint</span> localhost <span class="token parameter variable">--rdzv_id</span> <span class="token number">54321</span> <span class="token parameter variable">--role</span> trainer dlrm_main.py
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></li><li><div class="language-bash line-numbers-mode" data-ext="sh" data-title="sh"><pre class="language-bash"><code> <span class="token comment"># pytorchå®‰è£…ä½ç½®:</span>
 /torch/venv3/pytorch/lib/python3.7/site-packages/torch
 <span class="token comment"># fbgemmå®‰è£…ä½ç½®:</span>
 /torch/venv3/pytorch/lib/python3.7/site-packages/fbgemm_gpu
 
 cmake  <span class="token parameter variable">-DCMAKE_PREFIX_PATH</span><span class="token operator">=</span>/torch/venv3/pytorch/lib/python3.7/site-packages/torch <span class="token punctuation">..</span> <span class="token operator">&amp;&amp;</span> <span class="token function">make</span> <span class="token parameter variable">-j</span>
 
 cmake  <span class="token parameter variable">-DCMAKE_PREFIX_PATH</span><span class="token operator">=</span>/opt/conda/lib/python3.8/site-packages/torch <span class="token punctuation">..</span> <span class="token operator">&amp;&amp;</span> <span class="token function">make</span> <span class="token parameter variable">-j</span>
 
 <span class="token comment"># fbgemmæºç å®‰è£…</span>
 <span class="token comment">#cmake -DUSE_SANITIZER=address -DFBGEMM_LIBRARY_TYPE=shared -DPYTHON_EXECUTABLE=/torch/venv3/pytorch/bin/python3 ..</span>
 
 <span class="token comment">#cmake -DUSE_SANITIZER=address -DFBGEMM_LIBRARY_TYPE=shared -DPYTHON_EXECUTABLE=/opt/conda/bin/python3 ..</span>
 
 <span class="token comment">#make -j VERBOSE=1    </span>
 
 <span class="token comment"># fbgemm-gpuæºç å®‰è£…:</span>
 <span class="token comment"># å®‰è£…conda</span>
 <span class="token assign-left variable">miniconda_prefix</span><span class="token operator">=</span><span class="token environment constant">$HOME</span>/miniconda
 <span class="token function">bash</span> miniconda.sh <span class="token parameter variable">-b</span> <span class="token parameter variable">-p</span> <span class="token string">&quot;<span class="token variable">$miniconda_prefix</span>&quot;</span> <span class="token parameter variable">-u</span>
 <span class="token builtin class-name">export</span> <span class="token assign-left variable"><span class="token environment constant">PATH</span></span><span class="token operator">=</span><span class="token variable">$miniconda_prefix</span>/bin:<span class="token environment constant">$PATH</span>
 conda update <span class="token parameter variable">-n</span> base <span class="token parameter variable">-c</span> defaults <span class="token parameter variable">-y</span> conda
 
 <span class="token assign-left variable">env_name</span><span class="token operator">=</span>fbgemm-install
 conda create <span class="token parameter variable">-y</span> <span class="token parameter variable">--name</span> <span class="token string">&quot;<span class="token variable">${env_name}</span>&quot;</span> <span class="token assign-left variable">python</span><span class="token operator">=</span><span class="token string">&quot;3.7&quot;</span>
 <span class="token builtin class-name">source</span> /root/miniconda/etc/profile.d/conda.sh
 
 <span class="token comment"># source /opt/conda/etc/profile.d/conda.sh</span>
 <span class="token comment"># conda activate fbgemm_install</span>
 conda run <span class="token parameter variable">-n</span> <span class="token string">&quot;<span class="token variable">${env_name}</span>&quot;</span> pip <span class="token function">install</span> <span class="token parameter variable">--upgrade</span> pip
 conda run <span class="token parameter variable">-n</span> <span class="token string">&quot;<span class="token variable">${env_name}</span>&quot;</span> python <span class="token parameter variable">-m</span> pip <span class="token function">install</span> pyOpenSSL<span class="token operator">&gt;</span><span class="token number">22.1</span>.0
 conda <span class="token function">install</span> <span class="token parameter variable">-n</span> <span class="token string">&quot;<span class="token variable">${env_name}</span>&quot;</span> <span class="token parameter variable">-y</span> gxx_linux-64<span class="token operator">=</span><span class="token number">10.4</span>.0 sysroot_linux-64<span class="token operator">=</span><span class="token number">2.17</span> <span class="token parameter variable">-c</span> conda-forge
 
 conda activate fbgemm_install
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></li></ul><h2 id="_2-data-preprocess" tabindex="-1"><a class="header-anchor" href="#_2-data-preprocess"><span>2. Data Preprocess</span></a></h2><ol><li><p>criteo-kaggle (7.8GB):</p><ul><li><p>ä¸‹è½½ä¸è§£å‹æ•°æ®é›†</p><div class="language-bash line-numbers-mode" data-ext="sh" data-title="sh"><pre class="language-bash"><code><span class="token function">wget</span> http://go.criteo.net/criteo-research-kaggle-display-advertising-challenge-dataset.tar.gz <span class="token operator">&amp;&amp;</span> /
<span class="token function">tar</span> zxvf criteo-research-kaggle-display-advertising-challenge-dataset.tar.gz
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div></li><li><p>é¢„å¤„ç†æ•°æ®é›†ï¼ˆéœ€è¦70GBå†…å­˜ï¼‰</p></li></ul><div class="language-bash line-numbers-mode" data-ext="sh" data-title="sh"><pre class="language-bash"><code>python <span class="token parameter variable">-m</span> torchrec.datasets.scripts.npy_preproc_criteo <span class="token parameter variable">--input_dir</span> <span class="token variable">$INPUT_PATH</span> <span class="token parameter variable">--output_dir</span> <span class="token variable">$OUTPUT_PATH</span> <span class="token parameter variable">--dataset_name</span> criteo_kaggle
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div></li><li><p>criteo-1t (655GB):</p><ul><li><p>è„šæœ¬è§é¡¹ç›®dlrm https://github.com/facebookresearch/dlrm/blob/main/torchrec_dlrm/scripts/process_Criteo_1TB_Click_Logs_dataset.sh</p><div class="language-bash line-numbers-mode" data-ext="sh" data-title="sh"><pre class="language-bash"><code><span class="token function">git</span> clone <span class="token parameter variable">--recursive</span> https://github.com/facebookresearch/dlrm.git
 
<span class="token builtin class-name">cd</span> ./dlrm/torchrec_dlrm/scripts <span class="token operator">&amp;&amp;</span> <span class="token punctuation">\</span>
<span class="token function">bash</span> ./process_Criteo_1TB_Click_Logs_dataset.sh /workspace/dataset/favorite/modelzoo-datasets/v1/criteo_terybyte/ /workspace/volume/<span class="token punctuation">[</span>your-workspace<span class="token punctuation">]</span>/criteo_terabyte/intermediate /workspace/volume/<span class="token punctuation">[</span>your-workspace<span class="token punctuation">]</span>/criteo_terabyte/output
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></li><li><p>å¤„ç†æ–¹æ³•:</p><ol><li><p>å°†tsvæ–‡ä»¶è½¬ä¸ºnpyæ–‡ä»¶ï¼Œåˆ’åˆ†ä¸ºdenseï¼Œsparseï¼Œlabelä¸‰ä¸ªnpyæ–‡ä»¶ï¼ˆéœ€è¦320GBå†…å­˜ï¼‰</p><ul><li><p><a href="https://ailab.criteo.com/download-criteo-1tb-click-logs-dataset/" target="_blank" rel="noopener noreferrer">åŸå§‹æ•°æ®ç±»å‹<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>ï¼šæ¯æ¡æ•°æ®1ä¸ªlabelï¼ˆæ˜¯å¦è¢«ç‚¹å‡»ï¼‰ï¼Œ13ä¸ªdenseç‰¹å¾ï¼ˆintå‹ï¼Œå¤šä¸ºè®¡æ•°å€¼ï¼‰ï¼Œ26ä¸ªsparseç‰¹å¾ï¼ˆç»hashä¸º32bitsæ•°æ®ï¼‰</p></li><li><p>æ¯è¡Œæ•°æ®æ ¼å¼ï¼š [label] [integer feature 1] â€¦ [integer feature 13] [categorical feature 1] â€¦ [categorical feature 26]</p></li><li><p>torchrecå¤„ç†denseç‰¹å¾çš„æ–¹å¼ï¼Œå°†æ•´å‹denseç‰¹å¾è½¬ä¸ºå¤§äº1çš„float32å‹</p><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code><span class="token comment"># PyTorch tensors can&#39;t handle uint32, but we can save space by not using int64. Numpy will automatically handle dense values &gt;= 2 ** 31.</span>
dense_np <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>dense<span class="token punctuation">,</span> dtype<span class="token operator">=</span>np<span class="token punctuation">.</span>int32<span class="token punctuation">)</span>
sparse_np <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>sparse<span class="token punctuation">,</span> dtype<span class="token operator">=</span>np<span class="token punctuation">.</span>int32<span class="token punctuation">)</span>
labels_np <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>labels<span class="token punctuation">,</span> dtype<span class="token operator">=</span>np<span class="token punctuation">.</span>int32<span class="token punctuation">)</span>

<span class="token comment"># Log is expensive to compute at runtime.</span>
dense_np <span class="token operator">+=</span> <span class="token number">3</span>
dense_np <span class="token operator">=</span> np<span class="token punctuation">.</span>log<span class="token punctuation">(</span>dense_np<span class="token punctuation">,</span> dtype<span class="token operator">=</span>np<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>

<span class="token comment"># To be consistent with dense and sparse.</span>
labels_np <span class="token operator">=</span> labels_np<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></li></ul></li><li><p>å°†sparseç‰¹å¾å¤„ç†ä¸ºcontiguousç‰¹å¾ï¼ˆéœ€è¦480GBå†…å­˜ï¼‰</p><ul><li><p>éœ€è¦æ‰€æœ‰24å¤©çš„æ•°æ®åŒæ—¶è¾“å…¥è„šæœ¬å¤„ç†</p></li><li><p>åœ¨æ‰€æœ‰æ–‡ä»¶ä¸­åˆ†åˆ«ç»Ÿè®¡æ¯ä¸€ä¸ªç‰¹å¾åŸŸå‡ºç°è¿‡çš„ç‰¹å¾çš„å‡ºç°é¢‘ç‡åˆ°sparse_to_frequencyä¸­</p></li><li><p>å°†å‡ºç°é¢‘ç‡ä½äºfrequency_thresholdçš„ç‰¹å¾å€¼éƒ½æ˜ å°„ä¸º1ï¼Œå…¶ä½™ç‰¹å¾å€¼æ˜ å°„ä¸ºä»2å¼€å§‹çš„è¿ç»­å€¼ï¼ˆå¯èƒ½å‡ºç°é¢‘ç‡ä½çš„ç‰¹å¾å¯¹æœ€ålabelçš„å½±å“å°ï¼Œæ‰€ä»¥ç»Ÿä¸€å¤„ç†ä¸º1ï¼Œèƒ½å¤Ÿå‡å°ç‰¹å¾åŸŸå¤§å°ï¼Ÿï¼‰</p><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code><span class="token comment"># Iterate through each row in each file for the current column and remap each</span>
<span class="token comment"># sparse id to a contiguous id. The contiguous ints start at a value of 2 so that</span>
<span class="token comment"># infrequenct IDs (determined by the frequency_threshold) can be remapped to 1.</span>
running_sum <span class="token operator">=</span> <span class="token number">2</span>
sparse_to_contiguous_int<span class="token punctuation">:</span> Dict<span class="token punctuation">[</span><span class="token builtin">int</span><span class="token punctuation">,</span> <span class="token builtin">int</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>

<span class="token keyword">for</span> f <span class="token keyword">in</span> file_to_features<span class="token punctuation">:</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f&quot;Processing file: </span><span class="token interpolation"><span class="token punctuation">{</span>f<span class="token punctuation">}</span></span><span class="token string">&quot;</span></span><span class="token punctuation">)</span>

    <span class="token keyword">for</span> i<span class="token punctuation">,</span> sparse <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>file_to_features<span class="token punctuation">[</span>f<span class="token punctuation">]</span><span class="token punctuation">[</span>col<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">if</span> sparse <span class="token keyword">not</span> <span class="token keyword">in</span> sparse_to_contiguous_int<span class="token punctuation">:</span>
            <span class="token comment"># If the ID appears less than frequency_threshold amount of times</span>
            <span class="token comment"># remap the value to 1.</span>
            <span class="token keyword">if</span> <span class="token punctuation">(</span>
                frequency_threshold <span class="token operator">&gt;</span> <span class="token number">1</span>
                <span class="token keyword">and</span> sparse_to_frequency<span class="token punctuation">[</span>sparse<span class="token punctuation">]</span> <span class="token operator">&lt;</span> frequency_threshold
            <span class="token punctuation">)</span><span class="token punctuation">:</span>
                sparse_to_contiguous_int<span class="token punctuation">[</span>sparse<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">1</span>
            <span class="token keyword">else</span><span class="token punctuation">:</span>
                sparse_to_contiguous_int<span class="token punctuation">[</span>sparse<span class="token punctuation">]</span> <span class="token operator">=</span> running_sum
                running_sum <span class="token operator">+=</span> <span class="token number">1</span>

        <span class="token comment"># Re-map sparse value to contiguous in place.</span>
        file_to_features<span class="token punctuation">[</span>f<span class="token punctuation">]</span><span class="token punctuation">[</span>col<span class="token punctuation">]</span><span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> sparse_to_contiguous_int<span class="token punctuation">[</span>sparse<span class="token punctuation">]</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></li></ul></li><li><p>shuffleï¼ˆéœ€è¦700GBå†…å­˜ï¼‰</p><ul><li>day0-day22åšè®­ç»ƒé›†ï¼Œshuffle</li><li>day23åšéªŒè¯é›†ï¼Œä¸åšshuffle</li></ul></li></ol></li></ul></li><li><p>criteo-multihot (3.5TB):</p><ul><li><p>åˆ©ç”¨ä¹‹å‰å¤„ç†å¥½çš„criteo-1tæ•°æ®é›†, åˆæˆmulti-hotæ•°æ®é›†, ç”¨äºMLPerf DLRM v2 benchmark (éœ€è¦200GBå†…å­˜)</p><div class="language-bash line-numbers-mode" data-ext="sh" data-title="sh"><pre class="language-bash"><code>python materialize_synthetic_multihot_dataset.py <span class="token punctuation">\</span>
    <span class="token parameter variable">--in_memory_binary_criteo_path</span> <span class="token variable">$PREPROCESSED_CRITEO_1TB_CLICK_LOGS_DATASET_PATH</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--output_path</span> <span class="token variable">$MATERIALIZED_DATASET_PATH</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--num_embeddings_per_feature</span> <span class="token number">40000000,39060</span>,17295,7424,20265,3,7122,1543,63,40000000,3067956,405282,10,2209,11938,155,4,976,14,40000000,40000000,40000000,590152,12973,108,36 <span class="token punctuation">\</span>
    <span class="token parameter variable">--multi_hot_sizes</span> <span class="token number">3,2</span>,1,2,6,1,1,1,1,7,3,8,1,6,9,5,1,1,1,12,100,27,10,3,1,1 <span class="token punctuation">\</span>
    <span class="token parameter variable">--multi_hot_distribution_type</span> uniform
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></li></ul></li></ol><h2 id="_3-torchrec-benchmarks" tabindex="-1"><a class="header-anchor" href="#_3-torchrec-benchmarks"><span>3. Torchrec Benchmarks</span></a></h2><ul><li><p>benchmarkå¯¹ä¸¤ç§EmbeddingBagCollectionæ¨¡å‹è¿›è¡Œæ¯”è¾ƒ</p><ul><li><code>EmbeddingBagCollection</code> (EBC) (<a href="https://pytorch.org/torchrec/torchrec.modules.html#torchrec.modules.embedding_modules.EmbeddingBagCollection" target="_blank" rel="noopener noreferrer">code<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>): ç”± <a href="https://pytorch.org/docs/stable/generated/torch.nn.EmbeddingBag.html" target="_blank" rel="noopener noreferrer">torch.nn.EmbeddingBag<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>æ”¯æŒ</li><li><code>FusedEmbeddingBagCollection</code> (Fused EBC) (<a href="https://github.com/pytorch/torchrec/blob/main/torchrec/modules/fused_embedding_bag_collection.py#L299" target="_blank" rel="noopener noreferrer">code<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>): ç”±<a href="https://github.com/pytorch/FBGEMM" target="_blank" rel="noopener noreferrer">FBGEMM<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> kernels æ”¯æŒï¼Œé…å¤‡äº†èåˆä¼˜åŒ–å™¨å’Œ UVM æˆ–UVM Cachingï¼Œå¯ä¸º GPU æä¾›æ›´å¤§çš„å†…å­˜ <ul><li>UVMï¼š <ul><li>èƒ½å¤Ÿè¢«CPUæˆ–GPUè®¿é—®çš„hostå†…å­˜åœ°å€ç©ºé—´ï¼Œä½¿ç”¨cudaMalloManaged()åˆ†é…å†…å­˜</li><li>ä½¿ç”¨UVMèƒ½å¤Ÿåˆ†é…è¶…è¿‡æ˜¾å­˜å¤§å°çš„æ›´å¤šå†…å­˜ï¼Œå­˜ä¸‹æ›´å¤§çš„åµŒå…¥è¡¨</li><li>ä»¥pageä¸ºç²’åº¦è·å–Embedding Table</li></ul></li><li>UVM cachingï¼š <ul><li>ä»¥Embedding rowä¸ºç²’åº¦è·å–embedding</li><li>ä½¿ç”¨software managed cacheç®¡ç†ï¼Œå¦‚æœGPU missï¼Œåˆ™ä»å†…å­˜ä¸­è°ƒç”¨è¿™ä¸ªrowåˆ°GPUæ˜¾å­˜ä¸­</li><li>å¯è®¾ç½®caching ratioç®¡ç†ç¼“å­˜å¤§å°å æ•´ä¸ªEmbedding Tableçš„æ¯”ä¾‹</li></ul></li></ul></li></ul></li><li><p>run</p><div class="language-bash line-numbers-mode" data-ext="sh" data-title="sh"><pre class="language-bash"><code><span class="token builtin class-name">cd</span> benchmarks
<span class="token comment"># modify ebc_benchmarks.py line14: from torchrec.github.benchmarks import ebc_benchmarks_utils -&gt; import ebc_benchmarks_utils</span>
<span class="token comment"># mode: ebc_comparison_dlrm (default) / fused_ebc_uvm / ebc_comparison_scaling</span>
python ebc_benchmarks.py <span class="token punctuation">[</span>--mode MODE<span class="token punctuation">]</span> <span class="token punctuation">[</span>--cpu_only<span class="token punctuation">]</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></li><li><p>ç»“è®ºï¼š</p><ul><li>ä½¿ç”¨äº†UVM æˆ–UVM cachingçš„FusedEBCç›¸æ¯”EBCæ¨¡å‹å…·æœ‰æ›´å¿«çš„æ€§èƒ½è¡¨ç°ï¼Œä¸”FusedEBCæ”¯æŒè¶…è¿‡æ˜¾å­˜å¤§å°çš„Embedding<img src="/assets/EBC_benchmarks_dlrm_emb-SuBKNH8_.png" alt="" loading="lazy"></li></ul></li></ul><h2 id="_4-dlrm-benchmarks" tabindex="-1"><a class="header-anchor" href="#_4-dlrm-benchmarks"><span>4. DLRM Benchmarks</span></a></h2><ul><li><p>MLPerf DLRM v1 benchmark</p><ul><li><p>ä½¿ç”¨DLRMé¡¹ç›®ä¸‹çš„torchrec_dlrm</p><div class="language-bash line-numbers-mode" data-ext="sh" data-title="sh"><pre class="language-bash"><code><span class="token function">git</span> clone <span class="token parameter variable">--recursive</span> https://github.com/facebookresearch/dlrm.git
<span class="token builtin class-name">cd</span> dlrm/torchrec_dlrm
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div></li><li><p>é¢„å¤„ç†æ•°æ®ï¼Œå¤„ç†æ–¹æ³•è§3å°èŠ‚</p></li><li><p>run</p><div class="language-bash line-numbers-mode" data-ext="sh" data-title="sh"><pre class="language-bash"><code><span class="token builtin class-name">export</span> <span class="token assign-left variable">PREPROCESSED_DATASET</span><span class="token operator">=</span><span class="token variable">$insert_your_path_here</span>
<span class="token builtin class-name">export</span> <span class="token assign-left variable">TOTAL_TRAINING_SAMPLES</span><span class="token operator">=</span><span class="token number">4195197692</span> <span class="token punctuation">;</span>
<span class="token builtin class-name">export</span> <span class="token assign-left variable">GLOBAL_BATCH_SIZE</span><span class="token operator">=</span><span class="token number">16384</span> <span class="token punctuation">;</span>
<span class="token builtin class-name">export</span> <span class="token assign-left variable">WORLD_SIZE</span><span class="token operator">=</span><span class="token number">8</span> <span class="token punctuation">;</span>
torchx run <span class="token parameter variable">-s</span> local_cwd dist.ddp <span class="token parameter variable">-j</span> 1x8 <span class="token parameter variable">--script</span> dlrm_main.py -- <span class="token punctuation">\</span>
    <span class="token parameter variable">--embedding_dim</span> <span class="token number">128</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--dense_arch_layer_sizes</span> <span class="token number">512,256</span>,128 <span class="token punctuation">\</span>
    <span class="token parameter variable">--over_arch_layer_sizes</span> <span class="token number">1024,1024</span>,512,256,1 <span class="token punctuation">\</span>
    <span class="token parameter variable">--in_memory_binary_criteo_path</span> <span class="token variable">$PREPROCESSED_DATASET</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--num_embeddings_per_feature</span> <span class="token number">40000000,39060</span>,17295,7424,20265,3,7122,1543,63,40000000,3067956,405282,10,2209,11938,155,4,976,14,40000000,40000000,40000000,590152,12973,108,36 <span class="token punctuation">\</span>
    <span class="token parameter variable">--validation_freq_within_epoch</span> <span class="token variable"><span class="token variable">$((</span>TOTAL_TRAINING_SAMPLES <span class="token operator">/</span> <span class="token punctuation">(</span>GLOBAL_BATCH_SIZE <span class="token operator">*</span> <span class="token number">20</span><span class="token variable">))</span></span><span class="token punctuation">)</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--epochs</span> <span class="token number">1</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--pin_memory</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--mmap_mode</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--batch_size</span> <span class="token variable"><span class="token variable">$((</span>GLOBAL_BATCH_SIZE <span class="token operator">/</span> WORLD_SIZE<span class="token variable">))</span></span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--learning_rate</span> <span class="token number">1.0</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></li></ul></li><li></li><li><p>MLPerf DLRM v2 benchmark</p><ul><li><p>ä½¿ç”¨DLRMé¡¹ç›®ä¸‹çš„torchrec_dlrm</p><div class="language-bash line-numbers-mode" data-ext="sh" data-title="sh"><pre class="language-bash"><code><span class="token function">git</span> clone <span class="token parameter variable">--recursive</span> https://github.com/facebookresearch/dlrm.git
<span class="token builtin class-name">cd</span> dlrm/torchrec_dlrm
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div></li><li><p>é¢„å¤„ç†æ•°æ®ï¼Œå¤„ç†æ–¹æ³•è§3å°èŠ‚</p></li><li><p>runï¼ˆä½¿ç”¨åˆæˆmulti-hotæ•°æ®ï¼Œ3.8TBï¼‰</p><div class="language-bash line-numbers-mode" data-ext="sh" data-title="sh"><pre class="language-bash"><code><span class="token builtin class-name">export</span> <span class="token assign-left variable">MULTIHOT_PREPROCESSED_DATASET</span><span class="token operator">=</span><span class="token variable">$your_path_here</span>
<span class="token builtin class-name">export</span> <span class="token assign-left variable">TOTAL_TRAINING_SAMPLES</span><span class="token operator">=</span><span class="token number">4195197692</span> <span class="token punctuation">;</span>
<span class="token builtin class-name">export</span> <span class="token assign-left variable">GLOBAL_BATCH_SIZE</span><span class="token operator">=</span><span class="token number">65536</span> <span class="token punctuation">;</span>
<span class="token builtin class-name">export</span> <span class="token assign-left variable">WORLD_SIZE</span><span class="token operator">=</span><span class="token number">8</span> <span class="token punctuation">;</span>
torchx run <span class="token parameter variable">-s</span> local_cwd dist.ddp <span class="token parameter variable">-j</span> 1x8 <span class="token parameter variable">--script</span> dlrm_main.py -- <span class="token punctuation">\</span>
    <span class="token parameter variable">--embedding_dim</span> <span class="token number">128</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--dense_arch_layer_sizes</span> <span class="token number">512,256</span>,128 <span class="token punctuation">\</span>
    <span class="token parameter variable">--over_arch_layer_sizes</span> <span class="token number">1024,1024</span>,512,256,1 <span class="token punctuation">\</span>
    <span class="token parameter variable">--synthetic_multi_hot_criteo_path</span> <span class="token variable">$MULTIHOT_PREPROCESSED_DATASET</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--num_embeddings_per_feature</span> <span class="token number">40000000,39060</span>,17295,7424,20265,3,7122,1543,63,40000000,3067956,405282,10,2209,11938,155,4,976,14,40000000,40000000,40000000,590152,12973,108,36 <span class="token punctuation">\</span>
    <span class="token parameter variable">--validation_freq_within_epoch</span> <span class="token variable"><span class="token variable">$((</span>TOTAL_TRAINING_SAMPLES <span class="token operator">/</span> <span class="token punctuation">(</span>GLOBAL_BATCH_SIZE <span class="token operator">*</span> <span class="token number">20</span><span class="token variable">))</span></span><span class="token punctuation">)</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--epochs</span> <span class="token number">1</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--pin_memory</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--mmap_mode</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--batch_size</span> <span class="token variable"><span class="token variable">$((</span>GLOBAL_BATCH_SIZE <span class="token operator">/</span> WORLD_SIZE<span class="token variable">))</span></span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--interaction_type</span><span class="token operator">=</span>dcn <span class="token punctuation">\</span>
    <span class="token parameter variable">--dcn_num_layers</span><span class="token operator">=</span><span class="token number">3</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--dcn_low_rank_dim</span><span class="token operator">=</span><span class="token number">512</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--adagrad</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--learning_rate</span> <span class="token number">0.005</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></li><li><p>runï¼ˆä½¿ç”¨é¢„å¤„ç†åçš„criteo-1tæ•°æ®é›†åŠ¨æ€ç”Ÿæˆçš„multi-hotæ•°æ®ï¼Œå­˜å‚¨ç©ºé—´ä¸è¶³3.8TBæ—¶å¯ç”¨ï¼‰</p><div class="language-bash line-numbers-mode" data-ext="sh" data-title="sh"><pre class="language-bash"><code><span class="token builtin class-name">export</span> <span class="token assign-left variable">PREPROCESSED_DATASET</span><span class="token operator">=</span><span class="token variable">$insert_your_path_here</span>
<span class="token builtin class-name">export</span> <span class="token assign-left variable">TOTAL_TRAINING_SAMPLES</span><span class="token operator">=</span><span class="token number">4195197692</span> <span class="token punctuation">;</span>
<span class="token builtin class-name">export</span> <span class="token assign-left variable">BATCHSIZE</span><span class="token operator">=</span><span class="token number">65536</span> <span class="token punctuation">;</span>
<span class="token builtin class-name">export</span> <span class="token assign-left variable">WORLD_SIZE</span><span class="token operator">=</span><span class="token number">8</span> <span class="token punctuation">;</span>
torchx run <span class="token parameter variable">-s</span> local_cwd dist.ddp <span class="token parameter variable">-j</span> 1x8 <span class="token parameter variable">--script</span> dlrm_main.py -- <span class="token punctuation">\</span>
    <span class="token parameter variable">--embedding_dim</span> <span class="token number">128</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--dense_arch_layer_sizes</span> <span class="token number">512,256</span>,128 <span class="token punctuation">\</span>
    <span class="token parameter variable">--over_arch_layer_sizes</span> <span class="token number">1024,1024</span>,512,256,1 <span class="token punctuation">\</span>
    <span class="token parameter variable">--in_memory_binary_criteo_path</span> <span class="token variable">$PREPROCESSED_DATASET</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--num_embeddings_per_feature</span> <span class="token number">40000000,39060</span>,17295,7424,20265,3,7122,1543,63,40000000,3067956,405282,10,2209,11938,155,4,976,14,40000000,40000000,40000000,590152,12973,108,36 <span class="token punctuation">\</span>
    <span class="token parameter variable">--validation_freq_within_epoch</span> <span class="token variable"><span class="token variable">$((</span>TOTAL_TRAINING_SAMPLES <span class="token operator">/</span> <span class="token punctuation">(</span>BATCHSIZE <span class="token operator">*</span> <span class="token number">20</span><span class="token variable">))</span></span><span class="token punctuation">)</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--epochs</span> <span class="token number">1</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--pin_memory</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--mmap_mode</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--batch_size</span> <span class="token variable"><span class="token variable">$((</span>GLOBAL_BATCH_SIZE <span class="token operator">/</span> WORLD_SIZE<span class="token variable">))</span></span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--interaction_type</span><span class="token operator">=</span>dcn <span class="token punctuation">\</span>
    <span class="token parameter variable">--dcn_num_layers</span><span class="token operator">=</span><span class="token number">3</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--dcn_low_rank_dim</span><span class="token operator">=</span><span class="token number">512</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--adagrad</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--learning_rate</span> <span class="token number">0.005</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--multi_hot_distribution_type</span> uniform <span class="token punctuation">\</span>
    <span class="token parameter variable">--multi_hot_sizes</span><span class="token operator">=</span><span class="token number">3,2</span>,1,2,6,1,1,1,1,7,3,8,1,6,9,5,1,1,1,12,100,27,10,3,1,1

</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></li></ul></li><li><p>MLPerf DLRM Benchmark v1 &amp; v2æ¯”è¾ƒï¼š</p><ul><li><table><thead><tr><th></th><th>DLRM v1</th><th>DLRM v2</th></tr></thead><tbody><tr><td><strong>Optimizer</strong></td><td>SGD</td><td>Adagrad</td></tr><tr><td><strong>Learning Rate</strong></td><td>1.0</td><td>0.005</td></tr><tr><td><strong>Batch size</strong></td><td>AUC 0.8025 within 1 epoch using 16384</td><td>AUC 0.8025 within 1 epoch using 65536</td></tr><tr><td><strong>Interaction Layer</strong></td><td>dot interaction</td><td>DCN V2 with low rank approximation</td></tr><tr><td><strong>Dataset</strong></td><td>Criteo 1TB Click Logs Dataset, but uses a different preprocessing script (<a href="https://github.com/facebookresearch/dlrm/blob/main/data_utils.py" target="_blank" rel="noopener noreferrer">repo_root/data_utils.py<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>)</td><td>Criteo 1TB Click Logs Dataset but the sparse features are replaced with a multi-hot dataset.</td></tr></tbody></table></li></ul></li><li><p>ä½¿ç”¨Criteo Kaggle æ•°æ®é›†ï¼Œ é»˜è®¤ç½‘ç»œå‚æ•°</p><ul><li><p>ä½¿ç”¨DLRMé¡¹ç›®ä¸‹çš„torchrec_dlrm</p><div class="language-bash line-numbers-mode" data-ext="sh" data-title="sh"><pre class="language-bash"><code><span class="token function">git</span> clone <span class="token parameter variable">--recursive</span> https://github.com/facebookresearch/dlrm.git
<span class="token builtin class-name">cd</span> dlrm/torchrec_dlrm
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div></li><li><p>é¢„å¤„ç†æ•°æ®ï¼Œå¤„ç†æ–¹æ³•è§3å°èŠ‚</p></li><li><p>run</p><div class="language-bash line-numbers-mode" data-ext="sh" data-title="sh"><pre class="language-bash"><code><span class="token comment"># modify dlrm/torchrec_dlrm/data/dlrm_dataloader.py line87:</span>
<span class="token comment"># (root_name, stage) = (&quot;train&quot;, &quot;test&quot;) if stage == &quot;val&quot; else stage</span>
<span class="token comment"># -&gt; (root_name, stage) = (&quot;train&quot;, &quot;test&quot;) if stage == &quot;val&quot; else (stage, stage)</span>
<span class="token builtin class-name">export</span> <span class="token assign-left variable">PREPROCESSED_DATASET</span><span class="token operator">=</span><span class="token string">&quot;/workspace/volume/torchrec-criteo-datasets/criteo-kaggle&quot;</span>
<span class="token builtin class-name">export</span> <span class="token assign-left variable">GLOBAL_BATCH_SIZE</span><span class="token operator">=</span><span class="token number">16384</span> <span class="token punctuation">;</span>
<span class="token builtin class-name">export</span> <span class="token assign-left variable">WORLD_SIZE</span><span class="token operator">=</span><span class="token number">8</span> <span class="token punctuation">;</span>
torchx run <span class="token parameter variable">-s</span> local_cwd dist.ddp <span class="token parameter variable">-j</span> 1x8 <span class="token parameter variable">--script</span> dlrm_main.py -- <span class="token punctuation">\</span>
    <span class="token parameter variable">--in_memory_binary_criteo_path</span> <span class="token variable">$PREPROCESSED_DATASET</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--pin_memory</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--mmap_mode</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--batch_size</span> <span class="token variable"><span class="token variable">$((</span>GLOBAL_BATCH_SIZE <span class="token operator">/</span> WORLD_SIZE<span class="token variable">))</span></span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--learning_rate</span> <span class="token number">1.0</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--dataset_name</span> criteo_kaggle
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></li></ul></li></ul><h2 id="_6-dlrm-benchmarksæµ‹è¯•ç»“æœ" tabindex="-1"><a class="header-anchor" href="#_6-dlrm-benchmarksæµ‹è¯•ç»“æœ"><span>6. DLRM Benchmarksæµ‹è¯•ç»“æœ</span></a></h2><ul><li><p>æµ‹è¯•ç¯å¢ƒ:</p><table><thead><tr><th>distributed-training/torchrec:pytorch22.08-py3</th></tr></thead><tbody><tr><td>8*A100-SXM4-80GB</td></tr></tbody></table><ul><li><p>MLPerf DLRM v1 benchmark, 1 epoch</p><ul><li>AUROC over val set: <strong>0.8004854917526245</strong></li><li>AUROC over test set: <strong>0.7949966788291931</strong></li></ul></li><li><p>MLPerf DLRM v2 benchmark, 1 epoch</p><ul><li>AUROC over val set: <strong>0.8040649890899658</strong></li><li>AUROC over test set: <strong>0.7980538010597229</strong></li></ul></li><li><p>ä½¿ç”¨Criteo Kaggle æ•°æ®é›†ï¼Œ é»˜è®¤ç½‘ç»œå‚æ•°, 1 epoch</p><ul><li><p>AUROC over val set: <strong>0.5002527236938477</strong></p></li><li><p>torchrec-dlrmé¡¹ç›®åŸæ¥ä¸æ”¯æŒcriteo-kaggleæ•°æ®é›†ï¼Œä»Šå¹´2æœˆå¢åŠ æ”¯æŒ</p></li><li><p>å› ä¸ºcriteo-kaggleæ•°æ®é›†æ²¡æœ‰éªŒè¯é›†ï¼Œåªèƒ½ä»è®­ç»ƒé›†ä¸­åˆ’åˆ†ä¸€éƒ¨åˆ†åšéªŒè¯é›†</p></li><li><p>åŸä»£ç åŠ è½½æ•°æ®é›†æ—¶æŠ¥é”™ï¼š</p><div class="language-bash line-numbers-mode" data-ext="sh" data-title="sh"><pre class="language-bash"><code>File <span class="token string">&quot;./dlrm/torchrec_dlrm/data/dlrm_dataloader.py&quot;</span>, line <span class="token number">87</span>, <span class="token keyword">in</span> _get_in_memory_dataloader
dlrm_main/0 <span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span>:    <span class="token punctuation">(</span>root_name, stage<span class="token punctuation">)</span> <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token string">&quot;train&quot;</span>, <span class="token string">&quot;test&quot;</span><span class="token punctuation">)</span> <span class="token keyword">if</span> stage <span class="token operator">==</span> <span class="token string">&quot;val&quot;</span> <span class="token keyword">else</span> stage
dlrm_main/0 <span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">]</span>:ValueError: too many values to unpack <span class="token punctuation">(</span>expected <span class="token number">2</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>å°†(root_name, stage) = (&quot;train&quot;, &quot;test&quot;) if stage == &quot;val&quot; else stageï¼Œä¿®æ”¹ä¸º(root_name, stage) = (&quot;train&quot;, &quot;test&quot;) if stage == &quot;val&quot; else (stage, stage)åå¯è·‘é€šï¼Œä½†éªŒè¯æ—¶AUROCåªæœ‰0.5002527236938477ï¼Œä¸”ä¿®æ”¹è¶…å‚ä»æ— æ³•æå‡</p></li></ul></li></ul></li></ul><h2 id="_7-torchrecä¸hugectræ¯”è¾ƒ" tabindex="-1"><a class="header-anchor" href="#_7-torchrecä¸hugectræ¯”è¾ƒ"><span>7. Torchrecä¸HugeCTRæ¯”è¾ƒ</span></a></h2><ul><li><p>Torchrec:</p><ul><li>æ¶æ„ï¼Œç‰¹ç‚¹ï¼Œä¼˜ç¼ºç‚¹ï¼š <ul><li>ä¼˜ç‚¹ï¼š <ul><li>å®ç°åˆ†å¸ƒå¼æ¨¡å‹å¹¶è¡Œï¼Œæ”¯æŒæ•°æ®æ¨¡å‹æ··åˆå¹¶è¡Œï¼Œå¤šç§embåˆ‡åˆ†æ–¹å¼</li><li>å¯æ ¹æ®è®¾å¤‡å‹å·ä¸æ•°é‡ï¼Œå†…å­˜å¤§å°ç­‰ä¿¡æ¯è‡ªåŠ¨ç”Ÿæˆåˆ‡åˆ†ç­–ç•¥ï¼šæ ¹æ®è¾“å…¥ç½‘ç»œç»“æ„,å‚æ•°è§„æ¨¡, è¿è¡Œçš„è®¾å¤‡ç¯å¢ƒçš„æ‹“æ‰‘,è®¾å¤‡çš„å¸¦å®½,æ˜¾å­˜, æ•°æ®ç±»å‹ç­‰ä¿¡æ¯, ç©·ä¸¾æ‰€æœ‰åˆ‡åˆ†æ–¹æ¡ˆ,ç„¶åæŒ‘å‡ºæ‰€æœ‰å¯è¡Œçš„æ–¹æ¡ˆåšä¸€ä¸ªæ¨¡æ‹Ÿæ€§èƒ½è¯„ä¼°, æ ¹æ®è¯„ä¼°çš„ç»“æœå†æŒ‘å‡ºæ€§èƒ½æœ€å¥½çš„æ–¹æ¡ˆå‡ºæ¥</li><li>æ”¯æŒå¤šçº§æµæ°´ï¼Œé‡å æ•°æ®åŠ è½½ï¼Œè®¾å¤‡é—´é€šä¿¡ï¼ˆæ•°æ®åˆ†å‘ï¼‰å’Œå‰åå‘ä¼˜åŒ–è®¡ç®—</li><li>fbgemmåº“ä¼˜åŒ–kernelï¼Œé‡åŒ–ç­‰</li><li>PSæ¶æ„ï¼ŒCPUåšServerï¼ŒGPUåšWorkers <ul><li>ä¼ ç»ŸPSæ¶æ„embåœ¨CPUä¸Šï¼Œéœ€è¦å°†æ›´æ–°ç‰¹å¾æ¨é€å›psï¼Œéœ€ç­‰å¾…é€šä¿¡ï¼Œéš¾ä»¥ç»„æˆæµæ°´ï¼Œæ— æ³•å……åˆ†åˆ©ç”¨ç®—åŠ›</li><li>å½“å‰æ¶æ„åˆ©ç”¨gpué«˜å¸¦å®½äº¤æ¢embï¼Œå‡å°‘é€šä¿¡å¼€é”€ï¼Œä¸”å……åˆ†åˆ©ç”¨å¹¶è¡Œè®¡ç®—æ€§èƒ½</li></ul></li><li>æ”¯æŒGPU cacheæœºåˆ¶ï¼Œå­˜å‚¨çƒ­æ•°æ®ï¼Œä½¿ç”¨LRUæˆ–LFUæ··åˆé©±é€ç­–ç•¥ï¼Œä½¿ç”¨32ä½æ•°æ®ç»“æ„è®°å½•æ•°æ®ä½¿ç”¨æƒ…å†µï¼šä½27ä½è®°å½•æ—¶é—´æˆ³ï¼Œé«˜5ä½è®°å½•å‡ºç°é¢‘æ¬¡ï¼ˆæ¦‚ç‡ç®—æ³•ï¼Œæ¯æ¬¡å–é¢‘æ¬¡ä½éšæœºæ•°ï¼Œå…¨0åŠ ä¸€ï¼‰ï¼ŒLFUæ¯”LRUä¼˜å…ˆçº§é«˜</li><li>æ”¯æŒonehotå’Œmultihotä»¥åŠç¼ºçœå€¼æ•°æ®ï¼Œæ›´ç¬¦åˆçœŸå®åœºæ™¯</li></ul></li><li>ç¼ºç‚¹ï¼š <ul><li>æ”¯æŒåŠŸèƒ½å¤šï¼Œå¯¼è‡´æ¡†æ¶å†…éƒ¨ä¸­é—´æ•°æ®ä¼ é€’ä½æ•ˆï¼Œéœ€å¤šæ¬¡è½¬æ¢</li><li>ä¸æ”¯æŒåŠ¨æ€æ‰©è¡¨ï¼Œåªæ”¯æŒé™æ€è¡¨ï¼Œæ— æ³•å¤„ç†æ–°ç‰¹å¾</li><li>æ–°ç½‘ç»œé€‚é…åˆ°æ¡†æ¶éœ€è¦åšä¸€å®šå¼€å‘ï¼Œæ— æ³•ç›´æ¥ä½¿ç”¨å°è£…æ¥å£</li></ul></li></ul></li><li>æ•°æ®æµï¼š <ul><li>å¤§å¤šå®é™…ä¸šåŠ¡æ•°æ®æ˜¯TBçº§çš„ï¼Œå¯åˆ†ä¸ºonehotå’Œmultihotæ•°æ®ï¼Œç›¸å½“äºæŸä¸€é€‰é¡¹å•é€‰è¿˜æ˜¯å¤šé€‰</li><li>criteo-1tç‚¹å‡»ç‡æ•°æ®é›†ï¼š1label-13denseï¼ˆæ•°å€¼å‹ï¼‰-26-sparseï¼ˆåˆ†ç±»å‹ï¼‰ï¼Œå…±24å¤©æ•°æ®ï¼Œå¤§å°è¾¾655GB</li><li>æ•°æ®é¢„å¤„ç†ï¼š <ul><li>å°†dense int32è½¬ä¸ºfloat32ï¼ˆæ±‚logï¼‰ï¼Œåˆ’åˆ†ä¸ºä¸‰ä¸ªnpyæ–‡ä»¶</li><li>å°†sparseç‰¹å¾è¿ç»­åŒ–ï¼Œç»Ÿè®¡æ‰€æœ‰å‡ºç°çš„ç‰¹å¾æ•°é‡ï¼Œä½äºæŸä¸€é˜ˆå€¼çš„ç‰¹å¾éƒ½æ˜ å°„ä¸ºkey=1ï¼Œå…¶ä½™ä¾æ¬¡ç´¯è®¡keyå€¼</li><li>0-22å¤©çš„æ•°æ®åšè®­ç»ƒé›†ï¼Œshuffleï¼Œ23å¤©åšéªŒè¯é›†ï¼Œä¸shuffle</li></ul></li><li>æ•°æ®å¹¶è¡Œ-æ¨¡å‹å¹¶è¡Œ-æ•°æ®å¹¶è¡Œï¼š <ul><li>æ•°æ®åœ¨å†…å­˜ä¸­é€šè¿‡DDPå‡åŒ€åˆ†å‘åˆ°ä¸åŒè®¾å¤‡ä¸Šï¼ˆæ¯å¼ å¡æ•°æ®ä¸åŒï¼‰ã€scatterã€‘</li><li>ä¸åŒå¡æ‹¿åˆ°è®­ç»ƒæ•°æ®éœ€è¦åˆ°å¯¹åº”embè¡¨ä¸­æŸ¥è¯¢vectorï¼Œæ¯ç« è¡¨æ ¹æ®planå­˜å‚¨åœ¨ä¸åŒè®¾å¤‡ä¸Šã€all2allã€‘</li><li>æ‹¿åˆ°embæ•°æ®åå†ä¼ å…¥åç»­ç½‘ç»œç»“æ„ä¸­è®­ç»ƒï¼Œæ¯ä¸ªè®¾å¤‡ä¸Šçš„dense ç½‘ç»œéƒ¨åˆ†éœ€è¦åŒæ­¥ã€allreduceã€‘</li><li>æœ€åç»è¿‡åå‘æ¢¯åº¦ä¼ åˆ°æ¯ä¸ªè®¾å¤‡ä¸Šçš„embè¡¨ä¸­ï¼Œå†æ›´æ–°è¡¨çš„æƒå€¼</li></ul></li></ul></li><li>æµ‹è¯•ç»“æœï¼š8A100 SXM4 80G <ul><li>MLPerf DLRM v1: 0.8004</li><li>MLPerf DLRM v2: 0.8041</li></ul></li><li>è¿ç§»æ–¹æ¡ˆ&amp;é‡åˆ°çš„é—®é¢˜ä¸è§£å†³æ–¹æ³•ï¼š <ul><li>ä½¿ç”¨è®¾å¤‡ç«¯å“ˆå¸Œè¡¨å­˜å‚¨embï¼Œæ¥å®ç°åŠ¨æ€æ‰©å®¹ <ul><li>keyå’Œvalueåˆ†åˆ«å­˜å‚¨åˆ°ä¸¤å¼ è¡¨ï¼Œkeyæ˜ å°„åˆ°value,keyè¡¨æ‰©å®¹å¼€è¾Ÿä¸€å—æ–°ç©ºé—´ï¼Œæ‹·è´åˆ°æ–°è¡¨ï¼Œvalueä»¥é“¾è¡¨å½¢å¼å­˜å‚¨ï¼Œç›´æ¥è¿½åŠ åˆ°è¡¨å°¾</li></ul></li><li>ç‰ˆæœ¬ä¸ä¾èµ–åº“é—®é¢˜ï¼šåŸç”Ÿæ¡†æ¶åªæ”¯æŒpt1.13ä»¥ä¸Šï¼Œéœ€è¦åˆ‡åˆ†å’Œfbgemmåº“æä¾›æ”¯æŒï¼Œå› æ­¤éœ€è¦æ›¿æ¢æˆ–é¿å…è¿™äº›åŠŸèƒ½</li><li>å°†ä½¿ç”¨åˆ°fbgemmåº“çš„ä½ç½®æ›¿æ¢å®ç°æˆ–é‡å†™ï¼Œä½¿ç”¨cnco</li><li>å°†ä½¿ç”¨åˆ°é«˜ç‰ˆæœ¬ptçš„æ¨¡å—è§£è€¦å‡ºæ¥ä½œä¸ºå­æ¨¡å—ä½¿ç”¨</li><li>ä¸åŒè¯­è¨€å®ç°çš„æ¨¡å—ä½¿ç”¨torch_libraryç»‘å®šc++æ¥å£åˆ°pyç«¯</li><li>aurocä½¿ç”¨sklearnæ›¿æ¢å®ç°ï¼Œä½¿ç”¨é›†åˆé€šä¿¡æ¥å£å°†æ•°æ®ä¼ å›cpuåšè®¡ç®—</li></ul></li><li>æ€§èƒ½æå‡ï¼š <ul><li>ä½¿ç”¨profilerè·å–hostå’Œdeviceä¾§ç®—å­å’Œkernelè°ƒç”¨æƒ…å†µï¼Œæ‰¾å‡ºçƒ­ç‚¹ç®—å­ï¼Œå†æissueæˆ–æ›¿æ¢ç®—å­æ¥æå‡ç²¾åº¦</li><li>å› ä¸ºä¸å†ä½¿ç”¨fbgemmåº“ï¼Œéƒ¨åˆ†æ¡†æ¶åŠŸèƒ½å®ç°æ€§èƒ½å·®ï¼Œå¦‚å†…éƒ¨æ•°æ®é—´çš„ç›¸äº’è½¬æ¢ä½¿ç”¨ä½æ•ˆçš„æ ‡é‡æ“ä½œï¼Œæ›¿æ¢ä¸ºçŸ©é˜µæ“ä½œå¿«å¾ˆå¤š</li></ul></li><li>ç²¾åº¦æå‡ï¼š <ul><li>ä¸åŒè¯­è¨€é—´å®ç°çš„æ¨¡å—åœ¨é…åˆä½¿ç”¨æ—¶ç”¨åˆ°äº†ä¸åŒçš„streamï¼Œä¼ å‚æ—¶æ²¡èƒ½åŒæ­¥ï¼Œä¼šå¯¼è‡´æ•°æ®å¼‚å¸¸</li><li>æ•°æ®åˆå§‹åŒ–æ–¹å¼ä½¿ç”¨ä¸è¡¨å¤§å°ç›¸å…³çš„å‡åŒ€åˆ†å¸ƒï¼Œç²¾åº¦ä¼šæ›´é«˜</li></ul></li></ul></li><li><p>HugeCTR:</p><ul><li>æ¶æ„ï¼Œç‰¹ç‚¹ï¼Œä¼˜ç¼ºç‚¹ï¼Œå¼‚åŒï¼š <ul><li>ç›¸åŒï¼šéƒ½æ”¯æŒè¯»å–åˆ†å‘è®­ç»ƒæµæ°´</li><li>ä¸åŒï¼š <ul><li>hugectråªæ”¯æŒæŒ‰è¡ŒæŒ‰è¡¨åˆ‡åˆ†embï¼Œtorchrecèƒ½è‡ªåŠ¨é€‰æ‹©ä¸”èƒ½æ··åˆå¤šç§åˆ‡åˆ†æ–¹å¼</li><li>ä¸¤è€…æ•°æ®é¢„å¤„ç†æ–¹å¼ç›¸ä¼¼ï¼Œåªæ˜¯hugectrè¿˜å¯ä½¿ç”¨ç‰¹å¾ç»„åˆè¿›ä¸€æ­¥å‡å°‘ç‰¹å¾æ•°é‡ï¼Œæå‡è¡¨è¾¾èƒ½åŠ›</li><li>hugecträ½¿ç”¨keylisté¢„å…ˆåˆ’åˆ†å¥½è®­ç»ƒæ•°æ®ï¼Œtorchrecç›´æ¥ä½¿ç”¨dataloaderè¯»å–</li><li>hctræ”¯æŒåŠ¨æ€è¡¨ï¼Œä½¿ç”¨GPUä¸Šçš„hashtableå®ç°ï¼ˆcucollectionï¼‰</li></ul></li><li>hugectræ›´åŠ å®Œå–„ï¼Œæ˜¯merlinæ¨èç³»ç»Ÿæ¨¡å‹æ¨ç†è®­ç»ƒè§£å†³æ–¹æ¡ˆä¸‹çš„è®­ç»ƒæ¡†æ¶ï¼Œæ”¯æŒGPUä¸­çš„hashè¡¨ï¼Œå¼‚æ­¥ä¸å¤šçº¿ç¨‹æµæ°´ï¼Œåˆ†å±‚å‚æ•°æœåŠ¡å™¨åšæ¨ç†ï¼ŒTFæ’ä»¶sok</li><li>è®­ç»ƒæ—¶ï¼Œhugectré¢„å–æ¯ä¸ªpassçš„keyé›†åˆåˆ°keysetï¼Œä»è€Œè§£å†³æ— æ³•å­˜æ”¾æ‰€æœ‰æ•°æ®çš„éš¾é¢˜</li><li>å‚æ•°æœåŠ¡å™¨æ”¯æŒå…¨é‡è¯»å¦‚æ•´ä¸ªembåˆ°hostå†…å­˜ä¸­ï¼ˆé€Ÿåº¦å¿«ï¼‰ï¼Œæˆ–ä½¿ç”¨å¤šçº§ç¼“å­˜ç»“æ„åªå­˜éƒ¨åˆ†ï¼ˆå…‹æœå¯¹æ¨¡å‹è§„æ¨¡çš„é™åˆ¶ï¼‰</li><li>æ¨ç†æ—¶ï¼Œä¸‰çº§å­˜å‚¨ç»“æ„ï¼šä½¿ç”¨GPUåµŒå…¥ç¼“å­˜å°†çƒ­ç‚¹embæ”¾åœ¨gpuå†…å­˜ä¸­ï¼Œå†…å­˜ä½œä¸ºäºŒçº§ï¼Œä½¿ç”¨redisä¿å­˜éƒ¨åˆ†æ•°æ®ï¼›ç¬¬ä¸‰çº§ä½¿ç”¨SSD RocksDBä¿å­˜æ‰€æœ‰å‚æ•°ï¼ˆé«˜æ•ˆå­˜å‚¨é•¿å°¾åˆ†å¸ƒæ•°æ®ï¼‰ï¼Œä½¿ç”¨çš„æ˜¯LRU</li></ul></li></ul></li><li><p>Relevantï¼š</p><ul><li>GPU hashtableï¼š <ul><li>hashå†²çªå¤„ç†æ–¹æ³•ï¼š <ul><li>double hashï¼Œåˆ†åˆ«è®¡ç®—å†èšåˆ</li><li>frequency hashï¼šå¯¹ä½é¢‘åšåŒhashï¼Œé«˜é¢‘åšidentity hash</li></ul></li></ul></li><li>æ€§èƒ½æŒ‡æ ‡ï¼šprofiling timechartï¼Œ ååé‡</li><li>ç²¾åº¦æŒ‡æ ‡ï¼šAUC</li></ul></li></ul></div><!--[--><!----><!--]--><footer class="page-meta"><!----><div class="meta-item git-info"><!----><!----></div></footer><nav class="vp-page-nav"><a class="route-link nav-link prev" href="/notes/PLAN_Z.html" aria-label="TODO LIST"><div class="hint"><span class="arrow start"></span>ä¸Šä¸€é¡µ</div><div class="link"><!---->TODO LIST</div></a><a class="route-link nav-link next" href="/notes/uml_note.html" aria-label="UMLå­¦ä¹ ç¬”è®°"><div class="hint">ä¸‹ä¸€é¡µ<span class="arrow end"></span></div><div class="link">UMLå­¦ä¹ ç¬”è®°<!----></div></a></nav><div id="vp-comment" class="giscus-wrapper input-top" style="display:block;"><svg xmlns="http://www.w3.org/2000/svg" width="32" height="32" preserveAspectRatio="xMidYMid" viewBox="0 0 100 100"><circle cx="28" cy="75" r="11" fill="currentColor"><animate attributeName="fill-opacity" begin="0s" dur="1s" keyTimes="0;0.2;1" repeatCount="indefinite" values="0;1;1"></animate></circle><path fill="none" stroke="#88baf0" stroke-width="10" d="M28 47a28 28 0 0 1 28 28"><animate attributeName="stroke-opacity" begin="0.1s" dur="1s" keyTimes="0;0.2;1" repeatCount="indefinite" values="0;1;1"></animate></path><path fill="none" stroke="#88baf0" stroke-width="10" d="M28 25a50 50 0 0 1 50 50"><animate attributeName="stroke-opacity" begin="0.2s" dur="1s" keyTimes="0;0.2;1" repeatCount="indefinite" values="0;1;1"></animate></path></svg></div><!--[--><!----><!--]--><!--]--></main><!--]--><footer class="vp-footer-wrapper"><div class="vp-footer">BradZhone's Blog</div><div class="vp-copyright">Copyright Â© 2024 BradZhone </div></footer></div><!--]--><!--[--><!----><!--]--><!--]--></div>
    <script type="module" src="/assets/app-Coh1oo3x.js" defer></script>
  </body>
</html>
