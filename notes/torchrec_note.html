<!doctype html>
<html lang="zh-CN" data-theme="light">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width,initial-scale=1" />
    <meta name="generator" content="VuePress 2.0.0-rc.9" />
    <meta name="theme" content="VuePress Theme Hope 2.0.0-rc.32" />
    <style>
      html {
        background: var(--bg-color, #fff);
      }

      html[data-theme="dark"] {
        background: var(--bg-color, #1d1e1f);
      }

      body {
        background: var(--bg-color);
      }
    </style>
    <script>
      const userMode = localStorage.getItem("vuepress-theme-hope-scheme");
      const systemDarkMode =
        window.matchMedia &&
        window.matchMedia("(prefers-color-scheme: dark)").matches;

      if (userMode === "dark" || (userMode !== "light" && systemDarkMode)) {
        document.documentElement.setAttribute("data-theme", "dark");
      }
    </script>
    <meta property="og:url" content="https://bradzhone.github.io/notes/torchrec_note.html"><meta property="og:site_name" content="BradZhone's Blog"><meta property="og:title" content="Torchrec调研"><meta property="og:description" content="Torchrec调研 0.0 未迁移依赖库汇总 torch torch.distributed._shard torch.fx._compatibility torch.distributed._composable torch.distributed.optim._apply_optimizer_in_backward torch.distribut..."><meta property="og:type" content="article"><meta property="og:locale" content="zh-CN"><meta property="article:author" content="BradZhone"><meta property="article:tag" content="Torchrec"><meta property="article:published_time" content="2023-06-21T00:00:00.000Z"><script type="application/ld+json">{"@context":"https://schema.org","@type":"Article","headline":"Torchrec调研","image":[""],"datePublished":"2023-06-21T00:00:00.000Z","dateModified":null,"author":[{"@type":"Person","name":"BradZhone","url":"https://github.com/BradZhone"}]}</script><title>Torchrec调研 | BradZhone's Blog</title><meta name="description" content="Torchrec调研 0.0 未迁移依赖库汇总 torch torch.distributed._shard torch.fx._compatibility torch.distributed._composable torch.distributed.optim._apply_optimizer_in_backward torch.distribut...">
    <link rel="preload" href="/assets/style-BkjCpNEe.css" as="style"><link rel="stylesheet" href="/assets/style-BkjCpNEe.css">
    <link rel="modulepreload" href="/assets/app-Coh1oo3x.js"><link rel="modulepreload" href="/assets/torchrec_note.html-B-i_O6mg.js"><link rel="modulepreload" href="/assets/plugin-vue_export-helper-DlAUqK2U.js">
    <link rel="prefetch" href="/assets/intro.html-D9dwEpHI.js" as="script"><link rel="prefetch" href="/assets/index.html-D5LnkeFh.js" as="script"><link rel="prefetch" href="/assets/CRNN_blog.html-Cb6b7OD4.js" as="script"><link rel="prefetch" href="/assets/cucollection_blog.html-DLix_uWp.js" as="script"><link rel="prefetch" href="/assets/cuda_blog.html-CBZ1Rex-.js" as="script"><link rel="prefetch" href="/assets/distributed_embeddings_blog.html-Cw7oBr-d.js" as="script"><link rel="prefetch" href="/assets/howToBuildThisBlog.html-Dxn0xRj3.js" as="script"><link rel="prefetch" href="/assets/hugectr_blog.html-BrFINEES.js" as="script"><link rel="prefetch" href="/assets/hugectr_src_blog.html-d9qgPAFk.js" as="script"><link rel="prefetch" href="/assets/index.html-DI16Z1C4.js" as="script"><link rel="prefetch" href="/assets/torchrec_cn_embedding_note.html-YEVLbIAq.js" as="script"><link rel="prefetch" href="/assets/warpcore_blog.html-Cz_5IJvA.js" as="script"><link rel="prefetch" href="/assets/c___note.html-BYbhq9Nq.js" as="script"><link rel="prefetch" href="/assets/deep_learning.html-b-b3zdJE.js" as="script"><link rel="prefetch" href="/assets/linux_command.html-G4dNyKFL.js" as="script"><link rel="prefetch" href="/assets/LLM.html-9hu1pKVY.js" as="script"><link rel="prefetch" href="/assets/loss.html-CJxfnubw.js" as="script"><link rel="prefetch" href="/assets/markdown.html-DJcQDvEC.js" as="script"><link rel="prefetch" href="/assets/metrics.html-DKJGeFbq.js" as="script"><link rel="prefetch" href="/assets/PLAN_Z.html-B9yJBwFA.js" as="script"><link rel="prefetch" href="/assets/precision.html-B_dd_MdY.js" as="script"><link rel="prefetch" href="/assets/index.html-D1wTsZoY.js" as="script"><link rel="prefetch" href="/assets/uml_note.html-IaIR5y-b.js" as="script"><link rel="prefetch" href="/assets/index.html-Cgwhpoct.js" as="script"><link rel="prefetch" href="/assets/404.html-DYVdKbdY.js" as="script"><link rel="prefetch" href="/assets/index.html-1jQ9-Uww.js" as="script"><link rel="prefetch" href="/assets/index.html-BEDqx9YQ.js" as="script"><link rel="prefetch" href="/assets/index.html-BltlhrdK.js" as="script"><link rel="prefetch" href="/assets/index.html-C0yITs6x.js" as="script"><link rel="prefetch" href="/assets/index.html-maJbTduo.js" as="script"><link rel="prefetch" href="/assets/index.html-BPQ9UPx_.js" as="script"><link rel="prefetch" href="/assets/index.html-BQB6pLKG.js" as="script"><link rel="prefetch" href="/assets/index.html-Be8HnZrW.js" as="script"><link rel="prefetch" href="/assets/index.html-DzX6xTUq.js" as="script"><link rel="prefetch" href="/assets/index.html-BxZ6QV-X.js" as="script"><link rel="prefetch" href="/assets/index.html-Dl0UMf-I.js" as="script"><link rel="prefetch" href="/assets/index.html-xBRd_OsE.js" as="script"><link rel="prefetch" href="/assets/index.html-BTkUZ9Ws.js" as="script"><link rel="prefetch" href="/assets/index.html-D57wSldI.js" as="script"><link rel="prefetch" href="/assets/index.html-Dqr1foaP.js" as="script"><link rel="prefetch" href="/assets/index.html-CSGzSmAq.js" as="script"><link rel="prefetch" href="/assets/index.html-BnBAmG4V.js" as="script"><link rel="prefetch" href="/assets/index.html-d2G8e39M.js" as="script"><link rel="prefetch" href="/assets/index.html-CpCodvQl.js" as="script"><link rel="prefetch" href="/assets/index.html-hEB5WJuD.js" as="script"><link rel="prefetch" href="/assets/index.html-DX2Ijbyk.js" as="script"><link rel="prefetch" href="/assets/index.html-Da6Vp1bl.js" as="script"><link rel="prefetch" href="/assets/index.html-CGr0tdaC.js" as="script"><link rel="prefetch" href="/assets/index.html-D5fEOhj5.js" as="script"><link rel="prefetch" href="/assets/index.html-C1vK6J6C.js" as="script"><link rel="prefetch" href="/assets/index.html-DgD_mq8U.js" as="script"><link rel="prefetch" href="/assets/index.html-Demc7CPi.js" as="script"><link rel="prefetch" href="/assets/index.html-D31kcDwM.js" as="script"><link rel="prefetch" href="/assets/index.html-COTqqkDG.js" as="script"><link rel="prefetch" href="/assets/index.html-DtZDr4LJ.js" as="script"><link rel="prefetch" href="/assets/index.html-BYDcpP-H.js" as="script"><link rel="prefetch" href="/assets/index.html-QpKn8zOy.js" as="script"><link rel="prefetch" href="/assets/index.html-Fd1nQRc_.js" as="script"><link rel="prefetch" href="/assets/index.html-BEVLRn0k.js" as="script"><link rel="prefetch" href="/assets/giscus-7BMGhbDA.js" as="script"><link rel="prefetch" href="/assets/photoswipe.esm-SzV8tJDW.js" as="script">
  </head>
  <body>
    <div id="app"><!--[--><!--[--><!--[--><span tabindex="-1"></span><a href="#main-content" class="vp-skip-link sr-only">跳至主要內容</a><!--]--><div class="theme-container has-toc"><!--[--><header id="navbar" class="vp-navbar"><div class="vp-navbar-start"><button type="button" class="vp-toggle-sidebar-button" title="Toggle Sidebar"><span class="icon"></span></button><!--[--><!----><!--]--><!--[--><a class="route-link vp-brand" href="/"><img class="vp-nav-logo light" src="/light.png" alt><img class="vp-nav-logo dark" src="/dark.png" alt><span class="vp-site-name hide-in-pad">BradZhone&#39;s Blog</span></a><!--]--><!--[--><!----><!--]--></div><div class="vp-navbar-center"><!--[--><!----><!--]--><!--[--><nav class="vp-nav-links"><div class="nav-item hide-in-mobile"><a class="route-link nav-link" href="/" aria-label="Home"><span class="font-icon icon fa-fw fa-sm fas fa-home" style=""></span>Home<!----></a></div><div class="nav-item hide-in-mobile"><a class="route-link nav-link" href="/blogs/" aria-label="Blogs"><span class="font-icon icon fa-fw fa-sm fas fa-blog" style=""></span>Blogs<!----></a></div><div class="nav-item hide-in-mobile"><a class="route-link nav-link active" href="/notes/" aria-label="Notes"><span class="font-icon icon fa-fw fa-sm fas fa-note-sticky" style=""></span>Notes<!----></a></div><div class="nav-item hide-in-mobile"><a class="route-link nav-link" href="/thinking/" aria-label="Thinking"><span class="font-icon icon fa-fw fa-sm fas fa-lightbulb" style=""></span>Thinking<!----></a></div><div class="nav-item hide-in-mobile"><a class="route-link nav-link" href="/intro.html" aria-label="About Me"><span class="font-icon icon fa-fw fa-sm fas fa-star" style=""></span>About Me<!----></a></div></nav><!--]--><!--[--><!----><!--]--></div><div class="vp-navbar-end"><!--[--><!----><!--]--><!--[--><!----><!----><div class="nav-item hide-in-mobile"><button type="button" id="appearance-switch"><svg xmlns="http://www.w3.org/2000/svg" class="icon auto-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="auto icon" style="display:none;"><path d="M512 992C246.92 992 32 777.08 32 512S246.92 32 512 32s480 214.92 480 480-214.92 480-480 480zm0-840c-198.78 0-360 161.22-360 360 0 198.84 161.22 360 360 360s360-161.16 360-360c0-198.78-161.22-360-360-360zm0 660V212c165.72 0 300 134.34 300 300 0 165.72-134.28 300-300 300z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" class="icon dark-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="dark icon" style="display:none;"><path d="M524.8 938.667h-4.267a439.893 439.893 0 0 1-313.173-134.4 446.293 446.293 0 0 1-11.093-597.334A432.213 432.213 0 0 1 366.933 90.027a42.667 42.667 0 0 1 45.227 9.386 42.667 42.667 0 0 1 10.24 42.667 358.4 358.4 0 0 0 82.773 375.893 361.387 361.387 0 0 0 376.747 82.774 42.667 42.667 0 0 1 54.187 55.04 433.493 433.493 0 0 1-99.84 154.88 438.613 438.613 0 0 1-311.467 128z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" class="icon light-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="light icon" style="display:block;"><path d="M952 552h-80a40 40 0 0 1 0-80h80a40 40 0 0 1 0 80zM801.88 280.08a41 41 0 0 1-57.96-57.96l57.96-58a41.04 41.04 0 0 1 58 58l-58 57.96zM512 752a240 240 0 1 1 0-480 240 240 0 0 1 0 480zm0-560a40 40 0 0 1-40-40V72a40 40 0 0 1 80 0v80a40 40 0 0 1-40 40zm-289.88 88.08-58-57.96a41.04 41.04 0 0 1 58-58l57.96 58a41 41 0 0 1-57.96 57.96zM192 512a40 40 0 0 1-40 40H72a40 40 0 0 1 0-80h80a40 40 0 0 1 40 40zm30.12 231.92a41 41 0 0 1 57.96 57.96l-57.96 58a41.04 41.04 0 0 1-58-58l58-57.96zM512 832a40 40 0 0 1 40 40v80a40 40 0 0 1-80 0v-80a40 40 0 0 1 40-40zm289.88-88.08 58 57.96a41.04 41.04 0 0 1-58 58l-57.96-58a41 41 0 0 1 57.96-57.96z"></path></svg></button></div><!----><!--]--><!--[--><!----><!--]--><button type="button" class="vp-toggle-navbar-button" aria-label="Toggle Navbar" aria-expanded="false" aria-controls="nav-screen"><span><span class="vp-top"></span><span class="vp-middle"></span><span class="vp-bottom"></span></span></button></div></header><!----><!--]--><!----><div class="toggle-sidebar-wrapper"><span class="arrow start"></span></div><aside id="sidebar" class="vp-sidebar"><!--[--><!----><!--]--><ul class="vp-sidebar-links"><li><section class="vp-sidebar-group"><p class="vp-sidebar-header clickable"><span class="font-icon icon fa-fw fa-sm fas fa-blog" style=""></span><a class="route-link nav-link vp-sidebar-title" href="/blogs/" aria-label="Blogs"><!---->Blogs<!----></a><!----></p><ul class="vp-sidebar-links"><li><a class="route-link nav-link vp-sidebar-link vp-sidebar-page" href="/blogs/howToBuildThisBlog.html" aria-label="如何搭建本博客"><!---->如何搭建本博客<!----></a></li><li><a class="route-link nav-link vp-sidebar-link vp-sidebar-page" href="/blogs/CRNN_blog.html" aria-label="CRNN网络调研适配记录"><!---->CRNN网络调研适配记录<!----></a></li><li><a class="route-link nav-link vp-sidebar-link vp-sidebar-page" href="/blogs/cucollection_blog.html" aria-label="cuCollections性能测试"><!---->cuCollections性能测试<!----></a></li><li><a class="route-link nav-link vp-sidebar-link vp-sidebar-page" href="/blogs/cuda_blog.html" aria-label="CUDA学习笔记"><!---->CUDA学习笔记<!----></a></li><li><a class="route-link nav-link vp-sidebar-link vp-sidebar-page" href="/blogs/distributed_embeddings_blog.html" aria-label="Distributed_embeddings"><!---->Distributed_embeddings<!----></a></li><li><a class="route-link nav-link vp-sidebar-link vp-sidebar-page" href="/blogs/hugectr_blog.html" aria-label="HugeCTR 学习笔记"><!---->HugeCTR 学习笔记<!----></a></li><li><a class="route-link nav-link vp-sidebar-link vp-sidebar-page" href="/blogs/hugectr_src_blog.html" aria-label="HugeCTR源码阅读笔记"><!---->HugeCTR源码阅读笔记<!----></a></li><li><a class="route-link nav-link vp-sidebar-link vp-sidebar-page" href="/blogs/torchrec_cn_embedding_note.html" aria-label="torchrec cn_embedding模块设计方案"><!---->torchrec cn_embedding模块设计方案<!----></a></li><li><a class="route-link nav-link vp-sidebar-link vp-sidebar-page" href="/blogs/warpcore_blog.html" aria-label="WARPCORE 学习笔记"><!---->WARPCORE 学习笔记<!----></a></li></ul></section></li><li><section class="vp-sidebar-group"><p class="vp-sidebar-header clickable active"><span class="font-icon icon fa-fw fa-sm fas fa-note-sticky" style=""></span><a class="route-link nav-link active vp-sidebar-title" href="/notes/" aria-label="Notes"><!---->Notes<!----></a><!----></p><ul class="vp-sidebar-links"><li><a class="route-link nav-link vp-sidebar-link vp-sidebar-page" href="/notes/c___note.html" aria-label="C++ note"><!---->C++ note<!----></a></li><li><a class="route-link nav-link vp-sidebar-link vp-sidebar-page" href="/notes/deep_learning.html" aria-label="DL相关"><!---->DL相关<!----></a></li><li><a class="route-link nav-link vp-sidebar-link vp-sidebar-page" href="/notes/precision.html" aria-label="Floating Point precision formats"><!---->Floating Point precision formats<!----></a></li><li><a class="route-link nav-link vp-sidebar-link vp-sidebar-page" href="/notes/linux_command.html" aria-label="Linux 快捷指令"><!---->Linux 快捷指令<!----></a></li><li><a class="route-link nav-link vp-sidebar-link vp-sidebar-page" href="/notes/LLM.html" aria-label="LLM"><!---->LLM<!----></a></li><li><a class="route-link nav-link vp-sidebar-link vp-sidebar-page" href="/notes/loss.html" aria-label="Loss 相关问题"><!---->Loss 相关问题<!----></a></li><li><a class="route-link nav-link vp-sidebar-link vp-sidebar-page" href="/notes/markdown.html" aria-label="Markdown 语法"><!---->Markdown 语法<!----></a></li><li><a class="route-link nav-link vp-sidebar-link vp-sidebar-page" href="/notes/metrics.html" aria-label="Metrics"><!---->Metrics<!----></a></li><li><a class="route-link nav-link vp-sidebar-link vp-sidebar-page" href="/notes/PLAN_Z.html" aria-label="TODO LIST"><!---->TODO LIST<!----></a></li><li><a class="route-link nav-link active vp-sidebar-link vp-sidebar-page active" href="/notes/torchrec_note.html" aria-label="Torchrec调研"><!---->Torchrec调研<!----></a></li><li><a class="route-link nav-link vp-sidebar-link vp-sidebar-page" href="/notes/uml_note.html" aria-label="UML学习笔记"><!---->UML学习笔记<!----></a></li></ul></section></li><li><section class="vp-sidebar-group"><p class="vp-sidebar-header clickable"><span class="font-icon icon fa-fw fa-sm fas fa-lightbulb" style=""></span><a class="route-link nav-link vp-sidebar-title" href="/thinking/" aria-label="Thinking"><!---->Thinking<!----></a><!----></p><ul class="vp-sidebar-links"></ul></section></li></ul><!--[--><!----><!--]--></aside><!--[--><main id="main-content" class="vp-page"><!--[--><!--[--><!----><!--]--><!----><nav class="vp-breadcrumb disable"></nav><div class="vp-page-title"><h1><!---->Torchrec调研</h1><div class="page-info"><span class="page-author-info" aria-label="作者🖊" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon author-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="author icon"><path d="M649.6 633.6c86.4-48 147.2-144 147.2-249.6 0-160-128-288-288-288s-288 128-288 288c0 108.8 57.6 201.6 147.2 249.6-121.6 48-214.4 153.6-240 288-3.2 9.6 0 19.2 6.4 25.6 3.2 9.6 12.8 12.8 22.4 12.8h704c9.6 0 19.2-3.2 25.6-12.8 6.4-6.4 9.6-16 6.4-25.6-25.6-134.4-121.6-240-243.2-288z"></path></svg><span><a class="page-author-item" href="https://github.com/BradZhone" target="_blank" rel="noopener noreferrer">BradZhone</a></span><span property="author" content="BradZhone"></span></span><!----><span class="page-date-info" aria-label="写作日期📅" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon calendar-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="calendar icon"><path d="M716.4 110.137c0-18.753-14.72-33.473-33.472-33.473-18.753 0-33.473 14.72-33.473 33.473v33.473h66.993v-33.473zm-334.87 0c0-18.753-14.72-33.473-33.473-33.473s-33.52 14.72-33.52 33.473v33.473h66.993v-33.473zm468.81 33.52H716.4v100.465c0 18.753-14.72 33.473-33.472 33.473a33.145 33.145 0 01-33.473-33.473V143.657H381.53v100.465c0 18.753-14.72 33.473-33.473 33.473a33.145 33.145 0 01-33.473-33.473V143.657H180.6A134.314 134.314 0 0046.66 277.595v535.756A134.314 134.314 0 00180.6 947.289h669.74a134.36 134.36 0 00133.94-133.938V277.595a134.314 134.314 0 00-133.94-133.938zm33.473 267.877H147.126a33.145 33.145 0 01-33.473-33.473c0-18.752 14.72-33.473 33.473-33.473h736.687c18.752 0 33.472 14.72 33.472 33.473a33.145 33.145 0 01-33.472 33.473z"></path></svg><span><!----></span><meta property="datePublished" content="2023-06-21T00:00:00.000Z"></span><!----><span class="page-reading-time-info" aria-label="阅读时间⌛" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon timer-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="timer icon"><path d="M799.387 122.15c4.402-2.978 7.38-7.897 7.38-13.463v-1.165c0-8.933-7.38-16.312-16.312-16.312H256.33c-8.933 0-16.311 7.38-16.311 16.312v1.165c0 5.825 2.977 10.874 7.637 13.592 4.143 194.44 97.22 354.963 220.201 392.763-122.204 37.542-214.893 196.511-220.2 389.397-4.661 5.049-7.638 11.651-7.638 19.03v5.825h566.49v-5.825c0-7.379-2.849-13.981-7.509-18.9-5.049-193.016-97.867-351.985-220.2-389.527 123.24-37.67 216.446-198.453 220.588-392.892zM531.16 450.445v352.632c117.674 1.553 211.787 40.778 211.787 88.676H304.097c0-48.286 95.149-87.382 213.728-88.676V450.445c-93.077-3.107-167.901-81.297-167.901-177.093 0-8.803 6.99-15.793 15.793-15.793 8.803 0 15.794 6.99 15.794 15.793 0 80.261 63.69 145.635 142.01 145.635s142.011-65.374 142.011-145.635c0-8.803 6.99-15.793 15.794-15.793s15.793 6.99 15.793 15.793c0 95.019-73.789 172.82-165.96 177.093z"></path></svg><span>大约 26 分钟</span><meta property="timeRequired" content="PT26M"></span><span class="page-category-info" aria-label="分类🌈" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon category-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="category icon"><path d="M148.41 106.992h282.176c22.263 0 40.31 18.048 40.31 40.31V429.48c0 22.263-18.047 40.31-40.31 40.31H148.41c-22.263 0-40.311-18.047-40.311-40.31V147.302c0-22.263 18.048-40.31 40.311-40.31zM147.556 553.478H429.73c22.263 0 40.311 18.048 40.311 40.31v282.176c0 22.263-18.048 40.312-40.31 40.312H147.555c-22.263 0-40.311-18.049-40.311-40.312V593.79c0-22.263 18.048-40.311 40.31-40.311zM593.927 106.992h282.176c22.263 0 40.31 18.048 40.31 40.31V429.48c0 22.263-18.047 40.31-40.31 40.31H593.927c-22.263 0-40.311-18.047-40.311-40.31V147.302c0-22.263 18.048-40.31 40.31-40.31zM730.22 920.502H623.926c-40.925 0-74.22-33.388-74.22-74.425V623.992c0-41.038 33.387-74.424 74.425-74.424h222.085c41.038 0 74.424 33.226 74.424 74.067v114.233c0 10.244-8.304 18.548-18.547 18.548s-18.548-8.304-18.548-18.548V623.635c0-20.388-16.746-36.974-37.33-36.974H624.13c-20.585 0-37.331 16.747-37.331 37.33v222.086c0 20.585 16.654 37.331 37.126 37.331H730.22c10.243 0 18.547 8.304 18.547 18.547 0 10.244-8.304 18.547-18.547 18.547z"></path></svg><!--[--><span class="page-category-item category8 clickable" role="navigation">推荐系统</span><!--]--><meta property="articleSection" content="推荐系统"></span><span class="page-tag-info" aria-label="标签🏷" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon tag-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="tag icon"><path d="M939.902 458.563L910.17 144.567c-1.507-16.272-14.465-29.13-30.737-30.737L565.438 84.098h-.402c-3.215 0-5.726 1.005-7.634 2.913l-470.39 470.39a10.004 10.004 0 000 14.164l365.423 365.424c1.909 1.908 4.42 2.913 7.132 2.913s5.223-1.005 7.132-2.913l470.39-470.39c2.01-2.11 3.014-5.023 2.813-8.036zm-240.067-72.121c-35.458 0-64.286-28.828-64.286-64.286s28.828-64.285 64.286-64.285 64.286 28.828 64.286 64.285-28.829 64.286-64.286 64.286z"></path></svg><!--[--><span class="page-tag-item tag1 clickable" role="navigation">Torchrec</span><!--]--><meta property="keywords" content="Torchrec"></span></div><hr></div><div class="vp-toc-placeholder"><aside id="toc"><!--[--><!----><!--]--><div class="vp-toc-header">此页内容<button type="button" class="print-button" title="打印"><svg xmlns="http://www.w3.org/2000/svg" class="icon print-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="print icon"><path d="M819.2 364.8h-44.8V128c0-17.067-14.933-32-32-32H281.6c-17.067 0-32 14.933-32 32v236.8h-44.8C145.067 364.8 96 413.867 96 473.6v192c0 59.733 49.067 108.8 108.8 108.8h44.8V896c0 17.067 14.933 32 32 32h460.8c17.067 0 32-14.933 32-32V774.4h44.8c59.733 0 108.8-49.067 108.8-108.8v-192c0-59.733-49.067-108.8-108.8-108.8zM313.6 160h396.8v204.8H313.6V160zm396.8 704H313.6V620.8h396.8V864zM864 665.6c0 25.6-19.2 44.8-44.8 44.8h-44.8V588.8c0-17.067-14.933-32-32-32H281.6c-17.067 0-32 14.933-32 32v121.6h-44.8c-25.6 0-44.8-19.2-44.8-44.8v-192c0-25.6 19.2-44.8 44.8-44.8h614.4c25.6 0 44.8 19.2 44.8 44.8v192z"></path></svg></button><div class="arrow end"></div></div><div class="vp-toc-wrapper"><ul class="vp-toc-list"><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#_0-0-未迁移依赖库汇总">0.0 未迁移依赖库汇总</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#_0-1-qa">0.1 QA</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#_0-2-迁移计划">0.2 迁移计划</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#_1-介绍">1. 介绍</a></li><li><ul class="vp-toc-list"><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#_1-1-安装-测试">1.1 安装&amp;测试</a></li><!----><!--]--></ul></li><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#_2-data-preprocess">2. Data Preprocess</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#_3-torchrec-benchmarks">3. Torchrec Benchmarks</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#_4-dlrm-benchmarks">4. DLRM Benchmarks</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#_6-dlrm-benchmarks测试结果">6. DLRM Benchmarks测试结果</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#_7-torchrec与hugectr比较">7. Torchrec与HugeCTR比较</a></li><!----><!--]--></ul><div class="vp-toc-marker" style="top:-1.7rem;"></div></div><!--[--><!----><!--]--></aside></div><!--[--><!----><!--]--><div class="theme-hope-content"><h1 id="torchrec调研" tabindex="-1"><a class="header-anchor" href="#torchrec调研"><span>Torchrec调研</span></a></h1><h2 id="_0-0-未迁移依赖库汇总" tabindex="-1"><a class="header-anchor" href="#_0-0-未迁移依赖库汇总"><span>0.0 未迁移依赖库汇总</span></a></h2><ol><li>torch <ul><li>torch.distributed._shard</li><li>torch.fx._compatibility</li><li>torch.distributed._composable</li><li>torch.distributed.optim._apply_optimizer_in_backward</li><li>torch.distributed.fsdp</li><li>torch._C.distributed_c10d.ProcessGroupNCCL</li><li>torch.distributed._composable.contract</li></ul></li><li>fbgemm-gpu</li></ol><h2 id="_0-1-qa" tabindex="-1"><a class="header-anchor" href="#_0-1-qa"><span>0.1 QA</span></a></h2><ul><li>参考资料： <ul><li>https://pytorch.org/torchrec/ （torchrec文档）</li><li>https://github.com/facebookresearch/dlrm/tree/main/torchrec_dlrm （dlrm）</li><li>https://blog.csdn.net/u013701860/article/details/51140762 （uvm）</li><li>https://www.sohu.com/a/560275515_100093134（整体介绍）</li><li>https://zhuanlan.zhihu.com/p/619060815（整体介绍）</li></ul></li></ul><ol><li><!----><ol><li>DMP（Distributed Model Parallel）， EBC（Embedding Bag Collections）, KJT(Keyed Jagged Tensor), Planner, Sharder等</li><li>主要是对模型并行，embedding切分方式以及自动制定分片计划，稀疏，量化等的实现</li></ol></li><li><!----><ol><li><p>使用EBC数据结构存储embedding</p></li><li><p>使用fused ebc（见后续数据结构对比介绍）</p></li><li><p>支持动态表</p><ol><li>动态表是cpu还是gpu实现的? <ol><li>CPU\GPU混合实现，torchrec早期版本不支持动态扩容，对超百亿规模模型无法支持，后续是微信团队开发动态表功能后22.9月整合进新版本（见<a href="https://zhuanlan.zhihu.com/p/619060815" target="_blank" rel="noopener noreferrer">介绍<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>）</li><li>也类似于hugectr的gpu cache机制，cpu维护一个映射表，映射不同emb在gpu中的分布情况，将常用的emb放显存中，未命中的从ps内存捞，使用的驱逐策略比较有意思，结合了LRU和LFU</li><li>dynamic emb目前是以torchrec_dynamic_embedding扩展库的形式整合进torchrec项目中的，在torchrec/contrib/dynamic_embedding下，需要单独编译；整合进torchrec/torchrec/csrc中的代码是从cotrib路径下迁过来的，添加了更多benchmarks和unittests，但是python api貌似还没开发完成，建议使用contrib路径下的源码安装扩展</li></ol></li><li>驱逐策略 <ol><li>使用LRU和LFU混合的驱逐策略, 最后记录在队列中的内容被驱逐</li><li>频次使用指数位记录，5bits，概率算法，每次取频次位随机数，全为零是频次指数加一</li><li>时间使用27bits记录</li><li>频次位在时间位前，LFU优先级比LRU高（值越小说明使用的越少，则换出）</li><li>使用队列批量驱逐显存中的emb</li><li><img src="/assets/evict_strategy-1683885152669-14-9LymrW2h.png" alt="evict_strategy" tabindex="0" loading="lazy"><figcaption>evict_strategy</figcaption></li></ol></li></ol></li><li><p>支持多种embedding切分方式：</p><p>row-wise, column-wise, table-wise</p><p>及三种方式的混合</p><ol><li>使用多种不同的分片方式的原因： <ol><li>在实际场景中，不同特征的访问频率是不同的，呈现幂率分布</li><li>若只使用row-wise，则每个特征域对应的emb table将会分布到不同gpu上，由于特征的幂率分布，从不同gpu中获取emb vector的规模可能会不均衡，影响通信性能</li><li>而使用col-wise，则每找一个特征对应的emb vector都要从所有gpu中获取数据拼接成完整的数据，使通信量均衡</li><li>用户可根据业务场景自定义分片方式</li></ol></li></ol></li></ol></li><li><!----><ol><li>实现了DMP，对网络sparse部分作模型并行，对dense部分作DDP</li><li>pipline：支持数据读取、分发、训练流水 <ol><li>torchrec.distributed.train_pipeline.TrainPipelineBase 使用两个流 <ul><li>the current (default) stream: 执行前线反向优化计算</li><li>self._memcpy_stream:执行input从host到GPU</li></ul></li><li>torchrec.distributed.train_pipeline.<strong>TrainPipelineSparseDist</strong>, 隐藏all2all延迟，同时保留前向/反向的训练顺序 <ul><li>stage 3: forward, backward - uses default CUDA stream</li><li>stage 2: ShardedModule.input_dist() - uses data_dist CUDA stream</li><li>stage 1: device transfer - uses memcpy CUDA stream</li></ul></li></ol></li></ol></li><li><!----><ol><li>传统推荐系统的ps架构 <ol><li><img src="/assets/72bada1fb3b644b68e0c32d0c2c0401e-BgkF98s3.png" alt="img" style="zoom:15%;"></li><li>以cpu为中心，ps做kv存储，worker在每个iter从ps捞所需emb vector，前向反向后将更新后的特征推送回ps</li><li>emb table在cpu上</li><li>训练过程需要等待通信，难以组成流水线，无法充分利用算力</li></ol></li><li>torchrec的ps架构 <ol><li><img src="/assets/5b33518e573e4d2793b492e1b1f428e0-D437P5NJ.png" alt="img" style="zoom:15%;"></li><li>以gpu为中心，利用gpu间高带宽通信交换所需emb vector（数据、模型混合并行），能减少与ps交换数据的通信开销</li><li>emb shards（emb table的子集）分布式存储在多个gpu上（或uvm内存中）（规模不算太大）</li><li>还可支持动态表，利用ps存储gpu放不下的emb（超大规模）</li></ol></li><li>ps和uvm的关系 <ol><li>uvm是在cpu内存中开辟一块空间当作是对gpu显存的扩展，cpu、gpu都能使用同样地指针访问其中的数据，便于开发，但实际上还是会做cpu、gpu间数据的隐式传输</li><li>使用uvm是在emb table规模不算特别大，但gpu显存无法完全放入所有emb table的情况下使用的，一定程度上扩展显存大小（逻辑大小）</li><li>而使用超大规模的emb table时，就需要ps存储更多的emb table，再配合动态emb和缓存来实现</li></ol></li></ol></li><li><!----><ol><li>uvm只是对显存一定程度的扩展，便于开发与维护，不能完全解决显存不够用的情况，而实际上还是会发生cpu、gpu间的隐式数据拷贝，甚至使用stream和cudaMemcpyAsync性能会比用uvm更高（参考<a href="https://blog.csdn.net/u013701860/article/details/51140762" target="_blank" rel="noopener noreferrer">链接<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>）</li><li>使用sharder是为了对embedding做模型并行，是为了摆脱以cpu为中心的传统ps架构，提高gpu使用性能，不同的分片方式也是为了解决显存无法存下所有emb table的问题</li><li>不同的分片方式是考量了不同的系统运行环境以及用户需求，以求最大化性能，个人理解uvm是为分片提供了辅助和简化开发维护，并不是只要用了uvm就能解决显存问题，就不需要模型并行能直接从uvm中读emb做数据并行了，这样反而退化为早期的ps架构，无法充分利用gpu高带宽</li></ol></li><li><!----><ol><li><p>EBC&amp;FusedEBC</p><ol><li><p>Embedding 和EmbeddingBag的区别</p><ol><li><p>使用embeddingbag主要是为了解决multi-hot的情况, 而且还能同时支持one-hot</p></li><li><p>embedding是直接查询出多个emb vec, 而embeddingbag是将查询出的多个emb vec做池化(此处为相加)后输出一个最终的emb vec, 这样就能解决multio-hot的情况(对于一个特征域,不但支持单选,还支持多选)</p><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code><span class="token keyword">import</span> torch

vocab_size <span class="token operator">=</span> <span class="token number">5</span> <span class="token comment"># feature_slot size</span>
embedding_dim <span class="token operator">=</span> <span class="token number">3</span>

<span class="token comment"># 大小相同的emb 和emb_bag</span>
embedding <span class="token operator">=</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>num_embeddings<span class="token operator">=</span>vocab_size<span class="token punctuation">,</span> embedding_dim<span class="token operator">=</span>embedding_dim<span class="token punctuation">)</span>
embedding_bag <span class="token operator">=</span> nn<span class="token punctuation">.</span>EmbeddingBag<span class="token punctuation">(</span>num_embeddings<span class="token operator">=</span>vocab_size<span class="token punctuation">,</span> embedding_dim<span class="token operator">=</span>embedding_dim<span class="token punctuation">,</span> mode<span class="token operator">=</span><span class="token string">&#39;sum&#39;</span><span class="token punctuation">)</span>

<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> embedding<span class="token punctuation">.</span>weight
Parameter containing<span class="token punctuation">:</span>
tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span> <span class="token number">0.2669</span><span class="token punctuation">,</span>  <span class="token number">0.0411</span><span class="token punctuation">,</span>  <span class="token number">1.8483</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">0.1264</span><span class="token punctuation">,</span>  <span class="token number">0.4678</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.7871</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">0.9744</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1.1333</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">2.0062</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span> <span class="token number">0.3138</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1.0656</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.6442</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span> <span class="token number">0.1350</span><span class="token punctuation">,</span>  <span class="token number">0.1416</span><span class="token punctuation">,</span>  <span class="token number">0.0687</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
        
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> embedding_bag<span class="token punctuation">.</span>weight
Parameter containing<span class="token punctuation">:</span>
tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">0.9410</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1.2599</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.5800</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1.3137</span><span class="token punctuation">,</span>  <span class="token number">0.2207</span><span class="token punctuation">,</span>  <span class="token number">0.1835</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span> <span class="token number">0.1689</span><span class="token punctuation">,</span>  <span class="token number">2.0827</span><span class="token punctuation">,</span>  <span class="token number">0.7237</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span> <span class="token number">0.2223</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.5492</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.6188</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span> <span class="token number">0.4136</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1.1578</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.7838</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
        
input_ids <span class="token operator">=</span> torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> embedding<span class="token punctuation">(</span>input_ids<span class="token punctuation">)</span>
tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">0.1264</span><span class="token punctuation">,</span>  <span class="token number">0.4678</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.7871</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
         <span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">0.9744</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1.1333</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">2.0062</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
         <span class="token punctuation">[</span> <span class="token number">0.3138</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1.0656</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.6442</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> grad_fn<span class="token operator">=</span><span class="token operator">&lt;</span>EmbeddingBackward0<span class="token operator">&gt;</span><span class="token punctuation">)</span>
         
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> embedding_bag<span class="token punctuation">(</span>input_ids<span class="token punctuation">)</span>
tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">0.9225</span><span class="token punctuation">,</span>  <span class="token number">1.7543</span><span class="token punctuation">,</span>  <span class="token number">0.2883</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> grad_fn<span class="token operator">=</span><span class="token operator">&lt;</span>EmbeddingBagBackward0<span class="token operator">&gt;</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></li></ol></li><li><p>EmbeddingBag和EmbeddingBagCollection的区别</p><ol><li><p>EBC用于管理多个EmbeddingBags</p><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code>B <span class="token operator">=</span> <span class="token number">2</span>
D <span class="token operator">=</span> <span class="token number">8</span>
dense_in_features <span class="token operator">=</span> <span class="token number">100</span>

eb1_config <span class="token operator">=</span> EmbeddingBagConfig<span class="token punctuation">(</span>
    name<span class="token operator">=</span><span class="token string">&quot;t1&quot;</span><span class="token punctuation">,</span> embedding_dim<span class="token operator">=</span>D<span class="token punctuation">,</span> num_embeddings<span class="token operator">=</span><span class="token number">100</span><span class="token punctuation">,</span> feature_names<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">&quot;f1&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;f3&quot;</span><span class="token punctuation">]</span>
<span class="token punctuation">)</span>
eb2_config <span class="token operator">=</span> EmbeddingBagConfig<span class="token punctuation">(</span>
    name<span class="token operator">=</span><span class="token string">&quot;t2&quot;</span><span class="token punctuation">,</span>
    embedding_dim<span class="token operator">=</span>D<span class="token punctuation">,</span>
    num_embeddings<span class="token operator">=</span><span class="token number">100</span><span class="token punctuation">,</span>
    feature_names<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">&quot;f2&quot;</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
<span class="token punctuation">)</span>

ebc <span class="token operator">=</span> EmbeddingBagCollection<span class="token punctuation">(</span>tables<span class="token operator">=</span><span class="token punctuation">[</span>eb1_config<span class="token punctuation">,</span> eb2_config<span class="token punctuation">]</span><span class="token punctuation">)</span>

<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> ebc
EmbeddingBagCollection<span class="token punctuation">(</span>
  <span class="token punctuation">(</span>embedding_bags<span class="token punctuation">)</span><span class="token punctuation">:</span> ModuleDict<span class="token punctuation">(</span>
    <span class="token punctuation">(</span>t1<span class="token punctuation">)</span><span class="token punctuation">:</span> EmbeddingBag<span class="token punctuation">(</span><span class="token number">100</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> mode<span class="token operator">=</span><span class="token string">&#39;sum&#39;</span><span class="token punctuation">)</span>
    <span class="token punctuation">(</span>t2<span class="token punctuation">)</span><span class="token punctuation">:</span> EmbeddingBag<span class="token punctuation">(</span><span class="token number">100</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> mode<span class="token operator">=</span><span class="token string">&#39;sum&#39;</span><span class="token punctuation">)</span>
  <span class="token punctuation">)</span>
<span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></li></ol></li><li><p>EBC和FusedEBC的区别</p><ol><li><img src="/assets/mmexport1683360092606-BksPmBkX.png" alt="mmexport1683360092606" style="zoom:25%;"></li><li><img src="/assets/mmexport1683360093936-BPDzbGUc.png" alt="mmexport1683360093936" style="zoom:25%;"></li><li>fused ebc是对普通ebc作了算子方面的融合优化（依赖fbgemm库），如，可使用一个kernel实现多个emb的查询</li><li>在查表时，常规的方法是一个warp内的所有线程连续读取一组待查询key，再从emb table中随机读取对应emb vector，无法充分利用显存带宽；</li><li>fused ebc的查表方式是使用cuda的shfl_sync，一个warp内的所有thread同时拷贝同一条emb vector，这样两个过程都是连续读</li></ol></li></ol></li><li><p>KJT</p><ol><li><p>kjt数据结构是在cpu还是gpu使用?为何会用到不同长度的tensor?</p><ol><li><p>使用不同长度的tensor是为了表示如multi-hot, 缺省值，每个batch中对应特征域特征出现的情况, 在cpu, gpu都会使用</p></li><li><p>torchrec中是使用EBC结构(基于torch的embeddingbag)存储多个特征域的嵌入表的, 然后使用kjt数据结构作为待查询key的张量, 传入ebc得到对应的嵌入向量</p><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code><span class="token comment"># 示例：</span>
<span class="token comment">#|------------|------------|</span>
<span class="token comment">#| product ID | user ID    |</span>
<span class="token comment">#|------------|------------|</span>
<span class="token comment">#| [101, 202] | [404]      |</span>
<span class="token comment">#| []         | [505]      |</span>
<span class="token comment">#| [303]      | [606]      |</span>
<span class="token comment">#|------------|------------|</span>

mb <span class="token operator">=</span> torchrec<span class="token punctuation">.</span>KeyedJaggedTensor<span class="token punctuation">(</span>
    keys <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">&quot;product&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;user&quot;</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    values <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">101</span><span class="token punctuation">,</span> <span class="token number">202</span><span class="token punctuation">,</span> <span class="token number">303</span><span class="token punctuation">,</span> <span class="token number">404</span><span class="token punctuation">,</span> <span class="token number">505</span><span class="token punctuation">,</span> <span class="token number">606</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    lengths <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>int64<span class="token punctuation">)</span><span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
<span class="token punctuation">)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span>mb<span class="token punctuation">.</span>to<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">&quot;cpu&quot;</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> KeyedJaggedTensor<span class="token punctuation">(</span><span class="token punctuation">{</span>
    <span class="token string">&quot;product&quot;</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">101</span><span class="token punctuation">,</span> <span class="token number">202</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">303</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    <span class="token string">&quot;user&quot;</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">404</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">505</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">606</span><span class="token punctuation">]</span><span class="token punctuation">]</span>
<span class="token punctuation">}</span><span class="token punctuation">)</span>


<span class="token comment"># sparse 参数存储</span>
eb1_config <span class="token operator">=</span> EmbeddingBagConfig<span class="token punctuation">(</span>
name<span class="token operator">=</span><span class="token string">&quot;t1&quot;</span><span class="token punctuation">,</span> embedding_dim<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> num_embeddings<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">,</span> feature_names<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">&quot;f1&quot;</span><span class="token punctuation">]</span>
<span class="token punctuation">)</span>
eb2_config <span class="token operator">=</span> EmbeddingBagConfig<span class="token punctuation">(</span>
name<span class="token operator">=</span><span class="token string">&quot;t2&quot;</span><span class="token punctuation">,</span> embedding_dim<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> num_embeddings<span class="token operator">=</span><span class="token number">12</span><span class="token punctuation">,</span> feature_names<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">&quot;f2&quot;</span><span class="token punctuation">]</span>
<span class="token punctuation">)</span>

ebc <span class="token operator">=</span> EmbeddingBagCollection<span class="token punctuation">(</span>tables<span class="token operator">=</span><span class="token punctuation">[</span>eb1_config<span class="token punctuation">]</span><span class="token punctuation">)</span>
sparse_arch <span class="token operator">=</span> SparseArch<span class="token punctuation">(</span>ebc<span class="token punctuation">)</span>

<span class="token comment">#     0       1        2  &lt;-- batch</span>
<span class="token comment"># 0   [0,1] None    [2]</span>
<span class="token comment"># 1   [3]    [4]    [5,6,7]</span>
<span class="token comment"># feature</span>

features <span class="token operator">=</span> KeyedJaggedTensor<span class="token punctuation">.</span>from_offsets_sync<span class="token punctuation">(</span>
   keys<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">&quot;f1&quot;</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
   values<span class="token operator">=</span>torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token number">6</span><span class="token punctuation">,</span><span class="token number">7</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
   offsets<span class="token operator">=</span>torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
   <span class="token punctuation">)</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> sparse_arch<span class="token punctuation">(</span>features<span class="token punctuation">)</span>
tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">0.0635</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.6799</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1.8670</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token punctuation">[</span> <span class="token number">0.0000</span><span class="token punctuation">,</span>  <span class="token number">0.0000</span><span class="token punctuation">,</span>  <span class="token number">0.0000</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">0.8205</span><span class="token punctuation">,</span>  <span class="token number">0.8911</span><span class="token punctuation">,</span>  <span class="token number">0.5688</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token punctuation">[</span> <span class="token number">1.7768</span><span class="token punctuation">,</span>  <span class="token number">0.3763</span><span class="token punctuation">,</span>  <span class="token number">0.9286</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1.6304</span><span class="token punctuation">,</span>  <span class="token number">1.8683</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.0427</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1.6682</span><span class="token punctuation">,</span>  <span class="token number">2.4401</span><span class="token punctuation">,</span>  <span class="token number">0.6767</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> grad_fn<span class="token operator">=</span><span class="token operator">&lt;</span>ReshapeAliasBackward0<span class="token operator">&gt;</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></li></ol></li></ol></li></ol></li><li><!----><ol><li>embedding分片方式的区别: <ul><li>hugectr的distributedslotembeddinghash是将每个slot划分到所有GPU上,相当于torchrec的row-wise sharding</li><li>hugectr的localizedslotembeddinghash是将某个完整的slot分配到某个GPU上,相当于torchrec的table-wise sharding</li><li>除此之外, torchrec还<strong>支持更多的切分方式</strong>, 满足用户定制化网络拓扑的需求, 且能够<strong>自动选择</strong>在当前运行环境下最优的分片方式</li></ul></li><li>在分布式上都支持<strong>数据模型混合并行</strong>, dense网络部分的处理都是数据并行, embedding都是模型并行, 且前向反向的计算过程也都在GPU上进行, 最大程度利用了GPU的并行计算性能</li><li>在数据加载上也都支持数据读取,分发,训练流水</li><li>数据预处理: <ul><li>torchrec是先补全缺省值,然后重新映射key id到连续id空间,再将出现次数少于阈值3的key映射到key_id=1,再做shuffle</li><li>hugectr也是先补全缺省值,重新映射到连续id, 再将出现次数少于阈值6的key映射到某一特定id,再做shuffle,**最后还可以使用feature cross(特征组合)**进一步减少特征数量,提升组合特征的表达能力</li></ul></li></ol></li><li><!----><ol><li>用在模型并行模块中, 为FSDP方式提供支持, 实际上只用在test脚本中, dlrm网络未用到</li></ol></li><li><!----><ol><li>简单总结,就是根据输入网络结构,参数规模, 运行的设备环境的拓扑,设备的带宽,显存, 数据类型等信息, 穷举所有切分方案,然后挑出所有可行的方案做一个模拟性能评估, 根据评估的结果再挑出性能最好的方案出来</li><li>执行流程&amp;细节： <ol><li>设置Topology, 生成ShardingPlan, Planner有两个阶段: <ol><li>Planning stage:在给定sharders和运行环境(Topology)的前提下决定如何切分模型,输出ShardingPlan, 会给出什么切分方式,使用什么compute kernel, perf, rank, 评估将会占用多少HBM存储空间 <ol><li>Topology类: 代表网络设备集群组织方式, 可设置设备类型, world size, 每个设备的存储空间(hbm,ddr)大小,带宽等.</li><li>Shard: emb的子表, 主要记录了size和offset</li><li>enumerator类用于列举所有可行的方案，之后estimator类再对不同的切分方式做perf /storage estimation, 之后将方案传入proposers按照不同策略对所有方案的评估性能作优劣排序, 最后选择最优的方案</li></ol></li><li>Sharding stage:根据ShardingPlan使用给定的sharder切分模型,需要在运行环境中执行 <ol><li>Partitioner, 用于切分shards, 有多种策略,如Greedy, BLDM, Linear等(目前只有Greedy的实现)</li></ol></li></ol></li><li>在rank0上制定plan然后broadcast到所有设备上</li><li>需要预留出kjt和dense部分的空间出来不考虑在shards占据的空间之内</li><li>预估embedding wall time perf 和峰值内存，是按照经验设定了一系列如带宽一类的常量来预估的（见constants.py），会计算每一个shards的性能</li><li>使用ParameterConstaints选择sharding类型提供pooling因子 <ol><li>能够帮助planner更准确的评估性能</li><li>用户设置一些sharding限制项来指导planner, 如,限定什么划分方式, 最少划分多少块, 使用什么计算kernel等</li></ol></li><li>超过GPU显存时自动触发UVM Caching</li></ol></li></ol></li><li><!----><ol><li>使用torch.utils.data.DataLoader从torchrec.datasets.criteo.InMemoryBinaryCriteoIterDataPipe中读取数据, 以torchrec.datasets.utils.Batch数据结构存入用于每个iter</li><li>没有像hugectr使用keyset list文件直接预先划分好每个pass需要使用什么emb的步骤</li></ol></li><li><!----><ol><li><p>torchrec对dlrm网络结构做了对应实现以及优化版本，主要实现了SparseArch, DenseArch, InteractionArch, OverArch， 分别对应网络结构中的Embedding, Bottom MLP, Pairwise interaction, concat&amp;Top MLP<img src="/assets/2023-05-06%2014-52-59%20%E7%9A%84%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE-1683884972652-10-_uxnbtuX.png" alt="2023-05-06 14-52-59 的屏幕截图" style="zoom:67%;"></p></li><li><p><a href="https://github.com/facebookresearch/dlrm/tree/main/torchrec_dlrm" target="_blank" rel="noopener noreferrer">项目<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>中有torchrec对应的网络训练脚本</p></li><li><p>首先读取网络参数，使用torch.distributed设置好网络拓扑, 后端可选nccl和gloo</p><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code>rank <span class="token operator">=</span> <span class="token builtin">int</span><span class="token punctuation">(</span>os<span class="token punctuation">.</span>environ<span class="token punctuation">[</span><span class="token string">&quot;LOCAL_RANK&quot;</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token keyword">if</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    device<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>device <span class="token operator">=</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f&quot;cuda:</span><span class="token interpolation"><span class="token punctuation">{</span>rank<span class="token punctuation">}</span></span><span class="token string">&quot;</span></span><span class="token punctuation">)</span>
    backend <span class="token operator">=</span> <span class="token string">&quot;nccl&quot;</span>
    torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>set_device<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
<span class="token keyword">else</span><span class="token punctuation">:</span>
    device<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>device <span class="token operator">=</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">&quot;cpu&quot;</span><span class="token punctuation">)</span>
    backend <span class="token operator">=</span> <span class="token string">&quot;gloo&quot;</span>

<span class="token keyword">if</span> rank <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>
        <span class="token string">&quot;PARAMS: (lr, batch_size, warmup_steps, decay_start, decay_steps): &quot;</span>
        <span class="token string-interpolation"><span class="token string">f&quot;</span><span class="token interpolation"><span class="token punctuation">{</span><span class="token punctuation">(</span>args<span class="token punctuation">.</span>learning_rate<span class="token punctuation">,</span> args<span class="token punctuation">.</span>batch_size<span class="token punctuation">,</span> args<span class="token punctuation">.</span>lr_warmup_steps<span class="token punctuation">,</span> args<span class="token punctuation">.</span>lr_decay_start<span class="token punctuation">,</span> args<span class="token punctuation">.</span>lr_decay_steps<span class="token punctuation">)</span><span class="token punctuation">}</span></span><span class="token string">&quot;</span></span>
    <span class="token punctuation">)</span>
dist<span class="token punctuation">.</span>init_process_group<span class="token punctuation">(</span>backend<span class="token operator">=</span>backend<span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></li><li><p>然后设置dataloader，定义模型结构，需要根据使用的数据集定义EBC，定义不同网络层的大小</p><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code>train_dataloader <span class="token operator">=</span> get_dataloader<span class="token punctuation">(</span>args<span class="token punctuation">,</span> backend<span class="token punctuation">,</span> <span class="token string">&quot;train&quot;</span><span class="token punctuation">)</span>
val_dataloader <span class="token operator">=</span> get_dataloader<span class="token punctuation">(</span>args<span class="token punctuation">,</span> backend<span class="token punctuation">,</span> <span class="token string">&quot;val&quot;</span><span class="token punctuation">)</span>
test_dataloader <span class="token operator">=</span> get_dataloader<span class="token punctuation">(</span>args<span class="token punctuation">,</span> backend<span class="token punctuation">,</span> <span class="token string">&quot;test&quot;</span><span class="token punctuation">)</span>

eb_configs <span class="token operator">=</span> <span class="token punctuation">[</span>
    EmbeddingBagConfig<span class="token punctuation">(</span>
        name<span class="token operator">=</span><span class="token string-interpolation"><span class="token string">f&quot;t_</span><span class="token interpolation"><span class="token punctuation">{</span>feature_name<span class="token punctuation">}</span></span><span class="token string">&quot;</span></span><span class="token punctuation">,</span>
        embedding_dim<span class="token operator">=</span>args<span class="token punctuation">.</span>embedding_dim<span class="token punctuation">,</span>
        num_embeddings<span class="token operator">=</span>none_throws<span class="token punctuation">(</span>args<span class="token punctuation">.</span>num_embeddings_per_feature<span class="token punctuation">)</span><span class="token punctuation">[</span>feature_idx<span class="token punctuation">]</span>
        <span class="token keyword">if</span> args<span class="token punctuation">.</span>num_embeddings <span class="token keyword">is</span> <span class="token boolean">None</span>
        <span class="token keyword">else</span> args<span class="token punctuation">.</span>num_embeddings<span class="token punctuation">,</span>
        feature_names<span class="token operator">=</span><span class="token punctuation">[</span>feature_name<span class="token punctuation">]</span><span class="token punctuation">,</span>
    <span class="token punctuation">)</span>
    <span class="token keyword">for</span> feature_idx<span class="token punctuation">,</span> feature_name <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>DEFAULT_CAT_NAMES<span class="token punctuation">)</span>
<span class="token punctuation">]</span>
sharded_module_kwargs <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>

dlrm_model <span class="token operator">=</span> DLRM<span class="token punctuation">(</span>
    embedding_bag_collection<span class="token operator">=</span>EmbeddingBagCollection<span class="token punctuation">(</span>
        tables<span class="token operator">=</span>eb_configs<span class="token punctuation">,</span> device<span class="token operator">=</span>torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">&quot;meta&quot;</span><span class="token punctuation">)</span>
    <span class="token punctuation">)</span><span class="token punctuation">,</span>
    dense_in_features<span class="token operator">=</span><span class="token builtin">len</span><span class="token punctuation">(</span>DEFAULT_INT_NAMES<span class="token punctuation">)</span><span class="token punctuation">,</span>
    dense_arch_layer_sizes<span class="token operator">=</span>args<span class="token punctuation">.</span>dense_arch_layer_sizes<span class="token punctuation">,</span>
    over_arch_layer_sizes<span class="token operator">=</span>args<span class="token punctuation">.</span>over_arch_layer_sizes<span class="token punctuation">,</span>
    dense_device<span class="token operator">=</span>device<span class="token punctuation">,</span>
<span class="token punctuation">)</span>
train_model <span class="token operator">=</span> DLRMTrain<span class="token punctuation">(</span>dlrm_model<span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></li><li><p>指定embedding_optimizer， 使用adagrad或sgd</p><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code>embedding_optimizer <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>Adagrad <span class="token keyword">if</span> args<span class="token punctuation">.</span>adagrad <span class="token keyword">else</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>SGD
apply_optimizer_in_backward<span class="token punctuation">(</span>
        embedding_optimizer<span class="token punctuation">,</span>
        train_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>sparse_arch<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        optimizer_kwargs<span class="token punctuation">,</span>
    <span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></li><li><p>定义planner，获取plan</p><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code>planner <span class="token operator">=</span> EmbeddingShardingPlanner<span class="token punctuation">(</span>
    topology<span class="token operator">=</span>Topology<span class="token punctuation">(</span>
        local_world_size<span class="token operator">=</span>get_local_size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        world_size<span class="token operator">=</span>dist<span class="token punctuation">.</span>get_world_size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        compute_device<span class="token operator">=</span>device<span class="token punctuation">.</span><span class="token builtin">type</span><span class="token punctuation">,</span>
    <span class="token punctuation">)</span><span class="token punctuation">,</span>
    batch_size<span class="token operator">=</span>args<span class="token punctuation">.</span>batch_size<span class="token punctuation">,</span>
    <span class="token comment"># If experience OOM, increase the percentage. see</span>
    <span class="token comment"># https://pytorch.org/torchrec/torchrec.distributed.planner.html#torchrec.distributed.planner.storage_reservations.HeuristicalStorageReservation</span>
    storage_reservation<span class="token operator">=</span>HeuristicalStorageReservation<span class="token punctuation">(</span>percentage<span class="token operator">=</span><span class="token number">0.05</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
<span class="token punctuation">)</span>
plan <span class="token operator">=</span> planner<span class="token punctuation">.</span>collective_plan<span class="token punctuation">(</span>
    train_model<span class="token punctuation">,</span> get_default_sharders<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> dist<span class="token punctuation">.</span>GroupMember<span class="token punctuation">.</span>WORLD
<span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></li><li><p>将模型结构、设备、plan传入DMP封装为model</p><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code>model <span class="token operator">=</span> DistributedModelParallel<span class="token punctuation">(</span>
        module<span class="token operator">=</span>train_model<span class="token punctuation">,</span>
        device<span class="token operator">=</span>device<span class="token punctuation">,</span>
        plan<span class="token operator">=</span>plan<span class="token punctuation">,</span>
    <span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></li><li><p>指定dense_optimizer后，再与embedding_optimizer合并为最终optimizer</p><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code>dense_optimizer <span class="token operator">=</span> KeyedOptimizerWrapper<span class="token punctuation">(</span>
    <span class="token builtin">dict</span><span class="token punctuation">(</span>in_backward_optimizer_filter<span class="token punctuation">(</span>model<span class="token punctuation">.</span>named_parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    optimizer_with_params<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
<span class="token punctuation">)</span>
optimizer <span class="token operator">=</span> CombinedOptimizer<span class="token punctuation">(</span><span class="token punctuation">[</span>model<span class="token punctuation">.</span>fused_optimizer<span class="token punctuation">,</span> dense_optimizer<span class="token punctuation">]</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></li><li><p>定义lr_scheduler</p><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code>lr_scheduler <span class="token operator">=</span> LRPolicyScheduler<span class="token punctuation">(</span>
        optimizer<span class="token punctuation">,</span> args<span class="token punctuation">.</span>lr_warmup_steps<span class="token punctuation">,</span> args<span class="token punctuation">.</span>lr_decay_start<span class="token punctuation">,</span> args<span class="token punctuation">.</span>lr_decay_steps
    <span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></li><li><p>训练</p><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code><span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>args<span class="token punctuation">.</span>epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>
    _train<span class="token punctuation">(</span>
        pipeline<span class="token punctuation">,</span>
        train_dataloader<span class="token punctuation">,</span>
        val_dataloader<span class="token punctuation">,</span>
        epoch<span class="token punctuation">,</span>
        lr_scheduler<span class="token punctuation">,</span>
        args<span class="token punctuation">.</span>print_lr<span class="token punctuation">,</span>
        args<span class="token punctuation">.</span>validation_freq_within_epoch<span class="token punctuation">,</span>
        args<span class="token punctuation">.</span>limit_train_batches<span class="token punctuation">,</span>
        args<span class="token punctuation">.</span>limit_val_batches<span class="token punctuation">,</span>
    <span class="token punctuation">)</span>
    val_auroc <span class="token operator">=</span> _evaluate<span class="token punctuation">(</span>args<span class="token punctuation">.</span>limit_val_batches<span class="token punctuation">,</span> pipeline<span class="token punctuation">,</span> val_dataloader<span class="token punctuation">,</span> <span class="token string">&quot;val&quot;</span><span class="token punctuation">)</span>
    results<span class="token punctuation">.</span>val_aurocs<span class="token punctuation">.</span>append<span class="token punctuation">(</span>val_auroc<span class="token punctuation">)</span>

test_auroc <span class="token operator">=</span> _evaluate<span class="token punctuation">(</span>args<span class="token punctuation">.</span>limit_test_batches<span class="token punctuation">,</span> pipeline<span class="token punctuation">,</span> test_dataloader<span class="token punctuation">,</span> <span class="token string">&quot;test&quot;</span><span class="token punctuation">)</span>
results<span class="token punctuation">.</span>test_auroc <span class="token operator">=</span> test_auroc
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ol><li><p>按batch读取数据进行训练</p><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code><span class="token keyword">def</span> <span class="token function">_train</span><span class="token punctuation">(</span>
    pipeline<span class="token punctuation">:</span> TrainPipelineSparseDist<span class="token punctuation">,</span>
    train_dataloader<span class="token punctuation">:</span> DataLoader<span class="token punctuation">,</span>
    val_dataloader<span class="token punctuation">:</span> DataLoader<span class="token punctuation">,</span>
    epoch<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">,</span>
    lr_scheduler<span class="token punctuation">,</span>
    print_lr<span class="token punctuation">:</span> <span class="token builtin">bool</span><span class="token punctuation">,</span>
    validation_freq<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span><span class="token builtin">int</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    limit_train_batches<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span><span class="token builtin">int</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    limit_val_batches<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span><span class="token builtin">int</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">&gt;</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">&quot;&quot;&quot;
    Trains model for 1 epoch. Helper function for train_val_test.

    Args:
        pipeline (TrainPipelineSparseDist): data pipeline.
        train_dataloader (DataLoader): Training set&#39;s dataloader.
        val_dataloader (DataLoader): Validation set&#39;s dataloader.
        epoch (int): The number of complete passes through the training set so far.
        lr_scheduler (LRPolicyScheduler): Learning rate scheduler.
        print_lr (bool): Whether to print the learning rate every training step.
        validation_freq (Optional[int]): The number of training steps between validation runs within an epoch.
        limit_train_batches (Optional[int]): Limits the training set to the first `limit_train_batches` batches.
        limit_val_batches (Optional[int]): Limits the validation set to the first `limit_val_batches` batches.

    Returns:
        None.
    &quot;&quot;&quot;</span>
    pipeline<span class="token punctuation">.</span>_model<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span>

    iterator <span class="token operator">=</span> itertools<span class="token punctuation">.</span>islice<span class="token punctuation">(</span><span class="token builtin">iter</span><span class="token punctuation">(</span>train_dataloader<span class="token punctuation">)</span><span class="token punctuation">,</span> limit_train_batches<span class="token punctuation">)</span>

    is_rank_zero <span class="token operator">=</span> dist<span class="token punctuation">.</span>get_rank<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">0</span>
    <span class="token keyword">if</span> is_rank_zero<span class="token punctuation">:</span>
        pbar <span class="token operator">=</span> tqdm<span class="token punctuation">(</span>
            <span class="token builtin">iter</span><span class="token punctuation">(</span><span class="token builtin">int</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            desc<span class="token operator">=</span><span class="token string-interpolation"><span class="token string">f&quot;Epoch </span><span class="token interpolation"><span class="token punctuation">{</span>epoch<span class="token punctuation">}</span></span><span class="token string">&quot;</span></span><span class="token punctuation">,</span>
            total<span class="token operator">=</span><span class="token builtin">len</span><span class="token punctuation">(</span>train_dataloader<span class="token punctuation">)</span><span class="token punctuation">,</span>
            disable<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>
        <span class="token punctuation">)</span>

    start_it <span class="token operator">=</span> <span class="token number">0</span>
    n <span class="token operator">=</span> <span class="token punctuation">(</span>
        validation_freq
        <span class="token keyword">if</span> validation_freq
        <span class="token keyword">else</span> limit_train_batches
        <span class="token keyword">if</span> limit_train_batches
        <span class="token keyword">else</span> <span class="token builtin">len</span><span class="token punctuation">(</span>train_dataloader<span class="token punctuation">)</span>
    <span class="token punctuation">)</span>
    <span class="token keyword">for</span> batched_iterator <span class="token keyword">in</span> batched<span class="token punctuation">(</span>iterator<span class="token punctuation">,</span> n<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">for</span> it <span class="token keyword">in</span> itertools<span class="token punctuation">.</span>count<span class="token punctuation">(</span>start_it<span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token keyword">try</span><span class="token punctuation">:</span>
                <span class="token keyword">if</span> is_rank_zero <span class="token keyword">and</span> print_lr<span class="token punctuation">:</span>
                    <span class="token keyword">for</span> i<span class="token punctuation">,</span> g <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>pipeline<span class="token punctuation">.</span>_optimizer<span class="token punctuation">.</span>param_groups<span class="token punctuation">)</span><span class="token punctuation">:</span>
                        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f&quot;lr: </span><span class="token interpolation"><span class="token punctuation">{</span>it<span class="token punctuation">}</span></span><span class="token string"> </span><span class="token interpolation"><span class="token punctuation">{</span>i<span class="token punctuation">}</span></span><span class="token string"> </span><span class="token interpolation"><span class="token punctuation">{</span>g<span class="token punctuation">[</span><span class="token string">&#39;lr&#39;</span><span class="token punctuation">]</span><span class="token punctuation">:</span><span class="token format-spec">.6f</span><span class="token punctuation">}</span></span><span class="token string">&quot;</span></span><span class="token punctuation">)</span>
                pipeline<span class="token punctuation">.</span>progress<span class="token punctuation">(</span>batched_iterator<span class="token punctuation">)</span>
                lr_scheduler<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
                <span class="token keyword">if</span> is_rank_zero<span class="token punctuation">:</span>
                    pbar<span class="token punctuation">.</span>update<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>
            <span class="token keyword">except</span> StopIteration<span class="token punctuation">:</span>
                <span class="token keyword">if</span> is_rank_zero<span class="token punctuation">:</span>
                    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;Total number of iterations:&quot;</span><span class="token punctuation">,</span> it<span class="token punctuation">)</span>
                start_it <span class="token operator">=</span> it
                <span class="token keyword">break</span>

        <span class="token keyword">if</span> validation_freq <span class="token keyword">and</span> start_it <span class="token operator">%</span> validation_freq <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>
            _evaluate<span class="token punctuation">(</span>limit_val_batches<span class="token punctuation">,</span> pipeline<span class="token punctuation">,</span> val_dataloader<span class="token punctuation">,</span> <span class="token string">&quot;val&quot;</span><span class="token punctuation">)</span>
            pipeline<span class="token punctuation">.</span>_model<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></li></ol></li></ol></li></ol><h2 id="_0-2-迁移计划" tabindex="-1"><a class="header-anchor" href="#_0-2-迁移计划"><span>0.2 迁移计划</span></a></h2><ol><li>将collection整合进框架中emb相关部分 <ol><li>torchrec的dynamic emb并非是真的动态表，只是实现了gpu cache用于存储更大的emb，并不能动态扩增gpu内表大小</li></ol></li><li>未支持依赖库解决方法 <ol><li>若pytorch组能提供临时版本支持就最好了</li><li>目前想到的方式是先将高版本pytorch中的对应依赖模块解耦出来放到现在的pytorch1.9版本中重新源码安装</li><li>或是现实现cpu版本</li></ol></li><li>迁移后DLRM网络性能测试、热点分析以及调优</li></ol><h2 id="_1-介绍" tabindex="-1"><a class="header-anchor" href="#_1-介绍"><span>1. 介绍</span></a></h2><ul><li><p>相关项目：</p><ul><li>https://github.com/pytorch/torchrec</li><li>https://github.com/facebookresearch/dlrm/tree/main/torchrec_dlrm</li></ul></li><li><p>TorchRec 是PyTorch下大规模推荐系统 (RecSys) 训练框架，能提供推荐系统所需的通用稀疏性和并行性原语，允许使用跨多个 GPU 分片的大型嵌入表来训练模型</p><ul><li>支持混合数据并行/模型并行，多设备/多节点训练</li><li>https://nvidia-merlin.github.io/HugeCTR/main/hugectr_embedding_training_cache.htmlhttps://nvidia-merlin.github.io/HugeCTR/main/hugectr_embedding_training_cache.htmlhttps://nvidia-merlin.github.io/HugeCTR/main/hugectr_embedding_training_cache.htmlhttps://nvidia-merlin.github.io/HugeCTR/main/hugectr_embedding_training_cache.htmlhttps://nvidia-merlin.github.io/HugeCTR/main/hugectr_embedding_training_cache.htmlhttps://nvidia-merlin.github.io/HugeCTR/main/hugectr_embedding_training_cache.htmlhttps://nvidia-merlin.github.io/HugeCTR/main/hugectr_embedding_training_cache.htmlhttps://nvidia-merlin.github.io/HugeCTR/main/hugectr_embedding_training_cache.html可以使用不同的分片策略对嵌入表进行分片，包括data-parallel, table-wise, row-wise, table-wise-row-wise, 和 column-wise sharding</li><li>TorchRec planner 可以自动为模型生成优化的分片计划</li><li>支持数据加载（复制到 GPU）、设备间通信和计算（前向、后向）重叠的流水线，以提高性能</li><li>由 FBGEMM 提供对RecSys的优化kernel，FBGEMM（Facebook 通用矩阵乘法）是用于服务器端推理的低精度、高性能矩阵-矩阵乘法和卷积库</li><li>支持降低精度的训练推理量化</li></ul></li><li><p>环境</p><div class="language-text line-numbers-mode" data-ext="text" data-title="text"><pre class="language-text"><code>Python &gt;= 3.7
CUDA &gt;= 11.0
nvcr.io/nvidia/pytorch:23.02-py3
nvcr.io/nvidia/pytorch:22.08-py3
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></li><li><p>依赖库</p><div class="language-txt line-numbers-mode" data-ext="txt" data-title="txt"><pre class="language-txt"><code>black
cmake
fbgemm-gpu-nightly
hypothesis
iopath
numpy
pandas
pyre-extensions
scikit-build
tabulate
torchmetrics
torchx
tqdm
usort
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></li></ul><h3 id="_1-1-安装-测试" tabindex="-1"><a class="header-anchor" href="#_1-1-安装-测试"><span>1.1 安装&amp;测试</span></a></h3><ul><li><div class="language-bash line-numbers-mode" data-ext="sh" data-title="sh"><pre class="language-bash"><code> <span class="token comment"># 建议使用pip安装</span>
 pip <span class="token function">install</span> torchrec_nightly --force-reinstall
 
 <span class="token comment"># 使用源码安装会出现fbgemm-gpu版本问题</span>
 pip <span class="token function">install</span> <span class="token parameter variable">-r</span> requirements.txt
 python setup.py <span class="token function">install</span> develop
 python setup.py --cpu-only <span class="token function">install</span> develop
 
 <span class="token comment"># 安装后测试</span>
 <span class="token comment"># torchx run -s local_cwd dist.ddp -j 1x2 --gpu 2 --script test_installation.py</span>
 
 <span class="token comment"># torchrun --nnodes 1 --nproc_per_node 2 --rdzv_backend c10d --rdzv_endpoint localhost --rdzv_id 54321 --role trainer test_installation.py</span>
 
 python <span class="token parameter variable">-m</span> torch.distributed.run <span class="token parameter variable">--nnodes</span> <span class="token number">1</span> <span class="token parameter variable">--nproc_per_node</span> <span class="token number">2</span> <span class="token parameter variable">--rdzv_backend</span> c10d <span class="token parameter variable">--rdzv_endpoint</span> localhost <span class="token parameter variable">--rdzv_id</span> <span class="token number">54321</span> <span class="token parameter variable">--role</span> trainer test_installation.py <span class="token parameter variable">--cpu_only</span>
 
 python <span class="token parameter variable">-m</span> torch.distributed.launch <span class="token punctuation">\</span>
     <span class="token parameter variable">--nproc_per_node</span> <span class="token number">2</span> <span class="token punctuation">\</span>
     <span class="token parameter variable">--nnodes</span> <span class="token number">1</span> <span class="token punctuation">\</span>
     <span class="token parameter variable">--node_rank</span> <span class="token number">0</span><span class="token punctuation">\</span>
     <span class="token parameter variable">--master_addr</span> localhost <span class="token punctuation">\</span>
     <span class="token parameter variable">--master_port</span> <span class="token number">54321</span><span class="token punctuation">\</span>
     --use-env test_installation.py
 
 <span class="token comment">#dlrm</span>
 python <span class="token parameter variable">-m</span> torch.distributed.run <span class="token parameter variable">--nnodes</span> <span class="token number">1</span> <span class="token parameter variable">--nproc_per_node</span> <span class="token number">2</span> <span class="token parameter variable">--rdzv_backend</span> c10d <span class="token parameter variable">--rdzv_endpoint</span> localhost <span class="token parameter variable">--rdzv_id</span> <span class="token number">54321</span> <span class="token parameter variable">--role</span> trainer dlrm_main.py
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></li><li><div class="language-bash line-numbers-mode" data-ext="sh" data-title="sh"><pre class="language-bash"><code> <span class="token comment"># pytorch安装位置:</span>
 /torch/venv3/pytorch/lib/python3.7/site-packages/torch
 <span class="token comment"># fbgemm安装位置:</span>
 /torch/venv3/pytorch/lib/python3.7/site-packages/fbgemm_gpu
 
 cmake  <span class="token parameter variable">-DCMAKE_PREFIX_PATH</span><span class="token operator">=</span>/torch/venv3/pytorch/lib/python3.7/site-packages/torch <span class="token punctuation">..</span> <span class="token operator">&amp;&amp;</span> <span class="token function">make</span> <span class="token parameter variable">-j</span>
 
 cmake  <span class="token parameter variable">-DCMAKE_PREFIX_PATH</span><span class="token operator">=</span>/opt/conda/lib/python3.8/site-packages/torch <span class="token punctuation">..</span> <span class="token operator">&amp;&amp;</span> <span class="token function">make</span> <span class="token parameter variable">-j</span>
 
 <span class="token comment"># fbgemm源码安装</span>
 <span class="token comment">#cmake -DUSE_SANITIZER=address -DFBGEMM_LIBRARY_TYPE=shared -DPYTHON_EXECUTABLE=/torch/venv3/pytorch/bin/python3 ..</span>
 
 <span class="token comment">#cmake -DUSE_SANITIZER=address -DFBGEMM_LIBRARY_TYPE=shared -DPYTHON_EXECUTABLE=/opt/conda/bin/python3 ..</span>
 
 <span class="token comment">#make -j VERBOSE=1    </span>
 
 <span class="token comment"># fbgemm-gpu源码安装:</span>
 <span class="token comment"># 安装conda</span>
 <span class="token assign-left variable">miniconda_prefix</span><span class="token operator">=</span><span class="token environment constant">$HOME</span>/miniconda
 <span class="token function">bash</span> miniconda.sh <span class="token parameter variable">-b</span> <span class="token parameter variable">-p</span> <span class="token string">&quot;<span class="token variable">$miniconda_prefix</span>&quot;</span> <span class="token parameter variable">-u</span>
 <span class="token builtin class-name">export</span> <span class="token assign-left variable"><span class="token environment constant">PATH</span></span><span class="token operator">=</span><span class="token variable">$miniconda_prefix</span>/bin:<span class="token environment constant">$PATH</span>
 conda update <span class="token parameter variable">-n</span> base <span class="token parameter variable">-c</span> defaults <span class="token parameter variable">-y</span> conda
 
 <span class="token assign-left variable">env_name</span><span class="token operator">=</span>fbgemm-install
 conda create <span class="token parameter variable">-y</span> <span class="token parameter variable">--name</span> <span class="token string">&quot;<span class="token variable">${env_name}</span>&quot;</span> <span class="token assign-left variable">python</span><span class="token operator">=</span><span class="token string">&quot;3.7&quot;</span>
 <span class="token builtin class-name">source</span> /root/miniconda/etc/profile.d/conda.sh
 
 <span class="token comment"># source /opt/conda/etc/profile.d/conda.sh</span>
 <span class="token comment"># conda activate fbgemm_install</span>
 conda run <span class="token parameter variable">-n</span> <span class="token string">&quot;<span class="token variable">${env_name}</span>&quot;</span> pip <span class="token function">install</span> <span class="token parameter variable">--upgrade</span> pip
 conda run <span class="token parameter variable">-n</span> <span class="token string">&quot;<span class="token variable">${env_name}</span>&quot;</span> python <span class="token parameter variable">-m</span> pip <span class="token function">install</span> pyOpenSSL<span class="token operator">&gt;</span><span class="token number">22.1</span>.0
 conda <span class="token function">install</span> <span class="token parameter variable">-n</span> <span class="token string">&quot;<span class="token variable">${env_name}</span>&quot;</span> <span class="token parameter variable">-y</span> gxx_linux-64<span class="token operator">=</span><span class="token number">10.4</span>.0 sysroot_linux-64<span class="token operator">=</span><span class="token number">2.17</span> <span class="token parameter variable">-c</span> conda-forge
 
 conda activate fbgemm_install
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></li></ul><h2 id="_2-data-preprocess" tabindex="-1"><a class="header-anchor" href="#_2-data-preprocess"><span>2. Data Preprocess</span></a></h2><ol><li><p>criteo-kaggle (7.8GB):</p><ul><li><p>下载与解压数据集</p><div class="language-bash line-numbers-mode" data-ext="sh" data-title="sh"><pre class="language-bash"><code><span class="token function">wget</span> http://go.criteo.net/criteo-research-kaggle-display-advertising-challenge-dataset.tar.gz <span class="token operator">&amp;&amp;</span> /
<span class="token function">tar</span> zxvf criteo-research-kaggle-display-advertising-challenge-dataset.tar.gz
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div></li><li><p>预处理数据集（需要70GB内存）</p></li></ul><div class="language-bash line-numbers-mode" data-ext="sh" data-title="sh"><pre class="language-bash"><code>python <span class="token parameter variable">-m</span> torchrec.datasets.scripts.npy_preproc_criteo <span class="token parameter variable">--input_dir</span> <span class="token variable">$INPUT_PATH</span> <span class="token parameter variable">--output_dir</span> <span class="token variable">$OUTPUT_PATH</span> <span class="token parameter variable">--dataset_name</span> criteo_kaggle
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div></li><li><p>criteo-1t (655GB):</p><ul><li><p>脚本见项目dlrm https://github.com/facebookresearch/dlrm/blob/main/torchrec_dlrm/scripts/process_Criteo_1TB_Click_Logs_dataset.sh</p><div class="language-bash line-numbers-mode" data-ext="sh" data-title="sh"><pre class="language-bash"><code><span class="token function">git</span> clone <span class="token parameter variable">--recursive</span> https://github.com/facebookresearch/dlrm.git
 
<span class="token builtin class-name">cd</span> ./dlrm/torchrec_dlrm/scripts <span class="token operator">&amp;&amp;</span> <span class="token punctuation">\</span>
<span class="token function">bash</span> ./process_Criteo_1TB_Click_Logs_dataset.sh /workspace/dataset/favorite/modelzoo-datasets/v1/criteo_terybyte/ /workspace/volume/<span class="token punctuation">[</span>your-workspace<span class="token punctuation">]</span>/criteo_terabyte/intermediate /workspace/volume/<span class="token punctuation">[</span>your-workspace<span class="token punctuation">]</span>/criteo_terabyte/output
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></li><li><p>处理方法:</p><ol><li><p>将tsv文件转为npy文件，划分为dense，sparse，label三个npy文件（需要320GB内存）</p><ul><li><p><a href="https://ailab.criteo.com/download-criteo-1tb-click-logs-dataset/" target="_blank" rel="noopener noreferrer">原始数据类型<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>：每条数据1个label（是否被点击），13个dense特征（int型，多为计数值），26个sparse特征（经hash为32bits数据）</p></li><li><p>每行数据格式： [label] [integer feature 1] … [integer feature 13] [categorical feature 1] … [categorical feature 26]</p></li><li><p>torchrec处理dense特征的方式，将整型dense特征转为大于1的float32型</p><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code><span class="token comment"># PyTorch tensors can&#39;t handle uint32, but we can save space by not using int64. Numpy will automatically handle dense values &gt;= 2 ** 31.</span>
dense_np <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>dense<span class="token punctuation">,</span> dtype<span class="token operator">=</span>np<span class="token punctuation">.</span>int32<span class="token punctuation">)</span>
sparse_np <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>sparse<span class="token punctuation">,</span> dtype<span class="token operator">=</span>np<span class="token punctuation">.</span>int32<span class="token punctuation">)</span>
labels_np <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>labels<span class="token punctuation">,</span> dtype<span class="token operator">=</span>np<span class="token punctuation">.</span>int32<span class="token punctuation">)</span>

<span class="token comment"># Log is expensive to compute at runtime.</span>
dense_np <span class="token operator">+=</span> <span class="token number">3</span>
dense_np <span class="token operator">=</span> np<span class="token punctuation">.</span>log<span class="token punctuation">(</span>dense_np<span class="token punctuation">,</span> dtype<span class="token operator">=</span>np<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>

<span class="token comment"># To be consistent with dense and sparse.</span>
labels_np <span class="token operator">=</span> labels_np<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></li></ul></li><li><p>将sparse特征处理为contiguous特征（需要480GB内存）</p><ul><li><p>需要所有24天的数据同时输入脚本处理</p></li><li><p>在所有文件中分别统计每一个特征域出现过的特征的出现频率到sparse_to_frequency中</p></li><li><p>将出现频率低于frequency_threshold的特征值都映射为1，其余特征值映射为从2开始的连续值（可能出现频率低的特征对最后label的影响小，所以统一处理为1，能够减小特征域大小？）</p><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code><span class="token comment"># Iterate through each row in each file for the current column and remap each</span>
<span class="token comment"># sparse id to a contiguous id. The contiguous ints start at a value of 2 so that</span>
<span class="token comment"># infrequenct IDs (determined by the frequency_threshold) can be remapped to 1.</span>
running_sum <span class="token operator">=</span> <span class="token number">2</span>
sparse_to_contiguous_int<span class="token punctuation">:</span> Dict<span class="token punctuation">[</span><span class="token builtin">int</span><span class="token punctuation">,</span> <span class="token builtin">int</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>

<span class="token keyword">for</span> f <span class="token keyword">in</span> file_to_features<span class="token punctuation">:</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f&quot;Processing file: </span><span class="token interpolation"><span class="token punctuation">{</span>f<span class="token punctuation">}</span></span><span class="token string">&quot;</span></span><span class="token punctuation">)</span>

    <span class="token keyword">for</span> i<span class="token punctuation">,</span> sparse <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>file_to_features<span class="token punctuation">[</span>f<span class="token punctuation">]</span><span class="token punctuation">[</span>col<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">if</span> sparse <span class="token keyword">not</span> <span class="token keyword">in</span> sparse_to_contiguous_int<span class="token punctuation">:</span>
            <span class="token comment"># If the ID appears less than frequency_threshold amount of times</span>
            <span class="token comment"># remap the value to 1.</span>
            <span class="token keyword">if</span> <span class="token punctuation">(</span>
                frequency_threshold <span class="token operator">&gt;</span> <span class="token number">1</span>
                <span class="token keyword">and</span> sparse_to_frequency<span class="token punctuation">[</span>sparse<span class="token punctuation">]</span> <span class="token operator">&lt;</span> frequency_threshold
            <span class="token punctuation">)</span><span class="token punctuation">:</span>
                sparse_to_contiguous_int<span class="token punctuation">[</span>sparse<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">1</span>
            <span class="token keyword">else</span><span class="token punctuation">:</span>
                sparse_to_contiguous_int<span class="token punctuation">[</span>sparse<span class="token punctuation">]</span> <span class="token operator">=</span> running_sum
                running_sum <span class="token operator">+=</span> <span class="token number">1</span>

        <span class="token comment"># Re-map sparse value to contiguous in place.</span>
        file_to_features<span class="token punctuation">[</span>f<span class="token punctuation">]</span><span class="token punctuation">[</span>col<span class="token punctuation">]</span><span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> sparse_to_contiguous_int<span class="token punctuation">[</span>sparse<span class="token punctuation">]</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></li></ul></li><li><p>shuffle（需要700GB内存）</p><ul><li>day0-day22做训练集，shuffle</li><li>day23做验证集，不做shuffle</li></ul></li></ol></li></ul></li><li><p>criteo-multihot (3.5TB):</p><ul><li><p>利用之前处理好的criteo-1t数据集, 合成multi-hot数据集, 用于MLPerf DLRM v2 benchmark (需要200GB内存)</p><div class="language-bash line-numbers-mode" data-ext="sh" data-title="sh"><pre class="language-bash"><code>python materialize_synthetic_multihot_dataset.py <span class="token punctuation">\</span>
    <span class="token parameter variable">--in_memory_binary_criteo_path</span> <span class="token variable">$PREPROCESSED_CRITEO_1TB_CLICK_LOGS_DATASET_PATH</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--output_path</span> <span class="token variable">$MATERIALIZED_DATASET_PATH</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--num_embeddings_per_feature</span> <span class="token number">40000000,39060</span>,17295,7424,20265,3,7122,1543,63,40000000,3067956,405282,10,2209,11938,155,4,976,14,40000000,40000000,40000000,590152,12973,108,36 <span class="token punctuation">\</span>
    <span class="token parameter variable">--multi_hot_sizes</span> <span class="token number">3,2</span>,1,2,6,1,1,1,1,7,3,8,1,6,9,5,1,1,1,12,100,27,10,3,1,1 <span class="token punctuation">\</span>
    <span class="token parameter variable">--multi_hot_distribution_type</span> uniform
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></li></ul></li></ol><h2 id="_3-torchrec-benchmarks" tabindex="-1"><a class="header-anchor" href="#_3-torchrec-benchmarks"><span>3. Torchrec Benchmarks</span></a></h2><ul><li><p>benchmark对两种EmbeddingBagCollection模型进行比较</p><ul><li><code>EmbeddingBagCollection</code> (EBC) (<a href="https://pytorch.org/torchrec/torchrec.modules.html#torchrec.modules.embedding_modules.EmbeddingBagCollection" target="_blank" rel="noopener noreferrer">code<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>): 由 <a href="https://pytorch.org/docs/stable/generated/torch.nn.EmbeddingBag.html" target="_blank" rel="noopener noreferrer">torch.nn.EmbeddingBag<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>支持</li><li><code>FusedEmbeddingBagCollection</code> (Fused EBC) (<a href="https://github.com/pytorch/torchrec/blob/main/torchrec/modules/fused_embedding_bag_collection.py#L299" target="_blank" rel="noopener noreferrer">code<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>): 由<a href="https://github.com/pytorch/FBGEMM" target="_blank" rel="noopener noreferrer">FBGEMM<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> kernels 支持，配备了融合优化器和 UVM 或UVM Caching，可为 GPU 提供更大的内存 <ul><li>UVM： <ul><li>能够被CPU或GPU访问的host内存地址空间，使用cudaMalloManaged()分配内存</li><li>使用UVM能够分配超过显存大小的更多内存，存下更大的嵌入表</li><li>以page为粒度获取Embedding Table</li></ul></li><li>UVM caching： <ul><li>以Embedding row为粒度获取embedding</li><li>使用software managed cache管理，如果GPU miss，则从内存中调用这个row到GPU显存中</li><li>可设置caching ratio管理缓存大小占整个Embedding Table的比例</li></ul></li></ul></li></ul></li><li><p>run</p><div class="language-bash line-numbers-mode" data-ext="sh" data-title="sh"><pre class="language-bash"><code><span class="token builtin class-name">cd</span> benchmarks
<span class="token comment"># modify ebc_benchmarks.py line14: from torchrec.github.benchmarks import ebc_benchmarks_utils -&gt; import ebc_benchmarks_utils</span>
<span class="token comment"># mode: ebc_comparison_dlrm (default) / fused_ebc_uvm / ebc_comparison_scaling</span>
python ebc_benchmarks.py <span class="token punctuation">[</span>--mode MODE<span class="token punctuation">]</span> <span class="token punctuation">[</span>--cpu_only<span class="token punctuation">]</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></li><li><p>结论：</p><ul><li>使用了UVM 或UVM caching的FusedEBC相比EBC模型具有更快的性能表现，且FusedEBC支持超过显存大小的Embedding<img src="/assets/EBC_benchmarks_dlrm_emb-SuBKNH8_.png" alt="" loading="lazy"></li></ul></li></ul><h2 id="_4-dlrm-benchmarks" tabindex="-1"><a class="header-anchor" href="#_4-dlrm-benchmarks"><span>4. DLRM Benchmarks</span></a></h2><ul><li><p>MLPerf DLRM v1 benchmark</p><ul><li><p>使用DLRM项目下的torchrec_dlrm</p><div class="language-bash line-numbers-mode" data-ext="sh" data-title="sh"><pre class="language-bash"><code><span class="token function">git</span> clone <span class="token parameter variable">--recursive</span> https://github.com/facebookresearch/dlrm.git
<span class="token builtin class-name">cd</span> dlrm/torchrec_dlrm
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div></li><li><p>预处理数据，处理方法见3小节</p></li><li><p>run</p><div class="language-bash line-numbers-mode" data-ext="sh" data-title="sh"><pre class="language-bash"><code><span class="token builtin class-name">export</span> <span class="token assign-left variable">PREPROCESSED_DATASET</span><span class="token operator">=</span><span class="token variable">$insert_your_path_here</span>
<span class="token builtin class-name">export</span> <span class="token assign-left variable">TOTAL_TRAINING_SAMPLES</span><span class="token operator">=</span><span class="token number">4195197692</span> <span class="token punctuation">;</span>
<span class="token builtin class-name">export</span> <span class="token assign-left variable">GLOBAL_BATCH_SIZE</span><span class="token operator">=</span><span class="token number">16384</span> <span class="token punctuation">;</span>
<span class="token builtin class-name">export</span> <span class="token assign-left variable">WORLD_SIZE</span><span class="token operator">=</span><span class="token number">8</span> <span class="token punctuation">;</span>
torchx run <span class="token parameter variable">-s</span> local_cwd dist.ddp <span class="token parameter variable">-j</span> 1x8 <span class="token parameter variable">--script</span> dlrm_main.py -- <span class="token punctuation">\</span>
    <span class="token parameter variable">--embedding_dim</span> <span class="token number">128</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--dense_arch_layer_sizes</span> <span class="token number">512,256</span>,128 <span class="token punctuation">\</span>
    <span class="token parameter variable">--over_arch_layer_sizes</span> <span class="token number">1024,1024</span>,512,256,1 <span class="token punctuation">\</span>
    <span class="token parameter variable">--in_memory_binary_criteo_path</span> <span class="token variable">$PREPROCESSED_DATASET</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--num_embeddings_per_feature</span> <span class="token number">40000000,39060</span>,17295,7424,20265,3,7122,1543,63,40000000,3067956,405282,10,2209,11938,155,4,976,14,40000000,40000000,40000000,590152,12973,108,36 <span class="token punctuation">\</span>
    <span class="token parameter variable">--validation_freq_within_epoch</span> <span class="token variable"><span class="token variable">$((</span>TOTAL_TRAINING_SAMPLES <span class="token operator">/</span> <span class="token punctuation">(</span>GLOBAL_BATCH_SIZE <span class="token operator">*</span> <span class="token number">20</span><span class="token variable">))</span></span><span class="token punctuation">)</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--epochs</span> <span class="token number">1</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--pin_memory</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--mmap_mode</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--batch_size</span> <span class="token variable"><span class="token variable">$((</span>GLOBAL_BATCH_SIZE <span class="token operator">/</span> WORLD_SIZE<span class="token variable">))</span></span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--learning_rate</span> <span class="token number">1.0</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></li></ul></li><li></li><li><p>MLPerf DLRM v2 benchmark</p><ul><li><p>使用DLRM项目下的torchrec_dlrm</p><div class="language-bash line-numbers-mode" data-ext="sh" data-title="sh"><pre class="language-bash"><code><span class="token function">git</span> clone <span class="token parameter variable">--recursive</span> https://github.com/facebookresearch/dlrm.git
<span class="token builtin class-name">cd</span> dlrm/torchrec_dlrm
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div></li><li><p>预处理数据，处理方法见3小节</p></li><li><p>run（使用合成multi-hot数据，3.8TB）</p><div class="language-bash line-numbers-mode" data-ext="sh" data-title="sh"><pre class="language-bash"><code><span class="token builtin class-name">export</span> <span class="token assign-left variable">MULTIHOT_PREPROCESSED_DATASET</span><span class="token operator">=</span><span class="token variable">$your_path_here</span>
<span class="token builtin class-name">export</span> <span class="token assign-left variable">TOTAL_TRAINING_SAMPLES</span><span class="token operator">=</span><span class="token number">4195197692</span> <span class="token punctuation">;</span>
<span class="token builtin class-name">export</span> <span class="token assign-left variable">GLOBAL_BATCH_SIZE</span><span class="token operator">=</span><span class="token number">65536</span> <span class="token punctuation">;</span>
<span class="token builtin class-name">export</span> <span class="token assign-left variable">WORLD_SIZE</span><span class="token operator">=</span><span class="token number">8</span> <span class="token punctuation">;</span>
torchx run <span class="token parameter variable">-s</span> local_cwd dist.ddp <span class="token parameter variable">-j</span> 1x8 <span class="token parameter variable">--script</span> dlrm_main.py -- <span class="token punctuation">\</span>
    <span class="token parameter variable">--embedding_dim</span> <span class="token number">128</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--dense_arch_layer_sizes</span> <span class="token number">512,256</span>,128 <span class="token punctuation">\</span>
    <span class="token parameter variable">--over_arch_layer_sizes</span> <span class="token number">1024,1024</span>,512,256,1 <span class="token punctuation">\</span>
    <span class="token parameter variable">--synthetic_multi_hot_criteo_path</span> <span class="token variable">$MULTIHOT_PREPROCESSED_DATASET</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--num_embeddings_per_feature</span> <span class="token number">40000000,39060</span>,17295,7424,20265,3,7122,1543,63,40000000,3067956,405282,10,2209,11938,155,4,976,14,40000000,40000000,40000000,590152,12973,108,36 <span class="token punctuation">\</span>
    <span class="token parameter variable">--validation_freq_within_epoch</span> <span class="token variable"><span class="token variable">$((</span>TOTAL_TRAINING_SAMPLES <span class="token operator">/</span> <span class="token punctuation">(</span>GLOBAL_BATCH_SIZE <span class="token operator">*</span> <span class="token number">20</span><span class="token variable">))</span></span><span class="token punctuation">)</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--epochs</span> <span class="token number">1</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--pin_memory</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--mmap_mode</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--batch_size</span> <span class="token variable"><span class="token variable">$((</span>GLOBAL_BATCH_SIZE <span class="token operator">/</span> WORLD_SIZE<span class="token variable">))</span></span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--interaction_type</span><span class="token operator">=</span>dcn <span class="token punctuation">\</span>
    <span class="token parameter variable">--dcn_num_layers</span><span class="token operator">=</span><span class="token number">3</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--dcn_low_rank_dim</span><span class="token operator">=</span><span class="token number">512</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--adagrad</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--learning_rate</span> <span class="token number">0.005</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></li><li><p>run（使用预处理后的criteo-1t数据集动态生成的multi-hot数据，存储空间不足3.8TB时可用）</p><div class="language-bash line-numbers-mode" data-ext="sh" data-title="sh"><pre class="language-bash"><code><span class="token builtin class-name">export</span> <span class="token assign-left variable">PREPROCESSED_DATASET</span><span class="token operator">=</span><span class="token variable">$insert_your_path_here</span>
<span class="token builtin class-name">export</span> <span class="token assign-left variable">TOTAL_TRAINING_SAMPLES</span><span class="token operator">=</span><span class="token number">4195197692</span> <span class="token punctuation">;</span>
<span class="token builtin class-name">export</span> <span class="token assign-left variable">BATCHSIZE</span><span class="token operator">=</span><span class="token number">65536</span> <span class="token punctuation">;</span>
<span class="token builtin class-name">export</span> <span class="token assign-left variable">WORLD_SIZE</span><span class="token operator">=</span><span class="token number">8</span> <span class="token punctuation">;</span>
torchx run <span class="token parameter variable">-s</span> local_cwd dist.ddp <span class="token parameter variable">-j</span> 1x8 <span class="token parameter variable">--script</span> dlrm_main.py -- <span class="token punctuation">\</span>
    <span class="token parameter variable">--embedding_dim</span> <span class="token number">128</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--dense_arch_layer_sizes</span> <span class="token number">512,256</span>,128 <span class="token punctuation">\</span>
    <span class="token parameter variable">--over_arch_layer_sizes</span> <span class="token number">1024,1024</span>,512,256,1 <span class="token punctuation">\</span>
    <span class="token parameter variable">--in_memory_binary_criteo_path</span> <span class="token variable">$PREPROCESSED_DATASET</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--num_embeddings_per_feature</span> <span class="token number">40000000,39060</span>,17295,7424,20265,3,7122,1543,63,40000000,3067956,405282,10,2209,11938,155,4,976,14,40000000,40000000,40000000,590152,12973,108,36 <span class="token punctuation">\</span>
    <span class="token parameter variable">--validation_freq_within_epoch</span> <span class="token variable"><span class="token variable">$((</span>TOTAL_TRAINING_SAMPLES <span class="token operator">/</span> <span class="token punctuation">(</span>BATCHSIZE <span class="token operator">*</span> <span class="token number">20</span><span class="token variable">))</span></span><span class="token punctuation">)</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--epochs</span> <span class="token number">1</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--pin_memory</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--mmap_mode</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--batch_size</span> <span class="token variable"><span class="token variable">$((</span>GLOBAL_BATCH_SIZE <span class="token operator">/</span> WORLD_SIZE<span class="token variable">))</span></span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--interaction_type</span><span class="token operator">=</span>dcn <span class="token punctuation">\</span>
    <span class="token parameter variable">--dcn_num_layers</span><span class="token operator">=</span><span class="token number">3</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--dcn_low_rank_dim</span><span class="token operator">=</span><span class="token number">512</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--adagrad</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--learning_rate</span> <span class="token number">0.005</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--multi_hot_distribution_type</span> uniform <span class="token punctuation">\</span>
    <span class="token parameter variable">--multi_hot_sizes</span><span class="token operator">=</span><span class="token number">3,2</span>,1,2,6,1,1,1,1,7,3,8,1,6,9,5,1,1,1,12,100,27,10,3,1,1

</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></li></ul></li><li><p>MLPerf DLRM Benchmark v1 &amp; v2比较：</p><ul><li><table><thead><tr><th></th><th>DLRM v1</th><th>DLRM v2</th></tr></thead><tbody><tr><td><strong>Optimizer</strong></td><td>SGD</td><td>Adagrad</td></tr><tr><td><strong>Learning Rate</strong></td><td>1.0</td><td>0.005</td></tr><tr><td><strong>Batch size</strong></td><td>AUC 0.8025 within 1 epoch using 16384</td><td>AUC 0.8025 within 1 epoch using 65536</td></tr><tr><td><strong>Interaction Layer</strong></td><td>dot interaction</td><td>DCN V2 with low rank approximation</td></tr><tr><td><strong>Dataset</strong></td><td>Criteo 1TB Click Logs Dataset, but uses a different preprocessing script (<a href="https://github.com/facebookresearch/dlrm/blob/main/data_utils.py" target="_blank" rel="noopener noreferrer">repo_root/data_utils.py<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>)</td><td>Criteo 1TB Click Logs Dataset but the sparse features are replaced with a multi-hot dataset.</td></tr></tbody></table></li></ul></li><li><p>使用Criteo Kaggle 数据集， 默认网络参数</p><ul><li><p>使用DLRM项目下的torchrec_dlrm</p><div class="language-bash line-numbers-mode" data-ext="sh" data-title="sh"><pre class="language-bash"><code><span class="token function">git</span> clone <span class="token parameter variable">--recursive</span> https://github.com/facebookresearch/dlrm.git
<span class="token builtin class-name">cd</span> dlrm/torchrec_dlrm
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div></li><li><p>预处理数据，处理方法见3小节</p></li><li><p>run</p><div class="language-bash line-numbers-mode" data-ext="sh" data-title="sh"><pre class="language-bash"><code><span class="token comment"># modify dlrm/torchrec_dlrm/data/dlrm_dataloader.py line87:</span>
<span class="token comment"># (root_name, stage) = (&quot;train&quot;, &quot;test&quot;) if stage == &quot;val&quot; else stage</span>
<span class="token comment"># -&gt; (root_name, stage) = (&quot;train&quot;, &quot;test&quot;) if stage == &quot;val&quot; else (stage, stage)</span>
<span class="token builtin class-name">export</span> <span class="token assign-left variable">PREPROCESSED_DATASET</span><span class="token operator">=</span><span class="token string">&quot;/workspace/volume/torchrec-criteo-datasets/criteo-kaggle&quot;</span>
<span class="token builtin class-name">export</span> <span class="token assign-left variable">GLOBAL_BATCH_SIZE</span><span class="token operator">=</span><span class="token number">16384</span> <span class="token punctuation">;</span>
<span class="token builtin class-name">export</span> <span class="token assign-left variable">WORLD_SIZE</span><span class="token operator">=</span><span class="token number">8</span> <span class="token punctuation">;</span>
torchx run <span class="token parameter variable">-s</span> local_cwd dist.ddp <span class="token parameter variable">-j</span> 1x8 <span class="token parameter variable">--script</span> dlrm_main.py -- <span class="token punctuation">\</span>
    <span class="token parameter variable">--in_memory_binary_criteo_path</span> <span class="token variable">$PREPROCESSED_DATASET</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--pin_memory</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--mmap_mode</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--batch_size</span> <span class="token variable"><span class="token variable">$((</span>GLOBAL_BATCH_SIZE <span class="token operator">/</span> WORLD_SIZE<span class="token variable">))</span></span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--learning_rate</span> <span class="token number">1.0</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--dataset_name</span> criteo_kaggle
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></li></ul></li></ul><h2 id="_6-dlrm-benchmarks测试结果" tabindex="-1"><a class="header-anchor" href="#_6-dlrm-benchmarks测试结果"><span>6. DLRM Benchmarks测试结果</span></a></h2><ul><li><p>测试环境:</p><table><thead><tr><th>distributed-training/torchrec:pytorch22.08-py3</th></tr></thead><tbody><tr><td>8*A100-SXM4-80GB</td></tr></tbody></table><ul><li><p>MLPerf DLRM v1 benchmark, 1 epoch</p><ul><li>AUROC over val set: <strong>0.8004854917526245</strong></li><li>AUROC over test set: <strong>0.7949966788291931</strong></li></ul></li><li><p>MLPerf DLRM v2 benchmark, 1 epoch</p><ul><li>AUROC over val set: <strong>0.8040649890899658</strong></li><li>AUROC over test set: <strong>0.7980538010597229</strong></li></ul></li><li><p>使用Criteo Kaggle 数据集， 默认网络参数, 1 epoch</p><ul><li><p>AUROC over val set: <strong>0.5002527236938477</strong></p></li><li><p>torchrec-dlrm项目原来不支持criteo-kaggle数据集，今年2月增加支持</p></li><li><p>因为criteo-kaggle数据集没有验证集，只能从训练集中划分一部分做验证集</p></li><li><p>原代码加载数据集时报错：</p><div class="language-bash line-numbers-mode" data-ext="sh" data-title="sh"><pre class="language-bash"><code>File <span class="token string">&quot;./dlrm/torchrec_dlrm/data/dlrm_dataloader.py&quot;</span>, line <span class="token number">87</span>, <span class="token keyword">in</span> _get_in_memory_dataloader
dlrm_main/0 <span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span>:    <span class="token punctuation">(</span>root_name, stage<span class="token punctuation">)</span> <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token string">&quot;train&quot;</span>, <span class="token string">&quot;test&quot;</span><span class="token punctuation">)</span> <span class="token keyword">if</span> stage <span class="token operator">==</span> <span class="token string">&quot;val&quot;</span> <span class="token keyword">else</span> stage
dlrm_main/0 <span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">]</span>:ValueError: too many values to unpack <span class="token punctuation">(</span>expected <span class="token number">2</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>将(root_name, stage) = (&quot;train&quot;, &quot;test&quot;) if stage == &quot;val&quot; else stage，修改为(root_name, stage) = (&quot;train&quot;, &quot;test&quot;) if stage == &quot;val&quot; else (stage, stage)后可跑通，但验证时AUROC只有0.5002527236938477，且修改超参仍无法提升</p></li></ul></li></ul></li></ul><h2 id="_7-torchrec与hugectr比较" tabindex="-1"><a class="header-anchor" href="#_7-torchrec与hugectr比较"><span>7. Torchrec与HugeCTR比较</span></a></h2><ul><li><p>Torchrec:</p><ul><li>架构，特点，优缺点： <ul><li>优点： <ul><li>实现分布式模型并行，支持数据模型混合并行，多种emb切分方式</li><li>可根据设备型号与数量，内存大小等信息自动生成切分策略：根据输入网络结构,参数规模, 运行的设备环境的拓扑,设备的带宽,显存, 数据类型等信息, 穷举所有切分方案,然后挑出所有可行的方案做一个模拟性能评估, 根据评估的结果再挑出性能最好的方案出来</li><li>支持多级流水，重叠数据加载，设备间通信（数据分发）和前反向优化计算</li><li>fbgemm库优化kernel，量化等</li><li>PS架构，CPU做Server，GPU做Workers <ul><li>传统PS架构emb在CPU上，需要将更新特征推送回ps，需等待通信，难以组成流水，无法充分利用算力</li><li>当前架构利用gpu高带宽交换emb，减少通信开销，且充分利用并行计算性能</li></ul></li><li>支持GPU cache机制，存储热数据，使用LRU或LFU混合驱逐策略，使用32位数据结构记录数据使用情况：低27位记录时间戳，高5位记录出现频次（概率算法，每次取频次位随机数，全0加一），LFU比LRU优先级高</li><li>支持onehot和multihot以及缺省值数据，更符合真实场景</li></ul></li><li>缺点： <ul><li>支持功能多，导致框架内部中间数据传递低效，需多次转换</li><li>不支持动态扩表，只支持静态表，无法处理新特征</li><li>新网络适配到框架需要做一定开发，无法直接使用封装接口</li></ul></li></ul></li><li>数据流： <ul><li>大多实际业务数据是TB级的，可分为onehot和multihot数据，相当于某一选项单选还是多选</li><li>criteo-1t点击率数据集：1label-13dense（数值型）-26-sparse（分类型），共24天数据，大小达655GB</li><li>数据预处理： <ul><li>将dense int32转为float32（求log），划分为三个npy文件</li><li>将sparse特征连续化，统计所有出现的特征数量，低于某一阈值的特征都映射为key=1，其余依次累计key值</li><li>0-22天的数据做训练集，shuffle，23天做验证集，不shuffle</li></ul></li><li>数据并行-模型并行-数据并行： <ul><li>数据在内存中通过DDP均匀分发到不同设备上（每张卡数据不同）【scatter】</li><li>不同卡拿到训练数据需要到对应emb表中查询vector，每章表根据plan存储在不同设备上【all2all】</li><li>拿到emb数据后再传入后续网络结构中训练，每个设备上的dense 网络部分需要同步【allreduce】</li><li>最后经过反向梯度传到每个设备上的emb表中，再更新表的权值</li></ul></li></ul></li><li>测试结果：8A100 SXM4 80G <ul><li>MLPerf DLRM v1: 0.8004</li><li>MLPerf DLRM v2: 0.8041</li></ul></li><li>迁移方案&amp;遇到的问题与解决方法： <ul><li>使用设备端哈希表存储emb，来实现动态扩容 <ul><li>key和value分别存储到两张表，key映射到value,key表扩容开辟一块新空间，拷贝到新表，value以链表形式存储，直接追加到表尾</li></ul></li><li>版本与依赖库问题：原生框架只支持pt1.13以上，需要切分和fbgemm库提供支持，因此需要替换或避免这些功能</li><li>将使用到fbgemm库的位置替换实现或重写，使用cnco</li><li>将使用到高版本pt的模块解耦出来作为子模块使用</li><li>不同语言实现的模块使用torch_library绑定c++接口到py端</li><li>auroc使用sklearn替换实现，使用集合通信接口将数据传回cpu做计算</li></ul></li><li>性能提升： <ul><li>使用profiler获取host和device侧算子和kernel调用情况，找出热点算子，再提issue或替换算子来提升精度</li><li>因为不再使用fbgemm库，部分框架功能实现性能差，如内部数据间的相互转换使用低效的标量操作，替换为矩阵操作快很多</li></ul></li><li>精度提升： <ul><li>不同语言间实现的模块在配合使用时用到了不同的stream，传参时没能同步，会导致数据异常</li><li>数据初始化方式使用与表大小相关的均匀分布，精度会更高</li></ul></li></ul></li><li><p>HugeCTR:</p><ul><li>架构，特点，优缺点，异同： <ul><li>相同：都支持读取分发训练流水</li><li>不同： <ul><li>hugectr只支持按行按表切分emb，torchrec能自动选择且能混合多种切分方式</li><li>两者数据预处理方式相似，只是hugectr还可使用特征组合进一步减少特征数量，提升表达能力</li><li>hugectr使用keylist预先划分好训练数据，torchrec直接使用dataloader读取</li><li>hctr支持动态表，使用GPU上的hashtable实现（cucollection）</li></ul></li><li>hugectr更加完善，是merlin推荐系统模型推理训练解决方案下的训练框架，支持GPU中的hash表，异步与多线程流水，分层参数服务器做推理，TF插件sok</li><li>训练时，hugectr预取每个pass的key集合到keyset，从而解决无法存放所有数据的难题</li><li>参数服务器支持全量读如整个emb到host内存中（速度快），或使用多级缓存结构只存部分（克服对模型规模的限制）</li><li>推理时，三级存储结构：使用GPU嵌入缓存将热点emb放在gpu内存中，内存作为二级，使用redis保存部分数据；第三级使用SSD RocksDB保存所有参数（高效存储长尾分布数据），使用的是LRU</li></ul></li></ul></li><li><p>Relevant：</p><ul><li>GPU hashtable： <ul><li>hash冲突处理方法： <ul><li>double hash，分别计算再聚合</li><li>frequency hash：对低频做双hash，高频做identity hash</li></ul></li></ul></li><li>性能指标：profiling timechart， 吞吐量</li><li>精度指标：AUC</li></ul></li></ul></div><!--[--><!----><!--]--><footer class="page-meta"><!----><div class="meta-item git-info"><!----><!----></div></footer><nav class="vp-page-nav"><a class="route-link nav-link prev" href="/notes/PLAN_Z.html" aria-label="TODO LIST"><div class="hint"><span class="arrow start"></span>上一页</div><div class="link"><!---->TODO LIST</div></a><a class="route-link nav-link next" href="/notes/uml_note.html" aria-label="UML学习笔记"><div class="hint">下一页<span class="arrow end"></span></div><div class="link">UML学习笔记<!----></div></a></nav><div id="vp-comment" class="giscus-wrapper input-top" style="display:block;"><svg xmlns="http://www.w3.org/2000/svg" width="32" height="32" preserveAspectRatio="xMidYMid" viewBox="0 0 100 100"><circle cx="28" cy="75" r="11" fill="currentColor"><animate attributeName="fill-opacity" begin="0s" dur="1s" keyTimes="0;0.2;1" repeatCount="indefinite" values="0;1;1"></animate></circle><path fill="none" stroke="#88baf0" stroke-width="10" d="M28 47a28 28 0 0 1 28 28"><animate attributeName="stroke-opacity" begin="0.1s" dur="1s" keyTimes="0;0.2;1" repeatCount="indefinite" values="0;1;1"></animate></path><path fill="none" stroke="#88baf0" stroke-width="10" d="M28 25a50 50 0 0 1 50 50"><animate attributeName="stroke-opacity" begin="0.2s" dur="1s" keyTimes="0;0.2;1" repeatCount="indefinite" values="0;1;1"></animate></path></svg></div><!--[--><!----><!--]--><!--]--></main><!--]--><footer class="vp-footer-wrapper"><div class="vp-footer">BradZhone's Blog</div><div class="vp-copyright">Copyright © 2024 BradZhone </div></footer></div><!--]--><!--[--><!----><!--]--><!--]--></div>
    <script type="module" src="/assets/app-Coh1oo3x.js" defer></script>
  </body>
</html>
